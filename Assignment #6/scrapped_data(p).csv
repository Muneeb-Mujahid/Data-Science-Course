Names,p
Artificial intelligence,"[<p class=""mw-empty-elt"">
</p>, <p class=""mw-empty-elt"">
</p>, <p><b>Artificial intelligence</b> (<b>AI</b>) is <a href=""/wiki/Intelligence"" title=""Intelligence"">intelligence</a> demonstrated by <a href=""/wiki/Machine"" title=""Machine"">machines</a>, as opposed to the <b>natural intelligence</b> displayed by <a href=""/wiki/Animal_cognition"" title=""Animal cognition"">animals</a> including <a href=""/wiki/Human_intelligence"" title=""Human intelligence"">humans</a>. AI research has been defined as the field of study of <a href=""/wiki/Intelligent_agent"" title=""Intelligent agent"">intelligent agents</a>, which refers to any system that perceives its environment and takes actions that maximize its chance of achieving its goals.<sup class=""reference"" id=""cite_ref-Definition_of_AI_1-0""><a href=""#cite_note-Definition_of_AI-1"">[a]</a></sup>
</p>, <p>The term ""artificial intelligence"" had previously been used to describe machines that mimic and display ""human"" cognitive skills that are associated with the <a class=""mw-redirect"" href=""/wiki/Human_mind"" title=""Human mind"">human mind</a>, such as ""learning"" and ""problem-solving"". This definition has since been rejected by major AI researchers who now describe AI in terms of <a href=""/wiki/Rationality"" title=""Rationality"">rationality</a> and acting rationally, which does not limit how intelligence can be articulated.<sup class=""reference"" id=""cite_ref-3""><a href=""#cite_note-3"">[b]</a></sup>
</p>, <p>AI applications include advanced <a class=""mw-redirect"" href=""/wiki/Web_search"" title=""Web search"">web search</a> engines (e.g., <a href=""/wiki/Google"" title=""Google"">Google</a>), <a href=""/wiki/Recommender_system"" title=""Recommender system"">recommendation systems</a> (used by <a href=""/wiki/YouTube"" title=""YouTube"">YouTube</a>, <a href=""/wiki/Amazon_(company)"" title=""Amazon (company)"">Amazon</a> and <a href=""/wiki/Netflix"" title=""Netflix"">Netflix</a>), <a href=""/wiki/Natural-language_understanding"" title=""Natural-language understanding"">understanding human speech</a> (such as <a href=""/wiki/Siri"" title=""Siri"">Siri</a> and <a href=""/wiki/Amazon_Alexa"" title=""Amazon Alexa"">Alexa</a>), <a href=""/wiki/Self-driving_car"" title=""Self-driving car"">self-driving cars</a> (e.g., <a href=""/wiki/Tesla,_Inc."" title=""Tesla, Inc."">Tesla</a>), <a href=""/wiki/Automated_decision-making"" title=""Automated decision-making"">automated decision-making</a> and competing at the highest level in <a class=""mw-redirect"" href=""/wiki/Strategic_game"" title=""Strategic game"">strategic game</a> systems (such as <a href=""/wiki/Chess"" title=""Chess"">chess</a> and <a href=""/wiki/Go_(game)"" title=""Go (game)"">Go</a>).<sup class=""reference"" id=""cite_ref-FOOTNOTEGoogle2016_4-0""><a href=""#cite_note-FOOTNOTEGoogle2016-4"">[2]</a></sup><sup class=""noprint Inline-Template Template-Fact"" style=""white-space:nowrap;"">[<i><a href=""/wiki/Wikipedia:Citation_needed"" title=""Wikipedia:Citation needed""><span title=""This claim needs references to reliable sources. (October 2021)"">citation needed</span></a></i>]</sup>
As machines become increasingly capable, tasks considered to require ""intelligence"" are often removed from the definition of AI, a phenomenon known as the <a href=""/wiki/AI_effect"" title=""AI effect"">AI effect</a>.<sup class=""reference"" id=""cite_ref-FOOTNOTEMcCorduck2004204_5-0""><a href=""#cite_note-FOOTNOTEMcCorduck2004204-5"">[3]</a></sup>  For instance, <a href=""/wiki/Optical_character_recognition"" title=""Optical character recognition"">optical character recognition</a> is frequently excluded from things considered to be AI,<sup class=""reference"" id=""cite_ref-FOOTNOTEAshok832019_6-0""><a href=""#cite_note-FOOTNOTEAshok832019-6"">[4]</a></sup> having become a routine technology.<sup class=""reference"" id=""cite_ref-FOOTNOTESchank199138_7-0""><a href=""#cite_note-FOOTNOTESchank199138-7"">[5]</a></sup>
</p>, <p>Artificial intelligence was founded as an academic discipline in 1956, and in the years since has experienced several waves of optimism,<sup class=""reference"" id=""cite_ref-FOOTNOTECrevier1993109_8-0""><a href=""#cite_note-FOOTNOTECrevier1993109-8"">[6]</a></sup><sup class=""reference"" id=""cite_ref-AI_in_the_80s_9-0""><a href=""#cite_note-AI_in_the_80s-9"">[7]</a></sup> followed by disappointment and the loss of funding (known as an ""<a href=""/wiki/AI_winter"" title=""AI winter"">AI winter</a>""),<sup class=""reference"" id=""cite_ref-First_AI_winter_10-0""><a href=""#cite_note-First_AI_winter-10"">[8]</a></sup><sup class=""reference"" id=""cite_ref-Second_AI_winter_11-0""><a href=""#cite_note-Second_AI_winter-11"">[9]</a></sup> followed by new approaches, success and renewed funding.<sup class=""reference"" id=""cite_ref-AI_in_the_80s_9-1""><a href=""#cite_note-AI_in_the_80s-9"">[7]</a></sup><sup class=""reference"" id=""cite_ref-FOOTNOTEClark2015b_12-0""><a href=""#cite_note-FOOTNOTEClark2015b-12"">[10]</a></sup> AI research has tried and discarded many different approaches since its founding, including simulating the brain, <a class=""mw-redirect"" href=""/wiki/Symbolic_AI#Cognitive_simulation"" title=""Symbolic AI"">modeling human problem solving</a>, <a class=""mw-redirect"" href=""/wiki/Symbolic_AI#Logic-based"" title=""Symbolic AI"">formal logic</a>, <a class=""mw-redirect"" href=""/wiki/Symbolic_AI#Knowledge-based"" title=""Symbolic AI"">large databases of knowledge</a> and imitating animal behavior. In the first decades of the 21st century, highly mathematical-statistical <a href=""/wiki/Machine_learning"" title=""Machine learning"">machine learning</a> has dominated the field, and this technique has proved highly successful, helping to solve many challenging problems throughout industry and academia.<sup class=""reference"" id=""cite_ref-AI_widely_used_1990s_13-0""><a href=""#cite_note-AI_widely_used_1990s-13"">[11]</a></sup><sup class=""reference"" id=""cite_ref-FOOTNOTEClark2015b_12-1""><a href=""#cite_note-FOOTNOTEClark2015b-12"">[10]</a></sup>
</p>, <p>The various sub-fields of AI research are centered around particular goals and the use of particular tools. The traditional goals of AI research include <a href=""/wiki/Automated_reasoning"" title=""Automated reasoning"">reasoning</a>, <a class=""mw-redirect"" href=""/wiki/Knowledge_representation"" title=""Knowledge representation"">knowledge representation</a>, <a href=""/wiki/Automated_planning_and_scheduling"" title=""Automated planning and scheduling"">planning</a>, <a href=""/wiki/Machine_learning"" title=""Machine learning"">learning</a>, <a href=""/wiki/Natural_language_processing"" title=""Natural language processing"">natural language processing</a>, <a href=""/wiki/Machine_perception"" title=""Machine perception"">perception</a>, and the ability to move and manipulate objects.<sup class=""reference"" id=""cite_ref-Problems_of_AI_14-0""><a href=""#cite_note-Problems_of_AI-14"">[c]</a></sup> <a href=""/wiki/Artificial_general_intelligence"" title=""Artificial general intelligence"">General intelligence</a> (the ability to solve an arbitrary problem) is among the field's long-term goals.<sup class=""reference"" id=""cite_ref-Artificial_General_Intelligence_15-0""><a href=""#cite_note-Artificial_General_Intelligence-15"">[12]</a></sup> To solve these problems, AI researchers have adapted and integrated a wide range of problem-solving techniques—including search and mathematical optimization, formal logic, <a href=""/wiki/Artificial_neural_network"" title=""Artificial neural network"">artificial neural networks</a>, and methods based on <a href=""/wiki/Statistics"" title=""Statistics"">statistics</a>, <a href=""/wiki/Probability"" title=""Probability"">probability</a> and <a href=""/wiki/Economics"" title=""Economics"">economics</a>. AI also draws upon <a href=""/wiki/Computer_science"" title=""Computer science"">computer science</a>, <a href=""/wiki/Psychology"" title=""Psychology"">psychology</a>, <a href=""/wiki/Linguistics"" title=""Linguistics"">linguistics</a>, <a href=""/wiki/Philosophy"" title=""Philosophy"">philosophy</a>, and many other fields.
</p>, <p>The field was founded on the assumption that human intelligence ""can be so precisely described that a machine can be made to simulate it"".<sup class=""reference"" id=""cite_ref-17""><a href=""#cite_note-17"">[d]</a></sup>
This raised philosophical arguments about the mind and the ethical consequences of creating artificial beings endowed with human-like intelligence; these issues have previously been explored by <a href=""/wiki/History_of_artificial_intelligence#Precursors"" title=""History of artificial intelligence"">myth</a>, <a href=""/wiki/Artificial_intelligence_in_fiction"" title=""Artificial intelligence in fiction"">fiction</a> and <a href=""/wiki/Philosophy_of_artificial_intelligence"" title=""Philosophy of artificial intelligence"">philosophy</a> since antiquity.<sup class=""reference"" id=""cite_ref-FOOTNOTENewquist199445–53_18-0""><a href=""#cite_note-FOOTNOTENewquist199445–53-18"">[14]</a></sup> <a href=""/wiki/Science_fiction"" title=""Science fiction"">Science fiction</a> writers and <a href=""/wiki/Futures_studies"" title=""Futures studies"">futurologists</a> have since suggested that AI may become an <a class=""mw-redirect"" href=""/wiki/Existential_risk"" title=""Existential risk"">existential risk</a> to humanity if its rational capacities are not overseen.<sup class=""reference"" id=""cite_ref-FOOTNOTESpadafora2016_19-0""><a href=""#cite_note-FOOTNOTESpadafora2016-19"">[15]</a></sup><sup class=""reference"" id=""cite_ref-FOOTNOTELombardoBoehmNairz2020_20-0""><a href=""#cite_note-FOOTNOTELombardoBoehmNairz2020-20"">[16]</a></sup>
</p>, <p><a class=""mw-redirect"" href=""/wiki/Artificial_being"" title=""Artificial being"">Artificial beings</a> with intelligence appeared as <a class=""mw-redirect"" href=""/wiki/Storytelling_device"" title=""Storytelling device"">storytelling devices</a> in antiquity,<sup class=""reference"" id=""cite_ref-AI_in_myth_21-0""><a href=""#cite_note-AI_in_myth-21"">[17]</a></sup>
and have been common in fiction, as in <a href=""/wiki/Mary_Shelley"" title=""Mary Shelley"">Mary Shelley</a>'s <i><a href=""/wiki/Frankenstein"" title=""Frankenstein"">Frankenstein</a></i> or <a href=""/wiki/Karel_%C4%8Capek"" title=""Karel Čapek"">Karel Čapek</a>'s <i><a href=""/wiki/R.U.R."" title=""R.U.R."">R.U.R.</a></i><sup class=""reference"" id=""cite_ref-FOOTNOTEMcCorduck200417–25_22-0""><a href=""#cite_note-FOOTNOTEMcCorduck200417–25-22"">[18]</a></sup> These characters and their fates raised many of the same issues now discussed in the <a href=""/wiki/Ethics_of_artificial_intelligence"" title=""Ethics of artificial intelligence"">ethics of artificial intelligence</a>.<sup class=""reference"" id=""cite_ref-FOOTNOTEMcCorduck2004340–400_23-0""><a href=""#cite_note-FOOTNOTEMcCorduck2004340–400-23"">[19]</a></sup>
</p>, <p>The study of mechanical or <a class=""mw-redirect"" href=""/wiki/Formal_reasoning"" title=""Formal reasoning"">""formal"" reasoning</a> began with <a href=""/wiki/Philosopher"" title=""Philosopher"">philosophers</a> and mathematicians in antiquity. The study of mathematical logic led directly to <a href=""/wiki/Alan_Turing"" title=""Alan Turing"">Alan Turing</a>'s <a href=""/wiki/Theory_of_computation"" title=""Theory of computation"">theory of computation</a>, which suggested that a machine, by shuffling symbols as simple as ""0"" and ""1"", could simulate any conceivable act of mathematical deduction. This insight that digital computers can simulate any process of formal reasoning is known as the <a href=""/wiki/Church%E2%80%93Turing_thesis"" title=""Church–Turing thesis"">Church–Turing thesis</a>.<sup class=""reference"" id=""cite_ref-FOOTNOTEBerlinski2000_24-0""><a href=""#cite_note-FOOTNOTEBerlinski2000-24"">[20]</a></sup>
</p>, <p>The Church-Turing thesis, along with concurrent discoveries in <a href=""/wiki/Neuroscience"" title=""Neuroscience"">neurobiology</a>, <a href=""/wiki/Information_theory"" title=""Information theory"">information theory</a> and <a href=""/wiki/Cybernetics"" title=""Cybernetics"">cybernetics</a>, led researchers to consider the possibility of building an electronic brain.<sup class=""reference"" id=""cite_ref-25""><a href=""#cite_note-25"">[21]</a></sup>
The first work that is now generally recognized as AI was <a class=""mw-redirect"" href=""/wiki/Warren_McCullouch"" title=""Warren McCullouch"">McCullouch</a> and <a href=""/wiki/Walter_Pitts"" title=""Walter Pitts"">Pitts</a>' 1943 formal design for <a class=""mw-redirect"" href=""/wiki/Turing-complete"" title=""Turing-complete"">Turing-complete</a> ""artificial neurons"".<sup class=""reference"" id=""cite_ref-FOOTNOTERussellNorvig200916_26-0""><a href=""#cite_note-FOOTNOTERussellNorvig200916-26"">[22]</a></sup>
</p>, <p>By the 1950s, two visions for how to achieve machine intelligence emerged. One vision, known <a class=""mw-redirect"" href=""/wiki/Symbolic_AI"" title=""Symbolic AI"">Symbolic AI</a> or <a class=""mw-redirect"" href=""/wiki/GOFAI"" title=""GOFAI"">GOFAI</a>, was to use computers to create a symbolic representation of the world and systems that could reason about the world. Proponents included <a href=""/wiki/Allen_Newell"" title=""Allen Newell"">Allen Newell</a>, <a href=""/wiki/Herbert_A._Simon"" title=""Herbert A. Simon"">Herbert A. Simon</a>, and <a href=""/wiki/Marvin_Minsky"" title=""Marvin Minsky"">Marvin Minsky</a>. Closely associated with this approach was the <a href=""/wiki/Heuristic_(computer_science)"" title=""Heuristic (computer science)"">""heuristic search""</a> approach, which likened intelligence to a problem of exploring a space of possibilities for answers. The second vision, known as the <a href=""/wiki/Connectionism"" title=""Connectionism"">connectionist approach</a>, sought to achieve intelligence through learning. Proponents of this approach, most prominently <a href=""/wiki/Frank_Rosenblatt"" title=""Frank Rosenblatt"">Frank Rosenblatt</a>, sought to connect <a class=""mw-redirect"" href=""/wiki/Perceptrons"" title=""Perceptrons"">Perceptron</a> in ways inspired by connections of neurons.<sup class=""reference"" id=""cite_ref-FOOTNOTEManyika20229_27-0""><a href=""#cite_note-FOOTNOTEManyika20229-27"">[23]</a></sup> <a href=""/wiki/James_Manyika"" title=""James Manyika"">James Manyika</a> and others have compared the two approaches to the mind (Symbolic AI) and the brain (connectionist). Manyika argues that symbolic approaches dominated the push for artificial intelligence in this period, due in part to it's connection to intellectual traditions of <a class=""mw-redirect"" href=""/wiki/Descarte"" title=""Descarte"">Descarte</a>, <a class=""mw-redirect"" href=""/wiki/Boole"" title=""Boole"">Boole</a>, <a href=""/wiki/Gottlob_Frege"" title=""Gottlob Frege"">Gottlob Frege</a>, <a href=""/wiki/Bertrand_Russell"" title=""Bertrand Russell"">Bertrand Russell</a>, and others. Connectionist approaches based on <a href=""/wiki/Cybernetics"" title=""Cybernetics"">cybernetics</a> or <a href=""/wiki/Artificial_neural_network"" title=""Artificial neural network"">artificial neural networks</a> were pushed to the background but have gained new prominence in recent decades.<sup class=""reference"" id=""cite_ref-FOOTNOTEManyika202210_28-0""><a href=""#cite_note-FOOTNOTEManyika202210-28"">[24]</a></sup>
</p>, <p>The field of AI research was born at <a href=""/wiki/Dartmouth_workshop"" title=""Dartmouth workshop"">a workshop</a> at <a href=""/wiki/Dartmouth_College"" title=""Dartmouth College"">Dartmouth College</a> in 1956.<sup class=""reference"" id=""cite_ref-31""><a href=""#cite_note-31"">[e]</a></sup><sup class=""reference"" id=""cite_ref-32""><a href=""#cite_note-32"">[27]</a></sup>
The attendees became the founders and leaders of AI research.<sup class=""reference"" id=""cite_ref-33""><a href=""#cite_note-33"">[f]</a></sup>
They and their students produced programs that the press described as ""astonishing"":<sup class=""reference"" id=""cite_ref-35""><a href=""#cite_note-35"">[g]</a></sup>
computers were learning <a class=""mw-redirect"" href=""/wiki/Draughts"" title=""Draughts"">checkers</a> strategies, solving word problems in algebra, proving <a href=""/wiki/Theorem"" title=""Theorem"">logical theorems</a> and speaking English.<sup class=""reference"" id=""cite_ref-36""><a href=""#cite_note-36"">[h]</a></sup><sup class=""reference"" id=""cite_ref-37""><a href=""#cite_note-37"">[29]</a></sup>
By the middle of the 1960s, research in the U.S. was heavily funded by the <a href=""/wiki/DARPA"" title=""DARPA"">Department of Defense</a><sup class=""reference"" id=""cite_ref-38""><a href=""#cite_note-38"">[30]</a></sup>
and laboratories had been established around the world.<sup class=""reference"" id=""cite_ref-FOOTNOTEHowe1994_39-0""><a href=""#cite_note-FOOTNOTEHowe1994-39"">[31]</a></sup>
</p>, <p>Researchers in the 1960s and the 1970s were convinced that symbolic approaches would eventually succeed in creating a machine with <a href=""/wiki/Artificial_general_intelligence"" title=""Artificial general intelligence"">artificial general intelligence</a> and considered this the goal of their field.<sup class=""reference"" id=""cite_ref-FOOTNOTENewquist199486–86_40-0""><a href=""#cite_note-FOOTNOTENewquist199486–86-40"">[32]</a></sup>
<a href=""/wiki/Herbert_A._Simon"" title=""Herbert A. Simon"">Herbert Simon</a> predicted, ""machines will be capable, within twenty years, of doing any work a man can do"".<sup class=""reference"" id=""cite_ref-41""><a href=""#cite_note-41"">[33]</a></sup>
<a href=""/wiki/Marvin_Minsky"" title=""Marvin Minsky"">Marvin Minsky</a> agreed, writing, ""within a generation ... the problem of creating 'artificial intelligence' will substantially be solved"".<sup class=""reference"" id=""cite_ref-42""><a href=""#cite_note-42"">[34]</a></sup>
</p>, <p>They failed to recognize the difficulty of some of the remaining tasks. Progress slowed and in 1974, in response to the <a href=""/wiki/Lighthill_report"" title=""Lighthill report"">criticism</a> of <a class=""mw-redirect"" href=""/wiki/Sir_James_Lighthill"" title=""Sir James Lighthill"">Sir James Lighthill</a><sup class=""reference"" id=""cite_ref-FOOTNOTELighthill1973_43-0""><a href=""#cite_note-FOOTNOTELighthill1973-43"">[35]</a></sup>
and ongoing pressure from the US Congress to <a class=""mw-redirect"" href=""/wiki/Mansfield_Amendment"" title=""Mansfield Amendment"">fund more productive projects</a>, both the U.S. and British governments cut off exploratory research in AI. The next few years would later be called an ""<a href=""/wiki/AI_winter"" title=""AI winter"">AI winter</a>"", a period when obtaining funding for AI projects was difficult.
<sup class=""reference"" id=""cite_ref-First_AI_winter_10-1""><a href=""#cite_note-First_AI_winter-10"">[8]</a></sup>
</p>, <p>In the early 1980s, AI research was revived by the commercial success of <a href=""/wiki/Expert_system"" title=""Expert system"">expert systems</a>,<sup class=""reference"" id=""cite_ref-44""><a href=""#cite_note-44"">[36]</a></sup>
a form of AI program that simulated the knowledge and analytical skills of human experts. By 1985, the market for AI had reached over a billion dollars. At the same time, Japan's <a class=""mw-redirect"" href=""/wiki/Fifth_generation_computer"" title=""Fifth generation computer"">fifth generation computer</a> project inspired the U.S and British governments to restore funding for <a class=""mw-redirect"" href=""/wiki/Academic_research"" title=""Academic research"">academic research</a>.<sup class=""reference"" id=""cite_ref-AI_in_the_80s_9-2""><a href=""#cite_note-AI_in_the_80s-9"">[7]</a></sup>
However, beginning with the collapse of the <a class=""mw-redirect"" href=""/wiki/Lisp_Machine"" title=""Lisp Machine"">Lisp Machine</a> market in 1987, AI once again fell into disrepute, and a second, longer-lasting winter began.<sup class=""reference"" id=""cite_ref-Second_AI_winter_11-1""><a href=""#cite_note-Second_AI_winter-11"">[9]</a></sup>
</p>, <p>Many researchers began to doubt that the <a class=""mw-redirect"" href=""/wiki/Symbolic_AI"" title=""Symbolic AI"">symbolic approach</a> would be able to imitate all the processes of human cognition, especially <a href=""/wiki/Machine_perception"" title=""Machine perception"">perception</a>, robotics, <a href=""/wiki/Machine_learning"" title=""Machine learning"">learning</a> and <a href=""/wiki/Pattern_recognition"" title=""Pattern recognition"">pattern recognition</a>. A number of researchers began to look into ""sub-symbolic"" approaches to specific AI problems.<sup class=""reference"" id=""cite_ref-FOOTNOTENilsson19987_45-0""><a href=""#cite_note-FOOTNOTENilsson19987-45"">[37]</a></sup> <a href=""/wiki/Robotics"" title=""Robotics"">Robotics</a> researchers, such as <a href=""/wiki/Rodney_Brooks"" title=""Rodney Brooks"">Rodney Brooks</a>, rejected symbolic AI and focused on the basic engineering problems that would allow robots to move, survive, and learn their environment.<sup class=""reference"" id=""cite_ref-50""><a href=""#cite_note-50"">[i]</a></sup>
Interest in <a href=""/wiki/Artificial_neural_network"" title=""Artificial neural network"">neural networks</a> and ""<a href=""/wiki/Connectionism"" title=""Connectionism"">connectionism</a>"" was revived by <a href=""/wiki/Geoffrey_Hinton"" title=""Geoffrey Hinton"">Geoffrey Hinton</a>, <a href=""/wiki/David_Rumelhart"" title=""David Rumelhart"">David Rumelhart</a> and others in the middle of the 1980s.<sup class=""reference"" id=""cite_ref-51""><a href=""#cite_note-51"">[42]</a></sup>
<a href=""/wiki/Soft_computing"" title=""Soft computing"">Soft computing</a> tools were developed in the 80s, such as <a href=""/wiki/Artificial_neural_network"" title=""Artificial neural network"">neural networks</a>, <a class=""mw-redirect"" href=""/wiki/Fuzzy_system"" title=""Fuzzy system"">fuzzy systems</a>, <a class=""mw-redirect"" href=""/wiki/Grey_system_theory"" title=""Grey system theory"">Grey system theory</a>, <a href=""/wiki/Evolutionary_computation"" title=""Evolutionary computation"">evolutionary computation</a> and many tools drawn from <a href=""/wiki/Statistics"" title=""Statistics"">statistics</a> or <a href=""/wiki/Mathematical_optimization"" title=""Mathematical optimization"">mathematical optimization</a>.
</p>, <p>AI gradually restored its reputation in the late 1990s and early 21st century by finding specific solutions to specific problems. The narrow focus allowed researchers to produce verifiable results, exploit more  mathematical methods, and collaborate with other fields (such as <a href=""/wiki/Statistics"" title=""Statistics"">statistics</a>, <a href=""/wiki/Economics"" title=""Economics"">economics</a> and <a href=""/wiki/Mathematical_optimization"" title=""Mathematical optimization"">mathematics</a>).<sup class=""reference"" id=""cite_ref-AI_1990s_52-0""><a href=""#cite_note-AI_1990s-52"">[43]</a></sup>
By 2000, solutions developed by AI researchers were being widely used, although in the 1990s they were rarely described as ""artificial intelligence"".<sup class=""reference"" id=""cite_ref-AI_widely_used_1990s_13-1""><a href=""#cite_note-AI_widely_used_1990s-13"">[11]</a></sup>
</p>, <p><a href=""/wiki/Moore%27s_law"" title=""Moore's law"">Faster computers</a>, algorithmic improvements, and access to <a href=""/wiki/Big_data"" title=""Big data"">large amounts of data</a> enabled advances in <a href=""/wiki/Machine_learning"" title=""Machine learning"">machine learning</a> and perception; data-hungry <a href=""/wiki/Deep_learning"" title=""Deep learning"">deep learning</a> methods started to dominate accuracy benchmarks <a href=""/wiki/Deep_learning#Deep_learning_revolution"" title=""Deep learning"">around 2012</a>.<sup class=""reference"" id=""cite_ref-FOOTNOTEMcKinsey2018_53-0""><a href=""#cite_note-FOOTNOTEMcKinsey2018-53"">[44]</a></sup>
According to <a href=""/wiki/Bloomberg_News"" title=""Bloomberg News"">Bloomberg's</a> Jack Clark, 2015 was a landmark year for artificial intelligence, with the number of software projects that use AI within <a href=""/wiki/Google"" title=""Google"">Google</a> increased from a ""sporadic usage"" in 2012 to more than 2,700 projects.<sup class=""reference"" id=""cite_ref-54""><a href=""#cite_note-54"">[j]</a></sup> He attributes this to an increase in affordable <a href=""/wiki/Artificial_neural_network"" title=""Artificial neural network"">neural networks</a>, due to a rise in cloud computing infrastructure and to an increase in research tools and datasets.<sup class=""reference"" id=""cite_ref-FOOTNOTEClark2015b_12-3""><a href=""#cite_note-FOOTNOTEClark2015b-12"">[10]</a></sup> In a 2017 survey, one in five companies reported they had ""incorporated AI in some offerings or processes"".<sup class=""reference"" id=""cite_ref-55""><a href=""#cite_note-55"">[45]</a></sup> The amount of research into AI (measured by total publications) increased by 50% in the years 2015–2019.<sup class=""reference"" id=""cite_ref-FOOTNOTEUNESCO2021_56-0""><a href=""#cite_note-FOOTNOTEUNESCO2021-56"">[46]</a></sup>
</p>, <p>Numerous academic researchers became concerned that AI was no longer pursuing the original goal of creating versatile, fully intelligent machines. Much of current research involves statistical AI, which is overwhelmingly used to solve specific problems, even highly successful techniques such as <a href=""/wiki/Deep_learning"" title=""Deep learning"">deep learning</a>. This concern has led to the subfield of <a href=""/wiki/Artificial_general_intelligence"" title=""Artificial general intelligence"">artificial general intelligence</a> (or ""AGI""), which had several well-funded institutions by the 2010s.<sup class=""reference"" id=""cite_ref-Artificial_General_Intelligence_15-1""><a href=""#cite_note-Artificial_General_Intelligence-15"">[12]</a></sup>
</p>, <p>The general problem of simulating (or creating) intelligence has been broken down into sub-problems. These consist of particular traits or capabilities that researchers expect an intelligent system to display. The traits described below have received the most attention.<sup class=""reference"" id=""cite_ref-Problems_of_AI_14-1""><a href=""#cite_note-Problems_of_AI-14"">[c]</a></sup>
</p>, <p>Early researchers developed algorithms that imitated step-by-step reasoning that humans use when they solve puzzles or make logical deductions.<sup class=""reference"" id=""cite_ref-57""><a href=""#cite_note-57"">[47]</a></sup>
By the late 1980s and 1990s, AI research had developed methods for dealing with <a href=""/wiki/Uncertainty"" title=""Uncertainty"">uncertain</a> or incomplete information, employing concepts from <a href=""/wiki/Probability"" title=""Probability"">probability</a> and <a href=""/wiki/Economics"" title=""Economics"">economics</a>.<sup class=""reference"" id=""cite_ref-58""><a href=""#cite_note-58"">[48]</a></sup>
</p>, <p>Many of these algorithms proved to be insufficient for solving large reasoning problems because they experienced a ""combinatorial explosion"": they became exponentially slower as the problems grew larger.<sup class=""reference"" id=""cite_ref-Intractability_59-0""><a href=""#cite_note-Intractability-59"">[49]</a></sup>
Even humans rarely use the step-by-step deduction that early AI research could model. They solve most of their problems using fast, intuitive judgments.<sup class=""reference"" id=""cite_ref-Psychological_evidence_of_sub-symbolic_reasoning_60-0""><a href=""#cite_note-Psychological_evidence_of_sub-symbolic_reasoning-60"">[50]</a></sup>
</p>, <p>Knowledge representation and <a href=""/wiki/Knowledge_engineering"" title=""Knowledge engineering"">knowledge engineering</a><sup class=""reference"" id=""cite_ref-61""><a href=""#cite_note-61"">[51]</a></sup>
allow AI programs to answer questions intelligently and make deductions about real-world facts.
</p>, <p>A representation of ""what exists"" is an <a href=""/wiki/Ontology_(information_science)"" title=""Ontology (information science)"">ontology</a>: the set of objects, relations, concepts, and properties formally described so that software agents can interpret them.<sup class=""reference"" id=""cite_ref-FOOTNOTERussellNorvig2003320–328_62-0""><a href=""#cite_note-FOOTNOTERussellNorvig2003320–328-62"">[52]</a></sup>
The most general ontologies are called <a href=""/wiki/Upper_ontology"" title=""Upper ontology"">upper ontologies</a>, which attempt to provide a foundation for all other knowledge and act as mediators between <a class=""mw-redirect"" href=""/wiki/Domain_ontology"" title=""Domain ontology"">domain ontologies</a> that cover specific knowledge about a particular knowledge <a class=""mw-redirect"" href=""/wiki/Domain_ontology"" title=""Domain ontology"">domain</a> (field of interest or area of concern). A truly intelligent program would also need access to commonsense knowledge; the set of facts that an average person knows. The <a href=""/wiki/Semantics"" title=""Semantics"">semantics</a> of an ontology is typically represented in description logic, such as the <a href=""/wiki/Web_Ontology_Language"" title=""Web Ontology Language"">Web Ontology Language</a>.<sup class=""reference"" id=""cite_ref-Representing_categories_and_relations_63-0""><a href=""#cite_note-Representing_categories_and_relations-63"">[53]</a></sup>
</p>, <p>AI research has developed tools to represent specific domains, such as objects, properties, categories and relations between objects;<sup class=""reference"" id=""cite_ref-Representing_categories_and_relations_63-1""><a href=""#cite_note-Representing_categories_and_relations-63"">[53]</a></sup>
situations, events, states and time;<sup class=""reference"" id=""cite_ref-Representing_time_64-0""><a href=""#cite_note-Representing_time-64"">[54]</a></sup>
causes and effects;<sup class=""reference"" id=""cite_ref-Representing_causation_65-0""><a href=""#cite_note-Representing_causation-65"">[55]</a></sup>
knowledge about knowledge (what we know about what other people know);.<sup class=""reference"" id=""cite_ref-Representing_knowledge_about_knowledge_66-0""><a href=""#cite_note-Representing_knowledge_about_knowledge-66"">[56]</a></sup>
<a class=""mw-redirect"" href=""/wiki/Default_reasoning"" title=""Default reasoning"">default reasoning</a> (things that humans assume are true until they are told differently and will remain true even when other facts are changing);
<sup class=""reference"" id=""cite_ref-Default_reasoning_and_non-monotonic_logic_67-0""><a href=""#cite_note-Default_reasoning_and_non-monotonic_logic-67"">[57]</a></sup>
as well as other domains. Among the most difficult problems in AI are: the breadth of commonsense knowledge (the number of atomic facts that the average person knows is enormous);<sup class=""reference"" id=""cite_ref-Breadth_of_commonsense_knowledge_68-0""><a href=""#cite_note-Breadth_of_commonsense_knowledge-68"">[58]</a></sup>
and the sub-symbolic form of most commonsense knowledge (much of what people know is not represented as ""facts"" or ""statements"" that they could express verbally).<sup class=""reference"" id=""cite_ref-Psychological_evidence_of_sub-symbolic_reasoning_60-1""><a href=""#cite_note-Psychological_evidence_of_sub-symbolic_reasoning-60"">[50]</a></sup>
</p>, <p>Formal knowledge representations are used in content-based indexing and retrieval,<sup class=""reference"" id=""cite_ref-FOOTNOTESmoliarZhang1994_69-0""><a href=""#cite_note-FOOTNOTESmoliarZhang1994-69"">[59]</a></sup>
scene interpretation,<sup class=""reference"" id=""cite_ref-FOOTNOTENeumannMöller2008_70-0""><a href=""#cite_note-FOOTNOTENeumannMöller2008-70"">[60]</a></sup>
clinical decision support,<sup class=""reference"" id=""cite_ref-FOOTNOTEKupermanReichleyBailey2006_71-0""><a href=""#cite_note-FOOTNOTEKupermanReichleyBailey2006-71"">[61]</a></sup>
knowledge discovery (mining ""interesting"" and actionable inferences from large databases),<sup class=""reference"" id=""cite_ref-FOOTNOTEMcGarry2005_72-0""><a href=""#cite_note-FOOTNOTEMcGarry2005-72"">[62]</a></sup>
and other areas.<sup class=""reference"" id=""cite_ref-FOOTNOTEBertiniDel_BimboTorniai2006_73-0""><a href=""#cite_note-FOOTNOTEBertiniDel_BimboTorniai2006-73"">[63]</a></sup>
</p>, <p>An intelligent agent that can <a href=""/wiki/Automated_planning_and_scheduling"" title=""Automated planning and scheduling"">plan</a> makes a representation of the state of the world, makes predictions about how their actions will change it and make choices that maximize the <a href=""/wiki/Utility"" title=""Utility"">utility</a> (or ""value"") of the available choices.<sup class=""reference"" id=""cite_ref-74""><a href=""#cite_note-74"">[64]</a></sup>
In classical planning problems, the agent can assume that it is the only system acting in the world, allowing the agent to be certain of the consequences of its actions.<sup class=""reference"" id=""cite_ref-75""><a href=""#cite_note-75"">[65]</a></sup>
However, if the agent is not the only actor, then it requires that the agent reason under uncertainty, and continuously re-assess its environment and adapt.<sup class=""reference"" id=""cite_ref-76""><a href=""#cite_note-76"">[66]</a></sup>
<a href=""/wiki/Multi-agent_planning"" title=""Multi-agent planning"">Multi-agent planning</a> uses the <a href=""/wiki/Cooperation"" title=""Cooperation"">cooperation</a> and competition of many agents to achieve a given goal. <a class=""mw-redirect"" href=""/wiki/Emergent_behavior"" title=""Emergent behavior"">Emergent behavior</a> such as this is used by <a class=""mw-redirect"" href=""/wiki/Evolutionary_algorithms"" title=""Evolutionary algorithms"">evolutionary algorithms</a> and <a href=""/wiki/Swarm_intelligence"" title=""Swarm intelligence"">swarm intelligence</a>.<sup class=""reference"" id=""cite_ref-77""><a href=""#cite_note-77"">[67]</a></sup>
</p>, <p>Machine learning (ML), a fundamental concept of AI research since the field's inception,<sup class=""reference"" id=""cite_ref-80""><a href=""#cite_note-80"">[k]</a></sup>
is the study of computer algorithms that improve automatically through experience.<sup class=""reference"" id=""cite_ref-82""><a href=""#cite_note-82"">[l]</a></sup>
</p>, <p><a href=""/wiki/Unsupervised_learning"" title=""Unsupervised learning"">Unsupervised learning</a> finds patterns in a stream of input. <a href=""/wiki/Supervised_learning"" title=""Supervised learning"">Supervised learning</a> requires a human to label the input data first, and comes in two main varieties: <a href=""/wiki/Statistical_classification"" title=""Statistical classification"">classification</a> and numerical <a href=""/wiki/Regression_analysis"" title=""Regression analysis"">regression</a>. Classification is used to determine what category something belongs in—the program sees a number of examples of things from several categories and will learn to classify new inputs. Regression is the attempt to produce a function that describes the relationship between inputs and outputs and predicts how the outputs should change as the inputs change. Both classifiers and regression learners can be viewed as ""function approximators"" trying to learn an unknown (possibly implicit) function; for example, a spam classifier can be viewed as learning a function that maps from the text of an email to one of two categories, ""spam"" or ""not spam"".<sup class=""reference"" id=""cite_ref-83""><a href=""#cite_note-83"">[71]</a></sup>
In <a href=""/wiki/Reinforcement_learning"" title=""Reinforcement learning"">reinforcement learning</a> the agent is rewarded for good responses and punished for bad ones. The agent classifies its responses to form a strategy for operating in its problem space.<sup class=""reference"" id=""cite_ref-84""><a href=""#cite_note-84"">[72]</a></sup>
<a href=""/wiki/Transfer_learning"" title=""Transfer learning"">Transfer learning</a> is when the knowledge gained from one problem is applied to a new problem.<sup class=""reference"" id=""cite_ref-FOOTNOTEThe_Economist2016_85-0""><a href=""#cite_note-FOOTNOTEThe_Economist2016-85"">[73]</a></sup>
</p>, <p><a href=""/wiki/Computational_learning_theory"" title=""Computational learning theory"">Computational learning theory</a> can assess learners by <a href=""/wiki/Computational_complexity"" title=""Computational complexity"">computational complexity</a>, by <a href=""/wiki/Sample_complexity"" title=""Sample complexity"">sample complexity</a> (how much data is required), or by other notions of <a class=""mw-redirect"" href=""/wiki/Optimization_theory"" title=""Optimization theory"">optimization</a>.<sup class=""reference"" id=""cite_ref-FOOTNOTEJordanMitchell2015_86-0""><a href=""#cite_note-FOOTNOTEJordanMitchell2015-86"">[74]</a></sup>
</p>, <p>Natural language processing (NLP)<sup class=""reference"" id=""cite_ref-87""><a href=""#cite_note-87"">[75]</a></sup>
allows machines to read and <a href=""/wiki/Natural-language_understanding"" title=""Natural-language understanding"">understand</a> human language. A sufficiently powerful natural language processing system would enable <a href=""/wiki/Natural-language_user_interface"" title=""Natural-language user interface"">natural-language user interfaces</a> and the acquisition of knowledge directly from human-written sources, such as newswire texts. Some straightforward applications of NLP include <a href=""/wiki/Information_retrieval"" title=""Information retrieval"">information retrieval</a>, <a href=""/wiki/Question_answering"" title=""Question answering"">question answering</a> and <a href=""/wiki/Machine_translation"" title=""Machine translation"">machine translation</a>.<sup class=""reference"" id=""cite_ref-88""><a href=""#cite_note-88"">[76]</a></sup>
</p>, <p><a class=""mw-redirect"" href=""/wiki/Symbolic_AI"" title=""Symbolic AI"">Symbolic AI</a> used formal <a href=""/wiki/Syntax"" title=""Syntax"">syntax</a> to translate the <a class=""mw-redirect"" href=""/wiki/Deep_structure"" title=""Deep structure"">deep structure</a> of sentences into <a href=""/wiki/Logic"" title=""Logic"">logic</a>. This failed to produce useful applications, due to the <a class=""mw-redirect"" href=""/wiki/Intractability_(complexity)"" title=""Intractability (complexity)"">intractability</a> of logic<sup class=""reference"" id=""cite_ref-Intractability_59-1""><a href=""#cite_note-Intractability-59"">[49]</a></sup> and the breadth of commonsense knowledge.<sup class=""reference"" id=""cite_ref-Breadth_of_commonsense_knowledge_68-1""><a href=""#cite_note-Breadth_of_commonsense_knowledge-68"">[58]</a></sup> Modern statistical techniques include co-occurrence frequencies (how often one word appears near another), ""Keyword spotting"" (searching for a particular word to retrieve information), <a href=""/wiki/Transformer_(machine_learning_model)"" title=""Transformer (machine learning model)"">transformer</a>-based <a href=""/wiki/Deep_learning"" title=""Deep learning"">deep learning</a> (which finds patterns in text), and others.<sup class=""reference"" id=""cite_ref-89""><a href=""#cite_note-89"">[77]</a></sup> They have achieved acceptable accuracy at the page or paragraph level, and, by 2019, could generate coherent text.<sup class=""reference"" id=""cite_ref-FOOTNOTEVincent2019_90-0""><a href=""#cite_note-FOOTNOTEVincent2019-90"">[78]</a></sup></p>, <p>Machine perception<sup class=""reference"" id=""cite_ref-91""><a href=""#cite_note-91"">[79]</a></sup>
is the ability to use input from sensors (such as cameras, microphones, wireless signals, and active <a href=""/wiki/Lidar"" title=""Lidar"">lidar</a>, sonar, radar, and <a href=""/wiki/Tactile_sensor"" title=""Tactile sensor"">tactile sensors</a>) to deduce aspects of the world. Applications include <a href=""/wiki/Speech_recognition"" title=""Speech recognition"">speech recognition</a>,<sup class=""reference"" id=""cite_ref-92""><a href=""#cite_note-92"">[80]</a></sup>
<a href=""/wiki/Facial_recognition_system"" title=""Facial recognition system"">facial recognition</a>, and <a class=""mw-redirect"" href=""/wiki/Object_recognition"" title=""Object recognition"">object recognition</a>.<sup class=""reference"" id=""cite_ref-93""><a href=""#cite_note-93"">[81]</a></sup>
</p>, <p>
Computer vision is the ability to analyze visual input.<sup class=""reference"" id=""cite_ref-94""><a href=""#cite_note-94"">[82]</a></sup></p>, <p>AI is heavily used in robotics.<sup class=""reference"" id=""cite_ref-95""><a href=""#cite_note-95"">[83]</a></sup>
<a class=""mw-redirect"" href=""/wiki/Robot_localization"" title=""Robot localization"">Localization</a> is how a robot knows its location and <a href=""/wiki/Robotic_mapping"" title=""Robotic mapping"">maps</a> its environment. When given a small, static, and visible environment, this is easy; however, dynamic environments, such as (in <a href=""/wiki/Endoscopy"" title=""Endoscopy"">endoscopy</a>) the interior of a patient's breathing body, pose a greater challenge.<sup class=""reference"" id=""cite_ref-96""><a href=""#cite_note-96"">[84]</a></sup>
</p>, <p><a href=""/wiki/Motion_planning"" title=""Motion planning"">Motion planning</a> is the process of breaking down a movement task into ""primitives"" such as individual joint movements. Such movement often involves compliant motion, a process where movement requires maintaining physical contact with an object. Robots can learn from experience how to move efficiently despite the presence of friction and gear slippage.<sup class=""reference"" id=""cite_ref-97""><a href=""#cite_note-97"">[85]</a></sup></p>, <p>Affective computing is an interdisciplinary umbrella that comprises systems that recognize, interpret, process or simulate human <a href=""/wiki/Affect_(psychology)"" title=""Affect (psychology)"">feeling, emotion and mood</a>.<sup class=""reference"" id=""cite_ref-99""><a href=""#cite_note-99"">[87]</a></sup> 
For example, some <a href=""/wiki/Virtual_assistant"" title=""Virtual assistant"">virtual assistants</a> are programmed to speak conversationally or even to banter humorously; it makes them appear more sensitive to the emotional dynamics of human interaction, or to otherwise facilitate <a href=""/wiki/Human%E2%80%93computer_interaction"" title=""Human–computer interaction"">human–computer interaction</a>.
However, this tends to give naïve users an unrealistic conception of how intelligent existing computer agents actually are.<sup class=""reference"" id=""cite_ref-FOOTNOTEWaddell2018_100-0""><a href=""#cite_note-FOOTNOTEWaddell2018-100"">[88]</a></sup>
</p>, <p>
Moderate successes related to affective computing include textual <a href=""/wiki/Sentiment_analysis"" title=""Sentiment analysis"">sentiment analysis</a> and, more recently, <a href=""/wiki/Multimodal_sentiment_analysis"" title=""Multimodal sentiment analysis"">multimodal sentiment analysis</a>), wherein AI classifies the affects displayed by a videotaped subject.<sup class=""reference"" id=""cite_ref-FOOTNOTEPoriaCambriaBajpaiHussain2017_101-0""><a href=""#cite_note-FOOTNOTEPoriaCambriaBajpaiHussain2017-101"">[89]</a></sup></p>, <p>A machine with general intelligence can solve a wide variety of problems with breadth and versatility similar to human intelligence. There are several competing ideas about how to develop artificial general intelligence. <a href=""/wiki/Hans_Moravec"" title=""Hans Moravec"">Hans Moravec</a> and <a href=""/wiki/Marvin_Minsky"" title=""Marvin Minsky"">Marvin Minsky</a> argue that work in different individual domains can be incorporated into an advanced <a href=""/wiki/Multi-agent_system"" title=""Multi-agent system"">multi-agent system</a> or <a href=""/wiki/Cognitive_architecture"" title=""Cognitive architecture"">cognitive architecture</a> with general intelligence.<sup class=""reference"" id=""cite_ref-102""><a href=""#cite_note-102"">[90]</a></sup>
<a href=""/wiki/Pedro_Domingos"" title=""Pedro Domingos"">Pedro Domingos</a> hopes that there is a conceptually straightforward, but mathematically difficult, ""<a href=""/wiki/The_Master_Algorithm"" title=""The Master Algorithm"">master algorithm</a>"" that could lead to AGI.<sup class=""reference"" id=""cite_ref-FOOTNOTEDomingos2015Chpt._9_103-0""><a href=""#cite_note-FOOTNOTEDomingos2015Chpt._9-103"">[91]</a></sup>
Others believe that <a href=""/wiki/Anthropomorphism"" title=""Anthropomorphism"">anthropomorphic</a> features like an <a href=""/wiki/Artificial_brain"" title=""Artificial brain"">artificial brain</a><sup class=""reference"" id=""cite_ref-104""><a href=""#cite_note-104"">[92]</a></sup>
or simulated <a href=""/wiki/Developmental_robotics"" title=""Developmental robotics"">child development</a><sup class=""reference"" id=""cite_ref-105""><a href=""#cite_note-105"">[m]</a></sup>
will someday reach a critical point where general intelligence <a href=""/wiki/Emergence"" title=""Emergence"">emerges</a>.
</p>, <p>Many problems in AI can be solved theoretically by intelligently searching through many possible solutions:<sup class=""reference"" id=""cite_ref-106""><a href=""#cite_note-106"">[93]</a></sup>
<a href=""/wiki/Applications_of_artificial_intelligence#Deduction,_reasoning,_problem_solving"" title=""Applications of artificial intelligence"">Reasoning</a> can be reduced to performing a search. For example, logical proof can be viewed as searching for a path that leads from <a href=""/wiki/Premise"" title=""Premise"">premises</a> to <a href=""/wiki/Logical_consequence"" title=""Logical consequence"">conclusions</a>, where each step is the application of an <a class=""mw-redirect"" href=""/wiki/Inference_rule"" title=""Inference rule"">inference rule</a>.<sup class=""reference"" id=""cite_ref-Logic_as_search_107-0""><a href=""#cite_note-Logic_as_search-107"">[94]</a></sup>
<a href=""/wiki/Automated_planning_and_scheduling"" title=""Automated planning and scheduling"">Planning</a> algorithms search through trees of goals and subgoals, attempting to find a path to a target goal, a process called <a class=""mw-redirect"" href=""/wiki/Means-ends_analysis"" title=""Means-ends analysis"">means-ends analysis</a>.<sup class=""reference"" id=""cite_ref-Planning_as_search_108-0""><a href=""#cite_note-Planning_as_search-108"">[95]</a></sup>
<a href=""/wiki/Robotics"" title=""Robotics"">Robotics</a> algorithms for moving limbs and grasping objects use <a href=""/wiki/Local_search_(optimization)"" title=""Local search (optimization)"">local searches</a> in <a href=""/wiki/Configuration_space_(physics)"" title=""Configuration space (physics)"">configuration space</a>.<sup class=""reference"" id=""cite_ref-Configuration_space_109-0""><a href=""#cite_note-Configuration_space-109"">[96]</a></sup>
</p>, <p>Simple exhaustive searches<sup class=""reference"" id=""cite_ref-Uninformed_search_110-0""><a href=""#cite_note-Uninformed_search-110"">[97]</a></sup>
are rarely sufficient for most real-world problems: the <a href=""/wiki/Search_algorithm"" title=""Search algorithm"">search space</a> (the number of places to search) quickly grows to <a class=""mw-redirect"" href=""/wiki/Astronomically_large"" title=""Astronomically large"">astronomical numbers</a>. The result is a search that is <a class=""mw-redirect"" href=""/wiki/Computation_time"" title=""Computation time"">too slow</a> or never completes. The solution, for many problems, is to use ""<a class=""mw-redirect"" href=""/wiki/Heuristics"" title=""Heuristics"">heuristics</a>"" or ""rules of thumb"" that prioritize choices in favor of those more likely to reach a goal and to do so in a shorter number of steps. In some search methodologies, heuristics can also serve to eliminate some choices unlikely to lead to a goal (called ""<a class=""mw-redirect"" href=""/wiki/Pruning_(algorithm)"" title=""Pruning (algorithm)"">pruning</a> the <a href=""/wiki/Search_tree"" title=""Search tree"">search tree</a>""). <a class=""mw-redirect"" href=""/wiki/Heuristics"" title=""Heuristics"">Heuristics</a> supply the program with a ""best guess"" for the path on which the solution lies.<sup class=""reference"" id=""cite_ref-Informed_search_111-0""><a href=""#cite_note-Informed_search-111"">[98]</a></sup>
Heuristics limit the search for solutions into a smaller sample size.<sup class=""reference"" id=""cite_ref-FOOTNOTETecuci2012_112-0""><a href=""#cite_note-FOOTNOTETecuci2012-112"">[99]</a></sup>
</p>, <p>A very different kind of search came to prominence in the 1990s, based on the mathematical theory of <a class=""mw-redirect"" href=""/wiki/Optimization_(mathematics)"" title=""Optimization (mathematics)"">optimization</a>. For many problems, it is possible to begin the search with some form of a guess and then refine the guess incrementally until no more refinements can be made. These algorithms can be visualized as blind <a href=""/wiki/Hill_climbing"" title=""Hill climbing"">hill climbing</a>: we begin the search at a random point on the landscape, and then, by jumps or steps, we keep moving our guess uphill, until we reach the top. Other related optimization algorithms include <a href=""/wiki/Random_optimization"" title=""Random optimization"">random optimization</a>, <a href=""/wiki/Beam_search"" title=""Beam search"">beam search</a> and <a href=""/wiki/Metaheuristic#Local_search_vs._global_search"" title=""Metaheuristic"">metaheuristics</a> like <a href=""/wiki/Simulated_annealing"" title=""Simulated annealing"">simulated annealing</a>.<sup class=""reference"" id=""cite_ref-Optimization_search_113-0""><a href=""#cite_note-Optimization_search-113"">[100]</a></sup>
<a href=""/wiki/Evolutionary_computation"" title=""Evolutionary computation"">Evolutionary computation</a> uses a form of optimization search. For example, they may begin with a population of organisms (the guesses) and then allow them to mutate and recombine, <a class=""mw-redirect"" href=""/wiki/Artificial_selection"" title=""Artificial selection"">selecting</a> only the fittest to survive each generation (refining the guesses). Classic <a class=""mw-redirect"" href=""/wiki/Evolutionary_algorithms"" title=""Evolutionary algorithms"">evolutionary algorithms</a> include <a class=""mw-redirect"" href=""/wiki/Genetic_algorithms"" title=""Genetic algorithms"">genetic algorithms</a>, <a href=""/wiki/Gene_expression_programming"" title=""Gene expression programming"">gene expression programming</a>, and <a href=""/wiki/Genetic_programming"" title=""Genetic programming"">genetic programming</a>.<sup class=""reference"" id=""cite_ref-Genetic_programming_114-0""><a href=""#cite_note-Genetic_programming-114"">[101]</a></sup>
Alternatively, distributed search processes can coordinate via <a href=""/wiki/Swarm_intelligence"" title=""Swarm intelligence"">swarm intelligence</a> algorithms. Two popular swarm algorithms used in search are <a href=""/wiki/Particle_swarm_optimization"" title=""Particle swarm optimization"">particle swarm optimization</a> (inspired by bird <a href=""/wiki/Flocking_(behavior)"" title=""Flocking (behavior)"">flocking</a>) and <a class=""mw-redirect"" href=""/wiki/Ant_colony_optimization"" title=""Ant colony optimization"">ant colony optimization</a> (inspired by <a class=""mw-redirect"" href=""/wiki/Ant_trail"" title=""Ant trail"">ant trails</a>).<sup class=""reference"" id=""cite_ref-Society_based_learning_115-0""><a href=""#cite_note-Society_based_learning-115"">[102]</a></sup>
</p>, <p><a href=""/wiki/Logic"" title=""Logic"">Logic</a><sup class=""reference"" id=""cite_ref-Logic_116-0""><a href=""#cite_note-Logic-116"">[103]</a></sup>
is used for knowledge representation and problem-solving, but it can be applied to other problems as well. For example, the <a href=""/wiki/Satplan"" title=""Satplan"">satplan</a> algorithm uses logic for <a href=""/wiki/Automated_planning_and_scheduling"" title=""Automated planning and scheduling"">planning</a><sup class=""reference"" id=""cite_ref-Satplan_117-0""><a href=""#cite_note-Satplan-117"">[104]</a></sup>
and <a href=""/wiki/Inductive_logic_programming"" title=""Inductive logic programming"">inductive logic programming</a> is a method for <a href=""/wiki/Machine_learning"" title=""Machine learning"">learning</a>.<sup class=""reference"" id=""cite_ref-Symbolic_learning_techniques_118-0""><a href=""#cite_note-Symbolic_learning_techniques-118"">[105]</a></sup>
</p>, <p>Several different forms of logic are used in AI research. <a class=""mw-redirect"" href=""/wiki/Propositional_logic"" title=""Propositional logic"">Propositional logic</a><sup class=""reference"" id=""cite_ref-Propositional_logic_119-0""><a href=""#cite_note-Propositional_logic-119"">[106]</a></sup> involves <a href=""/wiki/Truth_function"" title=""Truth function"">truth functions</a> such as ""or"" and ""not"". <a href=""/wiki/First-order_logic"" title=""First-order logic"">First-order logic</a><sup class=""reference"" id=""cite_ref-First-order_logic_120-0""><a href=""#cite_note-First-order_logic-120"">[107]</a></sup>
adds <a href=""/wiki/Quantifier_(logic)"" title=""Quantifier (logic)"">quantifiers</a> and <a href=""/wiki/Predicate_(mathematical_logic)"" title=""Predicate (mathematical logic)"">predicates</a> and can express facts about objects, their properties, and their relations with each other. <a href=""/wiki/Fuzzy_logic"" title=""Fuzzy logic"">Fuzzy logic</a> assigns a ""degree of truth"" (between 0 and 1) to vague statements such as ""Alice is old"" (or rich, or tall, or hungry), that are too linguistically imprecise to be completely true or false.<sup class=""reference"" id=""cite_ref-Fuzzy_logic_121-0""><a href=""#cite_note-Fuzzy_logic-121"">[108]</a></sup>
<a href=""/wiki/Default_logic"" title=""Default logic"">Default logics</a>, <a href=""/wiki/Non-monotonic_logic"" title=""Non-monotonic logic"">non-monotonic logics</a> and <a href=""/wiki/Circumscription_(logic)"" title=""Circumscription (logic)"">circumscription</a> are forms of logic designed to help with default reasoning and the <a href=""/wiki/Qualification_problem"" title=""Qualification problem"">qualification problem</a>.<sup class=""reference"" id=""cite_ref-Default_reasoning_and_non-monotonic_logic_67-1""><a href=""#cite_note-Default_reasoning_and_non-monotonic_logic-67"">[57]</a></sup>
Several extensions of logic have been designed to handle specific domains of <a class=""mw-redirect"" href=""/wiki/Knowledge_representation"" title=""Knowledge representation"">knowledge</a>, such as <a href=""/wiki/Description_logic"" title=""Description logic"">description logics</a>;<sup class=""reference"" id=""cite_ref-Representing_categories_and_relations_63-2""><a href=""#cite_note-Representing_categories_and_relations-63"">[53]</a></sup>
<a href=""/wiki/Situation_calculus"" title=""Situation calculus"">situation calculus</a>, <a href=""/wiki/Event_calculus"" title=""Event calculus"">event calculus</a> and <a href=""/wiki/Fluent_calculus"" title=""Fluent calculus"">fluent calculus</a> (for representing events and time);<sup class=""reference"" id=""cite_ref-Representing_time_64-1""><a href=""#cite_note-Representing_time-64"">[54]</a></sup>
<a href=""/wiki/Causality#Causal_calculus"" title=""Causality"">causal calculus</a>;<sup class=""reference"" id=""cite_ref-Representing_causation_65-1""><a href=""#cite_note-Representing_causation-65"">[55]</a></sup>
<a href=""/wiki/Belief_revision"" title=""Belief revision"">belief calculus (belief revision)</a>; and <a href=""/wiki/Modal_logic"" title=""Modal logic"">modal logics</a>.<sup class=""reference"" id=""cite_ref-Representing_knowledge_about_knowledge_66-1""><a href=""#cite_note-Representing_knowledge_about_knowledge-66"">[56]</a></sup>
Logics to model contradictory or inconsistent statements arising in multi-agent systems have also been designed, such as <a href=""/wiki/Paraconsistent_logic"" title=""Paraconsistent logic"">paraconsistent logics</a>.<sup class=""noprint Inline-Template Template-Fact"" style=""white-space:nowrap;"">[<i><a href=""/wiki/Wikipedia:Citation_needed"" title=""Wikipedia:Citation needed""><span title=""This claim needs references to reliable sources. (September 2021)"">citation needed</span></a></i>]</sup>
</p>, <p>Many problems in AI (including in reasoning, planning, learning, perception, and robotics) require the agent to operate with incomplete or uncertain information. AI researchers have devised a number of tools to solve these problems using methods from <a href=""/wiki/Probability"" title=""Probability"">probability</a> theory and economics.<sup class=""reference"" id=""cite_ref-122""><a href=""#cite_note-122"">[109]</a></sup>
<a href=""/wiki/Bayesian_network"" title=""Bayesian network"">Bayesian networks</a><sup class=""reference"" id=""cite_ref-Bayesian_networks_123-0""><a href=""#cite_note-Bayesian_networks-123"">[110]</a></sup>
are a very general tool that can be used for various problems, including reasoning (using the <a href=""/wiki/Bayesian_inference"" title=""Bayesian inference"">Bayesian inference</a> algorithm),<sup class=""reference"" id=""cite_ref-125""><a href=""#cite_note-125"">[n]</a></sup><sup class=""reference"" id=""cite_ref-Bayesian_inference_126-0""><a href=""#cite_note-Bayesian_inference-126"">[112]</a></sup>
<a href=""/wiki/Machine_learning"" title=""Machine learning"">learning</a> (using the <a class=""mw-redirect"" href=""/wiki/Expectation-maximization_algorithm"" title=""Expectation-maximization algorithm"">expectation-maximization algorithm</a>),<sup class=""reference"" id=""cite_ref-128""><a href=""#cite_note-128"">[o]</a></sup><sup class=""reference"" id=""cite_ref-Bayesian_learning_129-0""><a href=""#cite_note-Bayesian_learning-129"">[114]</a></sup>
<a href=""/wiki/Automated_planning_and_scheduling"" title=""Automated planning and scheduling"">planning</a> (using <a class=""mw-redirect"" href=""/wiki/Decision_network"" title=""Decision network"">decision networks</a>)<sup class=""reference"" id=""cite_ref-Bayesian_decision_networks_130-0""><a href=""#cite_note-Bayesian_decision_networks-130"">[115]</a></sup> and <a href=""/wiki/Machine_perception"" title=""Machine perception"">perception</a> (using <a href=""/wiki/Dynamic_Bayesian_network"" title=""Dynamic Bayesian network"">dynamic Bayesian networks</a>).<sup class=""reference"" id=""cite_ref-Stochastic_temporal_models_131-0""><a href=""#cite_note-Stochastic_temporal_models-131"">[116]</a></sup>
Probabilistic algorithms can also be used for filtering, prediction, smoothing and finding explanations for streams of data, helping <a href=""/wiki/Machine_perception"" title=""Machine perception"">perception</a> systems to analyze processes that occur over time (e.g., <a href=""/wiki/Hidden_Markov_model"" title=""Hidden Markov model"">hidden Markov models</a> or <a href=""/wiki/Kalman_filter"" title=""Kalman filter"">Kalman filters</a>).<sup class=""reference"" id=""cite_ref-Stochastic_temporal_models_131-1""><a href=""#cite_note-Stochastic_temporal_models-131"">[116]</a></sup>
</p>, <p>A key concept from the science of economics is ""<a href=""/wiki/Utility"" title=""Utility"">utility</a>"", a measure of how valuable something is to an intelligent agent. Precise mathematical tools have been developed that analyze how an agent can make choices and plan, using <a href=""/wiki/Decision_theory"" title=""Decision theory"">decision theory</a>, <a href=""/wiki/Decision_analysis"" title=""Decision analysis"">decision analysis</a>,<sup class=""reference"" id=""cite_ref-Decisions_theory_and_analysis_132-0""><a href=""#cite_note-Decisions_theory_and_analysis-132"">[117]</a></sup>
and <a href=""/wiki/Applied_information_economics"" title=""Applied information economics"">information value theory</a>.<sup class=""reference"" id=""cite_ref-Information_value_theory_133-0""><a href=""#cite_note-Information_value_theory-133"">[118]</a></sup> These tools include models such as <a href=""/wiki/Markov_decision_process"" title=""Markov decision process"">Markov decision processes</a>,<sup class=""reference"" id=""cite_ref-Markov_decision_process_134-0""><a href=""#cite_note-Markov_decision_process-134"">[119]</a></sup> dynamic <a class=""mw-redirect"" href=""/wiki/Decision_network"" title=""Decision network"">decision networks</a>,<sup class=""reference"" id=""cite_ref-Stochastic_temporal_models_131-2""><a href=""#cite_note-Stochastic_temporal_models-131"">[116]</a></sup> <a href=""/wiki/Game_theory"" title=""Game theory"">game theory</a> and <a href=""/wiki/Mechanism_design"" title=""Mechanism design"">mechanism design</a>.<sup class=""reference"" id=""cite_ref-Game_theory_and_mechanism_design_135-0""><a href=""#cite_note-Game_theory_and_mechanism_design-135"">[120]</a></sup>
</p>, <p>The simplest AI applications can be divided into two types: classifiers (""if shiny then diamond"") and controllers (""if diamond then pick up""). Controllers do, however, also classify conditions before inferring actions, and therefore classification forms a central part of many AI systems. <a class=""mw-redirect"" href=""/wiki/Classifier_(mathematics)"" title=""Classifier (mathematics)"">Classifiers</a> are functions that use <a href=""/wiki/Pattern_matching"" title=""Pattern matching"">pattern matching</a> to determine the closest match. They can be tuned according to examples, making them very attractive for use in AI. These examples are known as observations or patterns. In supervised learning, each pattern belongs to a certain predefined class. A class is a decision that has to be made. All the observations combined with their class labels are known as a data set. When a new observation is received, that observation is classified based on previous experience.<sup class=""reference"" id=""cite_ref-136""><a href=""#cite_note-136"">[121]</a></sup>
</p>, <p>A classifier can be trained in various ways; there are many statistical and <a href=""/wiki/Machine_learning"" title=""Machine learning"">machine learning</a> approaches.
The <a href=""/wiki/Decision_tree_learning"" title=""Decision tree learning"">decision tree</a> is the simplest and most widely used symbolic machine learning algorithm.<sup class=""reference"" id=""cite_ref-137""><a href=""#cite_note-137"">[122]</a></sup>
<a class=""mw-redirect"" href=""/wiki/K-nearest_neighbor_algorithm"" title=""K-nearest neighbor algorithm"">K-nearest neighbor algorithm</a> was the most widely used analogical AI until the mid-1990s.<sup class=""reference"" id=""cite_ref-138""><a href=""#cite_note-138"">[123]</a></sup>
<a class=""mw-redirect"" href=""/wiki/Kernel_methods"" title=""Kernel methods"">Kernel methods</a> such as the <a class=""mw-redirect"" href=""/wiki/Support_vector_machine"" title=""Support vector machine"">support vector machine</a> (SVM) displaced k-nearest neighbor in the 1990s.<sup class=""reference"" id=""cite_ref-139""><a href=""#cite_note-139"">[124]</a></sup>
The <a href=""/wiki/Naive_Bayes_classifier"" title=""Naive Bayes classifier"">naive Bayes classifier</a> is reportedly the ""most widely used learner""<sup class=""reference"" id=""cite_ref-FOOTNOTEDomingos2015152_140-0""><a href=""#cite_note-FOOTNOTEDomingos2015152-140"">[125]</a></sup> at Google, due in part to its scalability.<sup class=""reference"" id=""cite_ref-141""><a href=""#cite_note-141"">[126]</a></sup>
<a href=""/wiki/Artificial_neural_network"" title=""Artificial neural network"">Neural networks</a> are also used for classification.<sup class=""reference"" id=""cite_ref-Neural_networks_142-0""><a href=""#cite_note-Neural_networks-142"">[127]</a></sup>
</p>, <p>Classifier performance depends greatly on the characteristics of the data to be classified, such as the dataset size, distribution of samples across classes, dimensionality, and the level of noise. Model-based classifiers perform well if the assumed model is an extremely good fit for the actual data. Otherwise, if no matching model is available, and if accuracy (rather than speed or scalability) is the sole concern, conventional wisdom is that discriminative classifiers (especially SVM) tend to be more accurate than model-based classifiers such as ""naive Bayes"" on most practical data sets.<sup class=""reference"" id=""cite_ref-143""><a href=""#cite_note-143"">[128]</a></sup>
</p>, <p><a href=""/wiki/Artificial_neural_network"" title=""Artificial neural network"">Neural networks</a><sup class=""reference"" id=""cite_ref-Neural_networks_142-1""><a href=""#cite_note-Neural_networks-142"">[127]</a></sup>
were inspired by the architecture of neurons in the human brain. A simple ""neuron"" <i>N</i> accepts input from other neurons, each of which, when activated (or ""fired""), casts a weighted ""vote"" for or against whether neuron <i>N</i> should itself activate. Learning requires an algorithm to adjust these weights based on the training data; one simple algorithm (dubbed ""<a class=""mw-redirect"" href=""/wiki/Hebbian_learning"" title=""Hebbian learning"">fire together, wire together</a>"") is to increase the weight between two connected neurons when the activation of one triggers the successful activation of another. Neurons have a continuous spectrum of activation; in addition, neurons can process inputs in a nonlinear way rather than weighing straightforward votes.
</p>, <p>Modern neural networks model complex relationships between inputs and outputs and <a href=""/wiki/Pattern_recognition"" title=""Pattern recognition"">find patterns</a> in data. They can learn continuous functions and even digital logical operations. Neural networks can be viewed as a type of <a href=""/wiki/Mathematical_optimization"" title=""Mathematical optimization"">mathematical optimization</a> — they perform <a href=""/wiki/Gradient_descent"" title=""Gradient descent"">gradient descent</a> on a multi-dimensional topology that was created by <a href=""/wiki/Machine_learning"" title=""Machine learning"">training</a> the network. The most common training technique is the <a href=""/wiki/Backpropagation"" title=""Backpropagation"">backpropagation</a> algorithm.<sup class=""reference"" id=""cite_ref-Backpropagation_144-0""><a href=""#cite_note-Backpropagation-144"">[129]</a></sup>
Other <a href=""/wiki/Machine_learning"" title=""Machine learning"">learning</a> techniques for neural networks are <a class=""mw-redirect"" href=""/wiki/Hebbian_learning"" title=""Hebbian learning"">Hebbian learning</a> (""fire together, wire together""), <a class=""mw-redirect"" href=""/wiki/GMDH"" title=""GMDH"">GMDH</a> or <a href=""/wiki/Competitive_learning"" title=""Competitive learning"">competitive learning</a>.<sup class=""reference"" id=""cite_ref-Learning_in_neural_networks_145-0""><a href=""#cite_note-Learning_in_neural_networks-145"">[130]</a></sup>
</p>, <p>The main categories of networks are acyclic or <a href=""/wiki/Feedforward_neural_network"" title=""Feedforward neural network"">feedforward neural networks</a> (where the signal passes in only one direction) and <a href=""/wiki/Recurrent_neural_network"" title=""Recurrent neural network"">recurrent neural networks</a> (which allow feedback and short-term memories of previous input events). Among the most popular feedforward networks are <a href=""/wiki/Perceptron"" title=""Perceptron"">perceptrons</a>, <a class=""mw-redirect"" href=""/wiki/Multi-layer_perceptron"" title=""Multi-layer perceptron"">multi-layer perceptrons</a> and <a class=""mw-redirect"" href=""/wiki/Radial_basis_network"" title=""Radial basis network"">radial basis networks</a>.<sup class=""reference"" id=""cite_ref-Feedforward_neural_networks_146-0""><a href=""#cite_note-Feedforward_neural_networks-146"">[131]</a></sup>
</p>, <p><a href=""/wiki/Deep_learning"" title=""Deep learning"">Deep learning</a><sup class=""reference"" id=""cite_ref-148""><a href=""#cite_note-148"">[133]</a></sup>
uses several layers of neurons between the network's inputs and outputs. The multiple layers can progressively extract higher-level features from the raw input. For example, in <a class=""mw-redirect"" href=""/wiki/Image_processing"" title=""Image processing"">image processing</a>, lower layers may identify edges, while higher layers may identify the concepts relevant to a human such as digits or letters or faces.<sup class=""reference"" id=""cite_ref-FOOTNOTEDengYu2014199–200_149-0""><a href=""#cite_note-FOOTNOTEDengYu2014199–200-149"">[134]</a></sup> Deep learning has drastically improved the performance of programs in many important subfields of artificial intelligence, including <a href=""/wiki/Computer_vision"" title=""Computer vision"">computer vision</a>, <a href=""/wiki/Speech_recognition"" title=""Speech recognition"">speech recognition</a>, <a class=""mw-redirect"" href=""/wiki/Image_classification"" title=""Image classification"">image classification</a><sup class=""reference"" id=""cite_ref-FOOTNOTECiresanMeierSchmidhuber2012_150-0""><a href=""#cite_note-FOOTNOTECiresanMeierSchmidhuber2012-150"">[135]</a></sup> and others.
</p>, <p>Deep learning often uses <a href=""/wiki/Convolutional_neural_network"" title=""Convolutional neural network"">convolutional neural networks</a> for many or all of its layers. In a convolutional layer, each neuron receives input from only a restricted area of the previous layer called the neuron's <a href=""/wiki/Receptive_field"" title=""Receptive field"">receptive field</a>. This can substantially reduce the number of weighted connections between neurons,<sup class=""reference"" id=""cite_ref-FOOTNOTEHabibi2017_151-0""><a href=""#cite_note-FOOTNOTEHabibi2017-151"">[136]</a></sup> and creates a hierarchy similar to the organization of the animal visual cortex.<sup class=""reference"" id=""cite_ref-FOOTNOTEFukushima2007_152-0""><a href=""#cite_note-FOOTNOTEFukushima2007-152"">[137]</a></sup>
</p>, <p>In a <a href=""/wiki/Recurrent_neural_network"" title=""Recurrent neural network"">recurrent neural network</a> the signal will propagate through a layer more than once;<sup class=""reference"" id=""cite_ref-153""><a href=""#cite_note-153"">[138]</a></sup> 
thus, an RNN is an example of deep learning.<sup class=""reference"" id=""cite_ref-FOOTNOTESchmidhuber2015_154-0""><a href=""#cite_note-FOOTNOTESchmidhuber2015-154"">[139]</a></sup>
RNNs can be trained by <a href=""/wiki/Gradient_descent"" title=""Gradient descent"">gradient descent</a>,<sup class=""reference"" id=""cite_ref-155""><a href=""#cite_note-155"">[140]</a></sup>
however long-term gradients which are back-propagated can ""vanish"" (that is, they can tend to zero) or ""explode"" (that is, they can tend to infinity), known as the <a href=""/wiki/Vanishing_gradient_problem"" title=""Vanishing gradient problem"">vanishing gradient problem</a>.<sup class=""reference"" id=""cite_ref-156""><a href=""#cite_note-156"">[141]</a></sup>
The <a class=""mw-redirect"" href=""/wiki/Long_short_term_memory"" title=""Long short term memory"">long short term memory</a> (LSTM) technique can prevent this in most cases.<sup class=""reference"" id=""cite_ref-157""><a href=""#cite_note-157"">[142]</a></sup>
</p>, <p>Specialized languages for artificial intelligence have been developed, such as <a href=""/wiki/Lisp_(programming_language)"" title=""Lisp (programming language)"">Lisp</a>, <a href=""/wiki/Prolog"" title=""Prolog"">Prolog</a>, <a href=""/wiki/TensorFlow"" title=""TensorFlow"">TensorFlow</a> and many others. Hardware developed for AI includes <a href=""/wiki/AI_accelerator"" title=""AI accelerator"">AI accelerators</a> and <a class=""mw-redirect"" href=""/wiki/Neuromorphic_computing"" title=""Neuromorphic computing"">neuromorphic computing</a>.
</p>, <p>AI is relevant to any intellectual task.<sup class=""reference"" id=""cite_ref-FOOTNOTERussellNorvig20091_158-0""><a href=""#cite_note-FOOTNOTERussellNorvig20091-158"">[143]</a></sup>
Modern artificial intelligence techniques are pervasive and are too numerous to list here.<sup class=""reference"" id=""cite_ref-FOOTNOTEEuropean_Commission20201_159-0""><a href=""#cite_note-FOOTNOTEEuropean_Commission20201-159"">[144]</a></sup>
Frequently, when a technique reaches mainstream use, it is no longer considered artificial intelligence; this phenomenon is described as the <a href=""/wiki/AI_effect"" title=""AI effect"">AI effect</a>.<sup class=""reference"" id=""cite_ref-FOOTNOTECNN2006_160-0""><a href=""#cite_note-FOOTNOTECNN2006-160"">[145]</a></sup>
</p>, <p>In the 2010s, AI applications were at the heart of the most commercially successful areas of computing, and have become a ubiquitous feature of daily life. AI is used in <a class=""mw-redirect"" href=""/wiki/Search_engines"" title=""Search engines"">search engines</a> (such as <a href=""/wiki/Google_Search"" title=""Google Search"">Google Search</a>),
<a href=""/wiki/Targeted_advertising"" title=""Targeted advertising"">targeting online advertisements</a>,<sup class=""reference"" id=""cite_ref-161""><a href=""#cite_note-161"">[146]</a></sup><sup class=""noprint Inline-Template noprint Template-Fact"" style=""white-space:nowrap;"">[<i><a href=""/wiki/Wikipedia:No_original_research#Primary,_secondary_and_tertiary_sources"" title=""Wikipedia:No original research""><span title=""This claim needs references to reliable secondary sources. (October 2021)"">non-primary source needed</span></a></i>]</sup>
<a href=""/wiki/Recommender_system"" title=""Recommender system"">recommendation systems</a> (offered by <a href=""/wiki/Netflix"" title=""Netflix"">Netflix</a>, <a href=""/wiki/YouTube"" title=""YouTube"">YouTube</a> or <a href=""/wiki/Amazon_(company)"" title=""Amazon (company)"">Amazon</a>),
driving <a href=""/wiki/Internet_traffic"" title=""Internet traffic"">internet traffic</a>,<sup class=""reference"" id=""cite_ref-FOOTNOTELohr2016_162-0""><a href=""#cite_note-FOOTNOTELohr2016-162"">[147]</a></sup><sup class=""reference"" id=""cite_ref-FOOTNOTESmith2016_163-0""><a href=""#cite_note-FOOTNOTESmith2016-163"">[148]</a></sup>
<a href=""/wiki/Marketing_and_artificial_intelligence"" title=""Marketing and artificial intelligence"">targeted advertising</a> (<a class=""mw-redirect"" href=""/wiki/AdSense"" title=""AdSense"">AdSense</a>, <a href=""/wiki/Facebook"" title=""Facebook"">Facebook</a>),
<a href=""/wiki/Virtual_assistant"" title=""Virtual assistant"">virtual assistants</a> (such as <a href=""/wiki/Siri"" title=""Siri"">Siri</a> or <a href=""/wiki/Amazon_Alexa"" title=""Amazon Alexa"">Alexa</a>),<sup class=""reference"" id=""cite_ref-FOOTNOTERowinski2013_164-0""><a href=""#cite_note-FOOTNOTERowinski2013-164"">[149]</a></sup>
<a class=""mw-redirect"" href=""/wiki/Autonomous_vehicles"" title=""Autonomous vehicles"">autonomous vehicles</a> (including <a href=""/wiki/Unmanned_aerial_vehicle"" title=""Unmanned aerial vehicle"">drones</a> and <a class=""mw-redirect"" href=""/wiki/Self-driving_cars"" title=""Self-driving cars"">self-driving cars</a>),
<a href=""/wiki/Machine_translation"" title=""Machine translation"">automatic language translation</a> (<a href=""/wiki/Microsoft_Translator"" title=""Microsoft Translator"">Microsoft Translator</a>, <a href=""/wiki/Google_Translate"" title=""Google Translate"">Google Translate</a>),
<a href=""/wiki/Facial_recognition_system"" title=""Facial recognition system"">facial recognition</a> (<a class=""mw-redirect"" href=""/wiki/Apple_Computer"" title=""Apple Computer"">Apple</a>'s <a href=""/wiki/Face_ID"" title=""Face ID"">Face ID</a> or <a href=""/wiki/Microsoft"" title=""Microsoft"">Microsoft</a>'s <a href=""/wiki/DeepFace"" title=""DeepFace"">DeepFace</a>),
<a href=""/wiki/Automatic_image_annotation"" title=""Automatic image annotation"">image labeling</a> (used by <a href=""/wiki/Facebook"" title=""Facebook"">Facebook</a>, <a class=""mw-redirect"" href=""/wiki/Apple_Computer"" title=""Apple Computer"">Apple</a>'s <a href=""/wiki/IPhoto"" title=""IPhoto"">iPhoto</a> and <a href=""/wiki/TikTok"" title=""TikTok"">TikTok</a>)
and <a class=""mw-redirect"" href=""/wiki/Spam_filtering"" title=""Spam filtering"">spam filtering</a>.
</p>, <p>There are also thousands of successful AI applications used to solve problems for specific industries or institutions. A few examples are 
<a href=""/wiki/Energy_storage"" title=""Energy storage"">energy storage</a>,<sup class=""reference"" id=""cite_ref-FOOTNOTEFrangoul2019_165-0""><a href=""#cite_note-FOOTNOTEFrangoul2019-165"">[150]</a></sup>
<a href=""/wiki/Deepfake"" title=""Deepfake"">deepfakes</a>,<sup class=""reference"" id=""cite_ref-FOOTNOTEBrown2019_166-0""><a href=""#cite_note-FOOTNOTEBrown2019-166"">[151]</a></sup>
medical diagnosis, 
military logistics, or 
supply chain management.
</p>, <p><a class=""mw-redirect"" href=""/wiki/Game_AI"" title=""Game AI"">Game playing</a> has been a test of AI's strength since the 1950s. <a class=""mw-redirect"" href=""/wiki/IBM_Deep_Blue"" title=""IBM Deep Blue"">Deep Blue</a> became the first computer chess-playing system to beat a reigning world chess champion, <a href=""/wiki/Garry_Kasparov"" title=""Garry Kasparov"">Garry Kasparov</a>, on 11 May 1997.<sup class=""reference"" id=""cite_ref-FOOTNOTEMcCorduck2004480–483_167-0""><a href=""#cite_note-FOOTNOTEMcCorduck2004480–483-167"">[152]</a></sup> 
In 2011, in a <i><a href=""/wiki/Jeopardy!"" title=""Jeopardy!"">Jeopardy!</a></i> <a class=""mw-redirect"" href=""/wiki/Quiz_show"" title=""Quiz show"">quiz show</a> exhibition match, <a href=""/wiki/IBM"" title=""IBM"">IBM</a>'s <a class=""mw-redirect"" href=""/wiki/Question_answering_system"" title=""Question answering system"">question answering system</a>, <a class=""mw-redirect"" href=""/wiki/Watson_(artificial_intelligence_software)"" title=""Watson (artificial intelligence software)"">Watson</a>, defeated the two greatest <i>Jeopardy!</i> champions, <a href=""/wiki/Brad_Rutter"" title=""Brad Rutter"">Brad Rutter</a> and <a href=""/wiki/Ken_Jennings"" title=""Ken Jennings"">Ken Jennings</a>, by a significant margin.<sup class=""reference"" id=""cite_ref-FOOTNOTEMarkoff2011_168-0""><a href=""#cite_note-FOOTNOTEMarkoff2011-168"">[153]</a></sup> 
In March 2016, <a href=""/wiki/AlphaGo"" title=""AlphaGo"">AlphaGo</a> won 4 out of 5 games of <a href=""/wiki/Go_(game)"" title=""Go (game)"">Go</a> in a match with Go champion <a href=""/wiki/Lee_Sedol"" title=""Lee Sedol"">Lee Sedol</a>, becoming the first <a href=""/wiki/Computer_Go"" title=""Computer Go"">computer Go</a>-playing system to beat a professional Go player without <a class=""mw-redirect"" href=""/wiki/Go_handicaps"" title=""Go handicaps"">handicaps</a>.<sup class=""reference"" id=""cite_ref-169""><a href=""#cite_note-169"">[154]</a></sup>
Other programs handle <a class=""mw-redirect"" href=""/wiki/Imperfect_information"" title=""Imperfect information"">imperfect-information</a> games; such as for <a href=""/wiki/Poker"" title=""Poker"">poker</a> at a superhuman level, <a href=""/wiki/Pluribus_(poker_bot)"" title=""Pluribus (poker bot)"">Pluribus</a><sup class=""reference"" id=""cite_ref-171""><a href=""#cite_note-171"">[p]</a></sup>
and <a href=""/wiki/Cepheus_(poker_bot)"" title=""Cepheus (poker bot)"">Cepheus</a>.<sup class=""reference"" id=""cite_ref-FOOTNOTEBowlingBurchJohansonTammelin2015_172-0""><a href=""#cite_note-FOOTNOTEBowlingBurchJohansonTammelin2015-172"">[156]</a></sup>
<a href=""/wiki/DeepMind"" title=""DeepMind"">DeepMind</a> in the 2010s developed a ""generalized artificial intelligence"" that could learn many diverse <a href=""/wiki/Atari_2600"" title=""Atari 2600"">Atari</a> games on its own.<sup class=""reference"" id=""cite_ref-FOOTNOTESample2017_173-0""><a href=""#cite_note-FOOTNOTESample2017-173"">[157]</a></sup>
</p>, <p>By 2020, <a class=""mw-redirect"" href=""/wiki/Natural_Language_Processing"" title=""Natural Language Processing"">Natural Language Processing</a> systems such as the enormous <a href=""/wiki/GPT-3"" title=""GPT-3"">GPT-3</a> (then by far the largest artificial neural network) were matching human performance on pre-existing benchmarks, albeit without the system attaining a commonsense understanding of the contents of the benchmarks.<sup class=""reference"" id=""cite_ref-FOOTNOTEAnadiotis2020_174-0""><a href=""#cite_note-FOOTNOTEAnadiotis2020-174"">[158]</a></sup>
DeepMind's <a class=""mw-redirect"" href=""/wiki/AlphaFold_2"" title=""AlphaFold 2"">AlphaFold 2</a> (2020) demonstrated the ability to approximate, in hours rather than months, the 3D structure of a protein.<sup class=""reference"" id=""cite_ref-FOOTNOTEHeath2020_175-0""><a href=""#cite_note-FOOTNOTEHeath2020-175"">[159]</a></sup>
Other applications predict the result of judicial decisions,<sup class=""reference"" id=""cite_ref-FOOTNOTEAletras_''et_al.''2016_176-0""><a href=""#cite_note-FOOTNOTEAletras_''et_al.''2016-176"">[160]</a></sup> <a href=""/wiki/Computer_art"" title=""Computer art"">create art</a> (such as poetry or painting) and <a class=""mw-redirect"" href=""/wiki/Automated_theorem_prover"" title=""Automated theorem prover"">prove mathematical theorems</a>.
</p>, <p>In 2019, <a class=""mw-redirect"" href=""/wiki/WIPO"" title=""WIPO"">WIPO</a> reported that AI was the most prolific <a href=""/wiki/Emerging_technologies"" title=""Emerging technologies"">emerging technology</a> in terms of number of <a href=""/wiki/Patent"" title=""Patent"">patent</a> applications and granted patents, the <a href=""/wiki/Internet_of_things"" title=""Internet of things"">Internet of things</a> was estimated to be the largest in terms of market size. It was followed, again in market size, by big data technologies, robotics, AI, 3D printing and the fifth generation of mobile services (5G).<sup class=""reference"" id=""cite_ref-177""><a href=""#cite_note-177"">[161]</a></sup> Since AI emerged in the 1950s, 340000 AI-related patent applications were filed by innovators and 1.6 million scientific papers have been published by researchers, with the majority of all AI-related patent filings published since 2013. Companies represent 26 out of the top 30 AI patent applicants, with universities or public research organizations accounting for the remaining four.<sup class=""reference"" id=""cite_ref-auto_178-0""><a href=""#cite_note-auto-178"">[162]</a></sup> The ratio of scientific papers to inventions has significantly decreased from 8:1 in 2010 to 3:1 in 2016, which is attributed to be indicative of a shift from theoretical research to the use of AI technologies in commercial products and services. <a href=""/wiki/Machine_learning"" title=""Machine learning"">Machine learning</a> is the dominant AI technique disclosed in patents and is included in more than one-third of all identified inventions (134777 machine learning patents filed for a total of 167038 AI patents filed in 2016), with <a href=""/wiki/Computer_vision"" title=""Computer vision"">computer vision</a> being the most popular functional application. AI-related patents not only disclose AI techniques and applications, they often also refer to an application field or industry. Twenty application fields were identified in 2016 and included, in order of magnitude: telecommunications (15 percent), transportation (15 percent), life and medical sciences (12 percent), and personal devices, computing and human–computer interaction (11 percent). Other sectors included banking, entertainment, security, industry and manufacturing, agriculture, and networks (including social networks, smart cities and the Internet of things). IBM has the largest portfolio of AI patents with 8,290 patent applications, followed by Microsoft with 5,930 patent applications.<sup class=""reference"" id=""cite_ref-auto_178-1""><a href=""#cite_note-auto-178"">[162]</a></sup>
</p>, <p><a href=""/wiki/Alan_Turing"" title=""Alan Turing"">Alan Turing</a> wrote in 1950 ""I propose to consider the question 'can machines think'?""<sup class=""reference"" id=""cite_ref-FOOTNOTETuring19501_179-0""><a href=""#cite_note-FOOTNOTETuring19501-179"">[163]</a></sup>
He advised changing the question from whether a machine ""thinks"", to ""whether or not it is possible for machinery to show intelligent behaviour"".<sup class=""reference"" id=""cite_ref-FOOTNOTETuring1948_180-0""><a href=""#cite_note-FOOTNOTETuring1948-180"">[164]</a></sup> 
The only thing visible is the behavior of the machine, so it does not matter if the machine is <a href=""/wiki/Consciousness"" title=""Consciousness"">conscious</a>, or has a <a href=""/wiki/Mind"" title=""Mind"">mind</a>, or whether the intelligence is merely a ""simulation"" and not ""the real thing"".  He noted that we also don't know these things about other people, but that we extend a ""polite convention"" that they are actually ""thinking"". This idea forms the basis of the <a href=""/wiki/Turing_test"" title=""Turing test"">Turing test</a>.<sup class=""reference"" id=""cite_ref-Turing_test_181-0""><a href=""#cite_note-Turing_test-181"">[165]</a></sup><sup class=""reference"" id=""cite_ref-183""><a href=""#cite_note-183"">[q]</a></sup>
</p>, <p>AI founder <a href=""/wiki/John_McCarthy_(computer_scientist)"" title=""John McCarthy (computer scientist)"">John McCarthy</a> said: ""Artificial intelligence is not, by definition, simulation of human intelligence"".<sup class=""reference"" id=""cite_ref-FOOTNOTEMaker2006_184-0""><a href=""#cite_note-FOOTNOTEMaker2006-184"">[167]</a></sup> <a href=""/wiki/Stuart_J._Russell"" title=""Stuart J. Russell"">Russell</a> and <a href=""/wiki/Peter_Norvig"" title=""Peter Norvig"">Norvig</a> agree and criticize the Turing test. They wrote: ""<a href=""/wiki/Aeronautics"" title=""Aeronautics"">Aeronautical engineering</a> texts do not define the goal of their field as 'making machines that fly so exactly like <a class=""mw-redirect"" href=""/wiki/Pigeon"" title=""Pigeon"">pigeons</a> that they can fool other pigeons.<span style=""padding-right:.15em;"">'</span>""<sup class=""reference"" id=""cite_ref-FOOTNOTERussellNorvig20033_182-1""><a href=""#cite_note-FOOTNOTERussellNorvig20033-182"">[166]</a></sup> Other researchers and analysts disagree and have argued that AI should simulate natural intelligence by studying <a href=""/wiki/Psychology"" title=""Psychology"">psychology</a> or <a href=""/wiki/Neuroscience"" title=""Neuroscience"">neurobiology</a>.<sup class=""reference"" id=""cite_ref-Biological_intelligence_vs._intelligence_in_general_186-0""><a href=""#cite_note-Biological_intelligence_vs._intelligence_in_general-186"">[r]</a></sup>
</p>, <p>The <a href=""/wiki/Intelligent_agent"" title=""Intelligent agent"">intelligent agent</a> paradigm<sup class=""reference"" id=""cite_ref-Intelligent_agents_187-0""><a href=""#cite_note-Intelligent_agents-187"">[169]</a></sup>
defines intelligent behavior in general, without reference to human beings. An <a href=""/wiki/Intelligent_agent"" title=""Intelligent agent"">intelligent agent</a> is a system that perceives its environment and takes actions that maximize its chances of success. Any system that has goal-directed behavior can be analyzed as an intelligent agent: something as simple as a thermostat, as complex as a human being, as well as large systems such as <a class=""mw-redirect"" href=""/wiki/Firm"" title=""Firm"">firms</a>, <a href=""/wiki/Biome"" title=""Biome"">biomes</a> or <a href=""/wiki/Nation"" title=""Nation"">nations</a>. The intelligent agent paradigm became widely accepted during the 1990s, and currently serves as the definition of the field.<sup class=""reference"" id=""cite_ref-Definition_of_AI_1-1""><a href=""#cite_note-Definition_of_AI-1"">[a]</a></sup>
</p>, <p>The paradigm has other advantages for AI. It provides a reliable and scientific way to test programs; researchers can directly compare or even combine different approaches to isolated problems, by asking which agent is best at maximizing a given ""goal function"".  It also gives them a common language to communicate with other fields — such as <a href=""/wiki/Mathematical_optimization"" title=""Mathematical optimization"">mathematical optimization</a> (which is defined in terms of ""goals"") or <a href=""/wiki/Economics"" title=""Economics"">economics</a> (which uses the same definition of a ""<a href=""/wiki/Rational_agent"" title=""Rational agent"">rational agent</a>"").<sup class=""reference"" id=""cite_ref-FOOTNOTERussellNorvig200327_188-0""><a href=""#cite_note-FOOTNOTERussellNorvig200327-188"">[170]</a></sup>
</p>, <p>No established unifying theory or <a href=""/wiki/Paradigm"" title=""Paradigm"">paradigm</a> has guided AI research for most of its history.<sup class=""reference"" id=""cite_ref-190""><a href=""#cite_note-190"">[s]</a></sup> The unprecedented success of statistical machine learning in the 2010s eclipsed all other approaches (so much so that some sources, especially in the business world, use the term ""artificial intelligence"" to mean ""machine learning with neural networks""). This approach is mostly <a class=""mw-redirect"" href=""/wiki/Sub-symbolic"" title=""Sub-symbolic"">sub-symbolic</a>, <a href=""/wiki/Neats_and_scruffies"" title=""Neats and scruffies"">neat</a>, <a href=""/wiki/Soft_computing"" title=""Soft computing"">soft</a> and <a href=""/wiki/Artificial_general_intelligence"" title=""Artificial general intelligence"">narrow</a> (see below). Critics argue that these questions may have to be revisited by future generations of AI researchers.
</p>, <p><a class=""mw-redirect"" href=""/wiki/Symbolic_AI"" title=""Symbolic AI"">Symbolic AI</a> (or ""<a class=""mw-redirect"" href=""/wiki/GOFAI"" title=""GOFAI"">GOFAI</a>"")<sup class=""reference"" id=""cite_ref-FOOTNOTEHaugeland1985112–117_191-0""><a href=""#cite_note-FOOTNOTEHaugeland1985112–117-191"">[172]</a></sup> simulated the high-level conscious reasoning that people use when they solve puzzles, express legal reasoning and do mathematics. They were highly successful at ""intelligent"" tasks such as algebra or IQ tests. In the 1960s, Newell and Simon proposed the <a class=""mw-redirect"" href=""/wiki/Physical_symbol_systems_hypothesis"" title=""Physical symbol systems hypothesis"">physical symbol systems hypothesis</a>: ""A physical symbol system has the necessary and sufficient means of general intelligent action.""<sup class=""reference"" id=""cite_ref-Physical_symbol_system_hypothesis_192-0""><a href=""#cite_note-Physical_symbol_system_hypothesis-192"">[173]</a></sup>
</p>, <p>However, the symbolic approach failed dismally on many tasks that humans solve easily, such as learning, recognizing an object or commonsense reasoning. <a href=""/wiki/Moravec%27s_paradox"" title=""Moravec's paradox"">Moravec's paradox</a> is the discovery that high-level ""intelligent"" tasks were easy for AI, but low level ""instinctive"" tasks were extremely difficult.<sup class=""reference"" id=""cite_ref-193""><a href=""#cite_note-193"">[174]</a></sup>
Philosopher <a href=""/wiki/Hubert_Dreyfus"" title=""Hubert Dreyfus"">Hubert Dreyfus</a> had argued since the 1960s that human expertise depends on unconscious instinct rather than conscious symbol manipulation, and on having a ""feel"" for the situation, rather than explicit symbolic knowledge.<sup class=""reference"" id=""cite_ref-Dreyfus'_critique_194-0""><a href=""#cite_note-Dreyfus'_critique-194"">[175]</a></sup>
Although his arguments had been ridiculed and ignored when they were first presented, eventually, AI research came to agree.<sup class=""reference"" id=""cite_ref-196""><a href=""#cite_note-196"">[t]</a></sup><sup class=""reference"" id=""cite_ref-Psychological_evidence_of_sub-symbolic_reasoning_60-2""><a href=""#cite_note-Psychological_evidence_of_sub-symbolic_reasoning-60"">[50]</a></sup>
</p>, <p>The issue is not resolved: <a class=""mw-redirect"" href=""/wiki/Sub-symbolic"" title=""Sub-symbolic"">sub-symbolic</a> reasoning can make many of the same inscrutable mistakes that human intuition does, such as <a href=""/wiki/Algorithmic_bias"" title=""Algorithmic bias"">algorithmic bias</a>. Critics such as  <a href=""/wiki/Noam_Chomsky"" title=""Noam Chomsky"">Noam Chomsky</a> argue continuing research into symbolic AI will still be necessary to attain general intelligence,<sup class=""reference"" id=""cite_ref-FOOTNOTELangley2011_197-0""><a href=""#cite_note-FOOTNOTELangley2011-197"">[177]</a></sup><sup class=""reference"" id=""cite_ref-FOOTNOTEKatz2012_198-0""><a href=""#cite_note-FOOTNOTEKatz2012-198"">[178]</a></sup> in part because sub-symbolic AI is a move away from <a class=""mw-redirect"" href=""/wiki/Explainable_AI"" title=""Explainable AI"">explainable AI</a>: it can be difficult or impossible to understand why a modern statistical AI program made a particular decision.
</p>, <p>""Neats"" hope that intelligent behavior is described using simple, elegant principles (such as <a href=""/wiki/Logic"" title=""Logic"">logic</a>, <a class=""mw-redirect"" href=""/wiki/Optimization_(mathematics)"" title=""Optimization (mathematics)"">optimization</a>, or <a class=""mw-redirect"" href=""/wiki/Neural_networks"" title=""Neural networks"">neural networks</a>). ""Scruffies"" expect that it necessarily requires solving a large number of unrelated problems. This issue was actively discussed in the 70s and 80s,<sup class=""reference"" id=""cite_ref-Neats_vs._scruffies_199-0""><a href=""#cite_note-Neats_vs._scruffies-199"">[179]</a></sup>
but in the 1990s mathematical methods and solid scientific standards became the norm, a transition that Russell and Norvig termed ""the victory of the <a href=""/wiki/Neats_and_scruffies"" title=""Neats and scruffies"">neats</a>"".<sup class=""reference"" id=""cite_ref-FOOTNOTERussellNorvig200325-26_200-0""><a href=""#cite_note-FOOTNOTERussellNorvig200325-26-200"">[180]</a></sup>
</p>, <p>Finding a provably correct or optimal solution is <a class=""mw-redirect"" href=""/wiki/Intractability_(complexity)"" title=""Intractability (complexity)"">intractable</a> for many important problems.<sup class=""reference"" id=""cite_ref-Intractability_59-2""><a href=""#cite_note-Intractability-59"">[49]</a></sup> <a href=""/wiki/Soft_computing"" title=""Soft computing"">Soft computing</a> is a set of techniques, including <a class=""mw-redirect"" href=""/wiki/Genetic_algorithms"" title=""Genetic algorithms"">genetic algorithms</a>, <a href=""/wiki/Fuzzy_logic"" title=""Fuzzy logic"">fuzzy logic</a> and <a class=""mw-redirect"" href=""/wiki/Neural_networks"" title=""Neural networks"">neural networks</a>, that are tolerant of imprecision, uncertainty, partial truth and approximation. <a href=""/wiki/Soft_computing"" title=""Soft computing"">Soft computing</a> was introduced in the late 80s and most successful AI programs in the 21st century are examples of soft computing with <a class=""mw-redirect"" href=""/wiki/Neural_networks"" title=""Neural networks"">neural networks</a>.
</p>, <p>AI researchers are divided as to whether to pursue the goals of <a href=""/wiki/Artificial_general_intelligence"" title=""Artificial general intelligence"">artificial general intelligence</a> and <a href=""/wiki/Superintelligence"" title=""Superintelligence"">superintelligence</a> (general AI) directly or to solve as many specific problems as possible (narrow AI) in hopes these solutions will lead indirectly to the field's long-term goals<sup class=""reference"" id=""cite_ref-FOOTNOTEPennachinGoertzel2007_201-0""><a href=""#cite_note-FOOTNOTEPennachinGoertzel2007-201"">[181]</a></sup><sup class=""reference"" id=""cite_ref-FOOTNOTERoberts2016_202-0""><a href=""#cite_note-FOOTNOTERoberts2016-202"">[182]</a></sup>
General intelligence is difficult to define and difficult to measure, and modern AI has had more verifiable successes by focussing on specific problems with specific solutions. The experimental sub-field of <a href=""/wiki/Artificial_general_intelligence"" title=""Artificial general intelligence"">artificial general intelligence</a> studies this area exclusively.
</p>, <p>The <a href=""/wiki/Philosophy_of_mind"" title=""Philosophy of mind"">philosophy of mind</a> does not know whether a machine can have a <a href=""/wiki/Mind"" title=""Mind"">mind</a>, <a href=""/wiki/Consciousness"" title=""Consciousness"">consciousness</a> and <a href=""/wiki/Philosophy_of_mind"" title=""Philosophy of mind"">mental states</a>, in the same sense that human beings do. This issue considers the internal experiences of the machine, rather than its external behavior. Mainstream AI research considers this issue irrelevant because it does not affect the goals of the field. <a href=""/wiki/Stuart_J._Russell"" title=""Stuart J. Russell"">Stuart Russell</a> and <a href=""/wiki/Peter_Norvig"" title=""Peter Norvig"">Peter Norvig</a> observe that most AI researchers ""don't care about the [philosophy of AI] — as long as the program works, they don't care whether you call it a simulation of intelligence or real intelligence.""<sup class=""reference"" id=""cite_ref-FOOTNOTERussellNorvig2003947_203-0""><a href=""#cite_note-FOOTNOTERussellNorvig2003947-203"">[183]</a></sup> However, the question has become central to the <a href=""/wiki/Philosophy_of_mind"" title=""Philosophy of mind"">philosophy of mind</a>. It is also typically the central question at issue in <a href=""/wiki/Artificial_intelligence_in_fiction"" title=""Artificial intelligence in fiction"">artificial intelligence in fiction</a>.
</p>, <p><a href=""/wiki/David_Chalmers"" title=""David Chalmers"">David Chalmers</a> identified two problems in understanding the mind, which he named the ""hard"" and ""easy"" problems of consciousness.<sup class=""reference"" id=""cite_ref-FOOTNOTEChalmers1995_204-0""><a href=""#cite_note-FOOTNOTEChalmers1995-204"">[184]</a></sup> The easy problem is understanding how the brain processes signals, makes plans and controls behavior. The hard problem is explaining how this <i>feels</i> or why it should feel like anything at all. Human <a href=""/wiki/Information_processing"" title=""Information processing"">information processing</a> is easy to explain, however, human <a class=""mw-redirect"" href=""/wiki/Subjective_experience"" title=""Subjective experience"">subjective experience</a> is difficult to explain. For example, it is easy to imagine a color-blind person who has learned to identify which objects in their field of view are red, but it is not clear what would be required for the person to <i>know what red looks like</i>.<sup class=""reference"" id=""cite_ref-FOOTNOTEDennett1991_205-0""><a href=""#cite_note-FOOTNOTEDennett1991-205"">[185]</a></sup>
</p>, <p>Computationalism is the position in the <a href=""/wiki/Philosophy_of_mind"" title=""Philosophy of mind"">philosophy of mind</a> that the human mind is an information processing system and that thinking is a form of computing. Computationalism argues that the relationship between mind and body is similar or identical to the relationship between software and hardware and thus may be a solution to the <a class=""mw-redirect"" href=""/wiki/Mind-body_problem"" title=""Mind-body problem"">mind-body problem</a>. This philosophical position was inspired by the work of AI researchers and cognitive scientists in the 1960s and was originally proposed by philosophers <a href=""/wiki/Jerry_Fodor"" title=""Jerry Fodor"">Jerry Fodor</a> and <a href=""/wiki/Hilary_Putnam"" title=""Hilary Putnam"">Hilary Putnam</a>.<sup class=""reference"" id=""cite_ref-FOOTNOTEHorst2005_206-0""><a href=""#cite_note-FOOTNOTEHorst2005-206"">[186]</a></sup>
</p>, <p>Philosopher <a href=""/wiki/John_Searle"" title=""John Searle"">John Searle</a> characterized this position as <a class=""mw-redirect"" href=""/wiki/Strong_AI_hypothesis"" title=""Strong AI hypothesis"">""strong AI""</a>: ""The appropriately programmed computer with the right inputs and outputs would thereby have a mind in exactly the same sense human beings have minds.""<sup class=""reference"" id=""cite_ref-Searle's_strong_AI_209-0""><a href=""#cite_note-Searle's_strong_AI-209"">[u]</a></sup>
Searle counters this assertion with his <a href=""/wiki/Chinese_room"" title=""Chinese room"">Chinese room</a> argument, which attempts to show that, even if a machine perfectly simulates human behavior, there is still no reason to suppose it also has a mind.<sup class=""reference"" id=""cite_ref-Chinese_room_210-0""><a href=""#cite_note-Chinese_room-210"">[189]</a></sup>
</p>, <p>If a machine has a mind and subjective experience, then it may also have <a href=""/wiki/Sentience"" title=""Sentience"">sentience</a> (the ability to feel), and if so, then it could also <i>suffer</i>, and thus it would be entitled to certain rights.<sup class=""reference"" id=""cite_ref-211""><a href=""#cite_note-211"">[190]</a></sup>
Any hypothetical robot rights would lie on a spectrum with <a href=""/wiki/Animal_rights"" title=""Animal rights"">animal rights</a> and human rights.<sup class=""reference"" id=""cite_ref-FOOTNOTEEvans2015_212-0""><a href=""#cite_note-FOOTNOTEEvans2015-212"">[191]</a></sup>
This issue has been considered in <a href=""/wiki/Artificial_intelligence_in_fiction"" title=""Artificial intelligence in fiction"">fiction</a> for centuries,<sup class=""reference"" id=""cite_ref-FOOTNOTEMcCorduck200419–25_213-0""><a href=""#cite_note-FOOTNOTEMcCorduck200419–25-213"">[192]</a></sup>
and is now being considered by, for example, California's <a href=""/wiki/Institute_for_the_Future"" title=""Institute for the Future"">Institute for the Future</a>, however, critics argue that the discussion is premature.<sup class=""reference"" id=""cite_ref-FOOTNOTEHenderson2007_214-0""><a href=""#cite_note-FOOTNOTEHenderson2007-214"">[193]</a></sup>
</p>, <p>A superintelligence, hyperintelligence, or superhuman intelligence, is a hypothetical agent that would possess intelligence far surpassing that of the brightest and most gifted human mind. <i>Superintelligence</i> may also refer to the form or degree of intelligence possessed by such an agent.<sup class=""reference"" id=""cite_ref-FOOTNOTERoberts2016_202-1""><a href=""#cite_note-FOOTNOTERoberts2016-202"">[182]</a></sup>
</p>, <p>If research into <a href=""/wiki/Artificial_general_intelligence"" title=""Artificial general intelligence"">artificial general intelligence</a> produced sufficiently intelligent software, it might be able to reprogram and improve itself. The improved software would be even better at improving itself, leading to <a class=""mw-redirect"" href=""/wiki/Intelligence_explosion"" title=""Intelligence explosion"">recursive self-improvement</a>.<sup class=""reference"" id=""cite_ref-FOOTNOTEOmohundro2008_215-0""><a href=""#cite_note-FOOTNOTEOmohundro2008-215"">[194]</a></sup>
Its intelligence would increase exponentially in an <a class=""mw-redirect"" href=""/wiki/Intelligence_explosion"" title=""Intelligence explosion"">intelligence explosion</a> and could dramatically surpass humans. Science fiction writer <a href=""/wiki/Vernor_Vinge"" title=""Vernor Vinge"">Vernor Vinge</a> named this scenario the ""singularity"".<sup class=""reference"" id=""cite_ref-FOOTNOTEVinge1993_216-0""><a href=""#cite_note-FOOTNOTEVinge1993-216"">[195]</a></sup>
Because it is difficult or impossible to know the limits of intelligence or the capabilities of superintelligent machines, the technological singularity is an occurrence beyond which events are unpredictable or even unfathomable.<sup class=""reference"" id=""cite_ref-FOOTNOTERussellNorvig2003963_217-0""><a href=""#cite_note-FOOTNOTERussellNorvig2003963-217"">[196]</a></sup>
</p>, <p>Robot designer <a href=""/wiki/Hans_Moravec"" title=""Hans Moravec"">Hans Moravec</a>, cyberneticist <a href=""/wiki/Kevin_Warwick"" title=""Kevin Warwick"">Kevin Warwick</a>, and inventor <a href=""/wiki/Ray_Kurzweil"" title=""Ray Kurzweil"">Ray Kurzweil</a> have predicted that humans and machines will merge in the future into <a href=""/wiki/Cyborg"" title=""Cyborg"">cyborgs</a> that are more capable and powerful than either. This idea, called transhumanism, has roots in <a href=""/wiki/Aldous_Huxley"" title=""Aldous Huxley"">Aldous Huxley</a> and <a href=""/wiki/Robert_Ettinger"" title=""Robert Ettinger"">Robert Ettinger</a>.<sup class=""reference"" id=""cite_ref-218""><a href=""#cite_note-218"">[197]</a></sup>
</p>, <p><a href=""/wiki/Edward_Fredkin"" title=""Edward Fredkin"">Edward Fredkin</a> argues that ""artificial intelligence is the next stage in evolution"", an idea first proposed by <a href=""/wiki/Samuel_Butler_(novelist)"" title=""Samuel Butler (novelist)"">Samuel Butler</a>'s ""<a href=""/wiki/Darwin_among_the_Machines"" title=""Darwin among the Machines"">Darwin among the Machines</a>"" as far back as 1863, and expanded upon by <a href=""/wiki/George_Dyson_(science_historian)"" title=""George Dyson (science historian)"">George Dyson</a> in his book of the same name in 1998.<sup class=""reference"" id=""cite_ref-219""><a href=""#cite_note-219"">[198]</a></sup>
</p>, <p>In the past technology has tended to increase rather than reduce total employment, but economists acknowledge that ""we're in uncharted territory"" with AI.<sup class=""reference"" id=""cite_ref-220""><a href=""#cite_note-220"">[199]</a></sup>
A survey of economists showed disagreement about whether the increasing use of robots and AI will cause a substantial increase in long-term <a href=""/wiki/Unemployment"" title=""Unemployment"">unemployment</a>, but they generally agree that it could be a net benefit if <a href=""/wiki/Productivity"" title=""Productivity"">productivity</a> gains are <a href=""/wiki/Redistribution_of_income_and_wealth"" title=""Redistribution of income and wealth"">redistributed</a>.<sup class=""reference"" id=""cite_ref-FOOTNOTEIGM_Chicago2017_221-0""><a href=""#cite_note-FOOTNOTEIGM_Chicago2017-221"">[200]</a></sup>
Subjective estimates of the risk vary widely; for example, Michael Osborne and <a href=""/wiki/Carl_Benedikt_Frey"" title=""Carl Benedikt Frey"">Carl Benedikt Frey</a> estimate 47% of U.S. jobs are at ""high risk"" of potential automation, while an OECD report classifies only 9% of U.S. jobs as ""high risk"".<sup class=""reference"" id=""cite_ref-223""><a href=""#cite_note-223"">[v]</a></sup><sup class=""reference"" id=""cite_ref-224""><a href=""#cite_note-224"">[202]</a></sup>
</p>, <p>Unlike previous waves of automation, many middle-class jobs may be eliminated by artificial intelligence; <i><a href=""/wiki/The_Economist"" title=""The Economist"">The Economist</a></i> states that ""the worry that AI could do to white-collar jobs what steam power did to blue-collar ones during the Industrial Revolution"" is ""worth taking seriously"".<sup class=""reference"" id=""cite_ref-FOOTNOTEMorgenstern2015_225-0""><a href=""#cite_note-FOOTNOTEMorgenstern2015-225"">[203]</a></sup>
Jobs at extreme risk range from paralegals to fast food cooks, while job demand is likely to increase for care-related professions ranging from personal healthcare to the clergy.<sup class=""reference"" id=""cite_ref-226""><a href=""#cite_note-226"">[204]</a></sup>
</p>, <p>AI provides a number of tools that are particularly useful for <a class=""mw-redirect"" href=""/wiki/Authoritarian"" title=""Authoritarian"">authoritarian</a> governments: smart <a href=""/wiki/Spyware"" title=""Spyware"">spyware</a>, <a href=""/wiki/Facial_recognition_system"" title=""Facial recognition system"">face recognition</a> and <a href=""/wiki/Speaker_recognition"" title=""Speaker recognition"">voice recognition</a> allow widespread <a href=""/wiki/Surveillance"" title=""Surveillance"">surveillance</a>; such surveillance allows <a href=""/wiki/Machine_learning"" title=""Machine learning"">machine learning</a> to <a class=""mw-redirect"" href=""/wiki/Classifier_(machine_learning)"" title=""Classifier (machine learning)"">classify</a> potential enemies of the state and can prevent them from hiding; <a href=""/wiki/Recommender_system"" title=""Recommender system"">recommendation systems</a> can precisely target <a href=""/wiki/Propaganda"" title=""Propaganda"">propaganda</a> and <a href=""/wiki/Misinformation"" title=""Misinformation"">misinformation</a> for maximum effect; <a class=""mw-redirect"" href=""/wiki/Deepfakes"" title=""Deepfakes"">deepfakes</a> aid in producing misinformation; advanced AI can make <a href=""/wiki/Technocracy"" title=""Technocracy"">centralized decision making</a> more competitive with liberal and decentralized systems such as markets.<sup class=""reference"" id=""cite_ref-FOOTNOTEHarari2018_227-0""><a href=""#cite_note-FOOTNOTEHarari2018-227"">[205]</a></sup>
</p>, <p>Terrorists, criminals and rogue states may use other forms of weaponized AI such as advanced <a class=""mw-redirect"" href=""/wiki/Digital_warfare"" title=""Digital warfare"">digital warfare</a> and <a href=""/wiki/Lethal_autonomous_weapon"" title=""Lethal autonomous weapon"">lethal autonomous weapons</a>. By 2015, over fifty countries were reported to be researching battlefield robots.<sup class=""reference"" id=""cite_ref-228""><a href=""#cite_note-228"">[206]</a></sup>
</p>, <p>Machine-learning AI is also able to design tens of thousands of toxic molecules in a matter of hours.<sup class=""reference"" id=""cite_ref-229""><a href=""#cite_note-229"">[207]</a></sup>
</p>, <p>AI programs can become biased after learning from real-world data. It is not typically introduced by the system designers but is learned by the program, and thus the programmers are often unaware that the bias exists.<sup class=""reference"" id=""cite_ref-FOOTNOTECNA2019_230-0""><a href=""#cite_note-FOOTNOTECNA2019-230"">[208]</a></sup>
Bias can be inadvertently introduced by the way <a class=""mw-redirect"" href=""/wiki/Training_data"" title=""Training data"">training data</a> is selected.<sup class=""reference"" id=""cite_ref-FOOTNOTEGoffrey200817_231-0""><a href=""#cite_note-FOOTNOTEGoffrey200817-231"">[209]</a></sup>
It can also <a href=""/wiki/Algorithmic_bias#Emergent"" title=""Algorithmic bias"">emerge</a> from <a class=""mw-redirect"" href=""/wiki/Correlations"" title=""Correlations"">correlations</a>: AI is used to <a class=""mw-redirect"" href=""/wiki/Statistical_classifier"" title=""Statistical classifier"">classify</a> individuals into groups and then make predictions assuming that the individual will resemble other members of the group. In some cases, this assumption may be unfair.<sup class=""reference"" id=""cite_ref-232""><a href=""#cite_note-232"">[210]</a></sup>
An example of this is <a href=""/wiki/COMPAS_(software)"" title=""COMPAS (software)"">COMPAS</a>, a commercial program widely used by <a class=""mw-redirect"" href=""/wiki/U.S._court"" title=""U.S. court"">U.S. courts</a> to assess the likelihood of a <a href=""/wiki/Defendant"" title=""Defendant"">defendant</a> becoming a <a class=""mw-redirect"" href=""/wiki/Recidivist"" title=""Recidivist"">recidivist</a>. <a href=""/wiki/ProPublica"" title=""ProPublica"">ProPublica</a> claims that the COMPAS-assigned recidivism risk level of black defendants is far more likely to be overestimated than that of white defendants, despite the fact that the program was not told the races of the defendants.<sup class=""reference"" id=""cite_ref-FOOTNOTELarsonAngwin2016_233-0""><a href=""#cite_note-FOOTNOTELarsonAngwin2016-233"">[211]</a></sup> Other examples where algorithmic bias can lead to unfair outcomes are when AI is used for <a href=""/wiki/Credit_rating"" title=""Credit rating"">credit rating</a> or <a href=""/wiki/Recruitment"" title=""Recruitment"">hiring</a>.
</p>, <p><a class=""mw-redirect"" href=""/wiki/Superintelligent"" title=""Superintelligent"">Superintelligent</a> AI may be able to improve itself to the point that humans could not control it. This could, as physicist <a href=""/wiki/Stephen_Hawking"" title=""Stephen Hawking"">Stephen Hawking</a> puts it, ""<a href=""/wiki/Global_catastrophic_risk"" title=""Global catastrophic risk"">spell the end of the human race</a>"".<sup class=""reference"" id=""cite_ref-FOOTNOTECellan-Jones2014_234-0""><a href=""#cite_note-FOOTNOTECellan-Jones2014-234"">[212]</a></sup> Philosopher <a href=""/wiki/Nick_Bostrom"" title=""Nick Bostrom"">Nick Bostrom</a> argues that sufficiently intelligent AI if it chooses actions based on achieving some goal, will exhibit <a href=""/wiki/Instrumental_convergence"" title=""Instrumental convergence"">convergent</a> behavior such as acquiring resources or protecting itself from being shut down. If this AI's goals do not fully reflect humanity's, it might need to harm humanity to acquire more resources or prevent itself from being shut down, ultimately to better achieve its goal. He concludes that AI poses a risk to mankind, however humble or ""<a class=""mw-redirect"" href=""/wiki/Friendly_AI"" title=""Friendly AI"">friendly</a>"" its stated goals might be.<sup class=""reference"" id=""cite_ref-235""><a href=""#cite_note-235"">[213]</a></sup>
Political scientist <a href=""/wiki/Charles_T._Rubin"" title=""Charles T. Rubin"">Charles T. Rubin</a> argues that ""any sufficiently advanced benevolence may be indistinguishable from malevolence."" Humans should not assume machines or robots would treat us favorably because there is no <i>a priori</i> reason to believe that they would share our system of morality.<sup class=""reference"" id=""cite_ref-FOOTNOTERubin2003_236-0""><a href=""#cite_note-FOOTNOTERubin2003-236"">[214]</a></sup>
</p>, <p>The opinion of experts and industry insiders is mixed, with sizable fractions both concerned and unconcerned by risk from eventual superhumanly-capable AI.<sup class=""reference"" id=""cite_ref-FOOTNOTEMüllerBostrom2014_237-0""><a href=""#cite_note-FOOTNOTEMüllerBostrom2014-237"">[215]</a></sup>
<a href=""/wiki/Stephen_Hawking"" title=""Stephen Hawking"">Stephen Hawking</a>, <a href=""/wiki/Microsoft"" title=""Microsoft"">Microsoft</a> founder <a href=""/wiki/Bill_Gates"" title=""Bill Gates"">Bill Gates</a>, history professor <a href=""/wiki/Yuval_Noah_Harari"" title=""Yuval Noah Harari"">Yuval Noah Harari</a>, and <a href=""/wiki/SpaceX"" title=""SpaceX"">SpaceX</a> founder <a href=""/wiki/Elon_Musk"" title=""Elon Musk"">Elon Musk</a> have all expressed serious misgivings about the future of AI.<sup class=""reference"" id=""cite_ref-238""><a href=""#cite_note-238"">[216]</a></sup>
Prominent tech titans including <a href=""/wiki/Peter_Thiel"" title=""Peter Thiel"">Peter Thiel</a> (<a href=""/wiki/Amazon_Web_Services"" title=""Amazon Web Services"">Amazon Web Services</a>) and Musk have committed more than $1 billion to nonprofit companies that champion responsible AI development, such as <a href=""/wiki/OpenAI"" title=""OpenAI"">OpenAI</a> and the <a href=""/wiki/Future_of_Life_Institute"" title=""Future of Life Institute"">Future of Life Institute</a>.<sup class=""reference"" id=""cite_ref-239""><a href=""#cite_note-239"">[217]</a></sup>
<a href=""/wiki/Mark_Zuckerberg"" title=""Mark Zuckerberg"">Mark Zuckerberg</a> (CEO, Facebook) has said that artificial intelligence is helpful in its current form and will continue to assist humans.<sup class=""reference"" id=""cite_ref-240""><a href=""#cite_note-240"">[218]</a></sup>
Other experts argue is that the risks are far enough in the future to not be worth researching,
or that humans will be valuable from the perspective of a superintelligent machine.<sup class=""reference"" id=""cite_ref-241""><a href=""#cite_note-241"">[219]</a></sup>
<a href=""/wiki/Rodney_Brooks"" title=""Rodney Brooks"">Rodney Brooks</a>, in particular, has said that ""malevolent"" AI is still centuries away.<sup class=""reference"" id=""cite_ref-243""><a href=""#cite_note-243"">[w]</a></sup>
</p>, <p>Friendly AI are machines that have been designed from the beginning to minimize risks and to make choices that benefit humans. <a href=""/wiki/Eliezer_Yudkowsky"" title=""Eliezer Yudkowsky"">Eliezer Yudkowsky</a>, who coined the term, argues that developing friendly AI should be a higher research priority: it may require a large investment and it must be completed before AI becomes an existential risk.<sup class=""reference"" id=""cite_ref-FOOTNOTEYudkowsky2008_244-0""><a href=""#cite_note-FOOTNOTEYudkowsky2008-244"">[221]</a></sup>
</p>, <p>Machines with intelligence have the potential to use their intelligence to make ethical decisions. The field of machine ethics provides machines with ethical principles and procedures for resolving ethical dilemmas.<sup class=""reference"" id=""cite_ref-FOOTNOTEAndersonAnderson2011_245-0""><a href=""#cite_note-FOOTNOTEAndersonAnderson2011-245"">[222]</a></sup>
Machine ethics is also called <a class=""mw-redirect"" href=""/wiki/Machine_morality"" title=""Machine morality"">machine morality</a>, <a class=""mw-redirect"" href=""/wiki/Computational_ethics"" title=""Computational ethics"">computational ethics</a> or computational morality,<sup class=""reference"" id=""cite_ref-FOOTNOTEAndersonAnderson2011_245-1""><a href=""#cite_note-FOOTNOTEAndersonAnderson2011-245"">[222]</a></sup>
and was founded at an <a class=""mw-redirect"" href=""/wiki/AAAI"" title=""AAAI"">AAAI</a> symposium in 2005.<sup class=""reference"" id=""cite_ref-FOOTNOTEAAAI2014_246-0""><a href=""#cite_note-FOOTNOTEAAAI2014-246"">[223]</a></sup>
</p>, <p>Other approaches include <a href=""/wiki/Wendell_Wallach"" title=""Wendell Wallach"">Wendell Wallach</a>'s ""artificial moral agents""<sup class=""reference"" id=""cite_ref-FOOTNOTEWallach2010_247-0""><a href=""#cite_note-FOOTNOTEWallach2010-247"">[224]</a></sup>
and <a href=""/wiki/Stuart_J._Russell"" title=""Stuart J. Russell"">Stuart J. Russell</a>'s <a href=""/wiki/Human_Compatible#Russell's_three_principles"" title=""Human Compatible"">three principles</a> for developing provably beneficial machines.<sup class=""reference"" id=""cite_ref-FOOTNOTERussell2019173_248-0""><a href=""#cite_note-FOOTNOTERussell2019173-248"">[225]</a></sup>
</p>, <p>The regulation of artificial intelligence is the development of public sector policies and laws for promoting and regulating artificial intelligence (AI); it is therefore related to the broader regulation of algorithms.<sup class=""reference"" id=""cite_ref-249""><a href=""#cite_note-249"">[226]</a></sup>
The regulatory and  policy landscape for AI is an emerging issue in jurisdictions globally.<sup class=""reference"" id=""cite_ref-FOOTNOTELaw_Library_of_Congress_(U.S.)._Global_Legal_Research_Directorate2019_250-0""><a href=""#cite_note-FOOTNOTELaw_Library_of_Congress_(U.S.)._Global_Legal_Research_Directorate2019-250"">[227]</a></sup>
Between 2016 and 2020, more than 30 countries adopted dedicated strategies for AI.<sup class=""reference"" id=""cite_ref-FOOTNOTEUNESCO2021_56-1""><a href=""#cite_note-FOOTNOTEUNESCO2021-56"">[46]</a></sup>
Most EU member states had released national AI strategies, as had Canada, China, India, Japan, Mauritius, the Russian Federation, Saudi Arabia, United Arab Emirates, USA and Vietnam. Others were in the process of elaborating their own AI strategy, including Bangladesh, Malaysia and Tunisia.<sup class=""reference"" id=""cite_ref-FOOTNOTEUNESCO2021_56-2""><a href=""#cite_note-FOOTNOTEUNESCO2021-56"">[46]</a></sup>
The <a href=""/wiki/Global_Partnership_on_Artificial_Intelligence"" title=""Global Partnership on Artificial Intelligence"">Global Partnership on Artificial Intelligence</a> was launched in June 2020, stating a need for AI to be developed in accordance with human rights and democratic values, to ensure public confidence and trust in the technology.<sup class=""reference"" id=""cite_ref-FOOTNOTEUNESCO2021_56-3""><a href=""#cite_note-FOOTNOTEUNESCO2021-56"">[46]</a></sup> <a href=""/wiki/Henry_Kissinger"" title=""Henry Kissinger"">Henry Kissinger</a>, <a href=""/wiki/Eric_Schmidt"" title=""Eric Schmidt"">Eric Schmidt</a>, and <a href=""/wiki/Daniel_P._Huttenlocher"" title=""Daniel P. Huttenlocher"">Daniel Huttenlocher</a> published a joint statement in November 2021 calling for a government commission to regulate AI.<sup class=""reference"" id=""cite_ref-251""><a href=""#cite_note-251"">[228]</a></sup>
</p>, <p>Thought-capable artificial beings have appeared as storytelling devices since antiquity,<sup class=""reference"" id=""cite_ref-AI_in_myth_21-1""><a href=""#cite_note-AI_in_myth-21"">[17]</a></sup>
and have been a persistent theme in <a href=""/wiki/Science_fiction"" title=""Science fiction"">science fiction</a>.<sup class=""reference"" id=""cite_ref-FOOTNOTEMcCorduck2004340–400_23-1""><a href=""#cite_note-FOOTNOTEMcCorduck2004340–400-23"">[19]</a></sup>
</p>, <p>A common <a href=""/wiki/Trope_(literature)"" title=""Trope (literature)"">trope</a> in these works began with <a href=""/wiki/Mary_Shelley"" title=""Mary Shelley"">Mary Shelley</a>'s <i><a href=""/wiki/Frankenstein"" title=""Frankenstein"">Frankenstein</a></i>, where a human creation becomes a threat to its masters. This includes such works as <a href=""/wiki/2001:_A_Space_Odyssey_(novel)"" title=""2001: A Space Odyssey (novel)"">Arthur C. Clarke's</a> and <a href=""/wiki/2001:_A_Space_Odyssey_(film)"" title=""2001: A Space Odyssey (film)"">Stanley Kubrick's</a> <i><a href=""/wiki/2001:_A_Space_Odyssey"" title=""2001: A Space Odyssey"">2001: A Space Odyssey</a></i> (both 1968), with <a href=""/wiki/HAL_9000"" title=""HAL 9000"">HAL 9000</a>, the murderous computer in charge of the <i><a href=""/wiki/Discovery_One"" title=""Discovery One"">Discovery One</a></i> spaceship, as well as <i><a href=""/wiki/The_Terminator"" title=""The Terminator"">The Terminator</a></i> (1984) and <i><a href=""/wiki/The_Matrix"" title=""The Matrix"">The Matrix</a></i> (1999). In contrast, the rare loyal robots such as Gort from <i><a href=""/wiki/The_Day_the_Earth_Stood_Still"" title=""The Day the Earth Stood Still"">The Day the Earth Stood Still</a></i> (1951) and Bishop from <i><a href=""/wiki/Aliens_(film)"" title=""Aliens (film)"">Aliens</a></i> (1986) are less prominent in popular culture.<sup class=""reference"" id=""cite_ref-FOOTNOTEButtazzo2001_252-0""><a href=""#cite_note-FOOTNOTEButtazzo2001-252"">[229]</a></sup>
</p>, <p><a href=""/wiki/Isaac_Asimov"" title=""Isaac Asimov"">Isaac Asimov</a> introduced the <a href=""/wiki/Three_Laws_of_Robotics"" title=""Three Laws of Robotics"">Three Laws of Robotics</a> in many books and stories, most notably the ""Multivac"" series about a super-intelligent computer of the same name. Asimov's laws are often brought up during lay discussions of machine ethics;<sup class=""reference"" id=""cite_ref-FOOTNOTEAnderson2008_253-0""><a href=""#cite_note-FOOTNOTEAnderson2008-253"">[230]</a></sup>
while almost all artificial intelligence researchers are familiar with Asimov's laws through popular culture, they generally consider the laws useless for many reasons, one of which is their ambiguity.<sup class=""reference"" id=""cite_ref-FOOTNOTEMcCauley2007_254-0""><a href=""#cite_note-FOOTNOTEMcCauley2007-254"">[231]</a></sup>
</p>, <p><a href=""/wiki/Transhumanism"" title=""Transhumanism"">Transhumanism</a> (the merging of humans and machines) is explored in the <a href=""/wiki/Manga"" title=""Manga"">manga</a> <i><a href=""/wiki/Ghost_in_the_Shell"" title=""Ghost in the Shell"">Ghost in the Shell</a></i> and the science-fiction series <i><a href=""/wiki/Dune_(novel)"" title=""Dune (novel)"">Dune</a></i>.
</p>, <p>Several works use AI to force us to confront the fundamental question of what makes us human, showing us artificial beings that have <a href=""/wiki/Sentience"" title=""Sentience"">the ability to feel</a>, and thus to suffer. This appears in <a href=""/wiki/Karel_%C4%8Capek"" title=""Karel Čapek"">Karel Čapek</a>'s <i><a href=""/wiki/R.U.R."" title=""R.U.R."">R.U.R.</a></i>, the films <i><a href=""/wiki/A.I._Artificial_Intelligence"" title=""A.I. Artificial Intelligence"">A.I. Artificial Intelligence</a></i> and <i><a href=""/wiki/Ex_Machina_(film)"" title=""Ex Machina (film)"">Ex Machina</a></i>, as well as the novel <i><a href=""/wiki/Do_Androids_Dream_of_Electric_Sheep%3F"" title=""Do Androids Dream of Electric Sheep?"">Do Androids Dream of Electric Sheep?</a></i>, by <a href=""/wiki/Philip_K._Dick"" title=""Philip K. Dick"">Philip K. Dick</a>. Dick considers the idea that our understanding of human subjectivity is altered by technology created with artificial intelligence.<sup class=""reference"" id=""cite_ref-FOOTNOTEGalvan1997_255-0""><a href=""#cite_note-FOOTNOTEGalvan1997-255"">[232]</a></sup>
</p>, <p>These were the four the most widely used AI textbooks in 2008.
</p>, <p>Later editions.
</p>, <p>The two most widely used textbooks in 2021.<a class=""external text"" href=""https://opensyllabus.org/result/field?id=Computer+Science"" rel=""nofollow"">Open Syllabus: Explorer</a>
</p>, <p><i>See also: <a href=""/wiki/Artificial_intelligence_in_fiction#Logic_machines"" title=""Artificial intelligence in fiction"">Logic machines in fiction</a> and <a href=""/wiki/List_of_fictional_computers"" title=""List of fictional computers"">List of fictional computers</a></i>
</p>]"
Blockchain,"[<p class=""mw-empty-elt"">
</p>, <p>A <b>blockchain</b> is a growing list of <a href=""/wiki/Record_(computer_science)"" title=""Record (computer science)"">records</a>, called <i>blocks</i>, that are securely linked together using <a href=""/wiki/Cryptography"" title=""Cryptography"">cryptography</a>.<sup class=""reference"" id=""cite_ref-fortune20160515_1-0""><a href=""#cite_note-fortune20160515-1"">[1]</a></sup><sup class=""reference"" id=""cite_ref-nyt20160521_2-0""><a href=""#cite_note-nyt20160521-2"">[2]</a></sup><sup class=""reference"" id=""cite_ref-te20151031_3-0""><a href=""#cite_note-te20151031-3"">[3]</a></sup><sup class=""reference"" id=""cite_ref-cryptocurrencytech_4-0""><a href=""#cite_note-cryptocurrencytech-4"">[4]</a></sup> Each block contains a <a href=""/wiki/Cryptographic_hash_function"" title=""Cryptographic hash function"">cryptographic hash</a> of the previous block, a <a href=""/wiki/Trusted_timestamping"" title=""Trusted timestamping"">timestamp</a>, and transaction data (generally represented as a <a href=""/wiki/Merkle_tree"" title=""Merkle tree"">Merkle tree</a>, where <a href=""/wiki/Node_(computer_science)"" title=""Node (computer science)"">data nodes</a> are represented by leafs). The timestamp proves that the transaction data existed when the block was published to get into its hash. As blocks each contain information about the block previous to it, they form a chain, with each additional block reinforcing the ones before it. Therefore, blockchains are resistant to modification of their data because once recorded, the data in any given block cannot be altered retroactively without altering all subsequent blocks.
</p>, <p>Blockchains are typically managed by a <a href=""/wiki/Peer-to-peer"" title=""Peer-to-peer"">peer-to-peer</a> network for use as a publicly <a href=""/wiki/Distributed_ledger"" title=""Distributed ledger"">distributed ledger</a>, where nodes collectively adhere to a <a href=""/wiki/Communication_protocol"" title=""Communication protocol"">protocol</a> to communicate and validate new blocks. Although blockchain records are not unalterable as <a href=""/wiki/Fork_(blockchain)"" title=""Fork (blockchain)"">forks</a> are possible, blockchains may be considered <a href=""/wiki/Secure_by_design"" title=""Secure by design"">secure by design</a> and exemplify a distributed computing system with high <a class=""mw-redirect"" href=""/wiki/Byzantine_fault_tolerance"" title=""Byzantine fault tolerance"">Byzantine fault tolerance</a>.<sup class=""reference"" id=""cite_ref-5""><a href=""#cite_note-5"">[5]</a></sup>
</p>, <p>The blockchain was popularized by a person (or group of people) using the name <a href=""/wiki/Satoshi_Nakamoto"" title=""Satoshi Nakamoto"">Satoshi Nakamoto</a> in 2008 to serve as the public transaction <a href=""/wiki/Ledger"" title=""Ledger"">ledger</a> of the <a href=""/wiki/Cryptocurrency"" title=""Cryptocurrency"">cryptocurrency</a> <a href=""/wiki/Bitcoin"" title=""Bitcoin"">bitcoin</a>, based on work by Stuart Haber, W. Scott Stornetta, and Dave Bayer.<sup class=""reference"" id=""cite_ref-te20151031_3-1""><a href=""#cite_note-te20151031-3"">[3]</a></sup><sup class=""reference"" id=""cite_ref-:0_6-0""><a href=""#cite_note-:0-6"">[6]</a></sup> The identity of Satoshi Nakamoto remains unknown to date. The implementation of the blockchain within bitcoin made it the first digital currency to solve the <a href=""/wiki/Double-spending"" title=""Double-spending"">double-spending</a> problem without the need of a trusted authority or central <a href=""/wiki/Server_(computing)"" title=""Server (computing)"">server</a>. The bitcoin design has inspired other applications<sup class=""reference"" id=""cite_ref-te20151031_3-2""><a href=""#cite_note-te20151031-3"">[3]</a></sup><sup class=""reference"" id=""cite_ref-nyt20160521_2-1""><a href=""#cite_note-nyt20160521-2"">[2]</a></sup> and blockchains that are readable by the public and are widely used by <a class=""mw-redirect"" href=""/wiki/Cryptocurrencies"" title=""Cryptocurrencies"">cryptocurrencies</a>. The blockchain is considered a type of <a href=""/wiki/Payment_rail"" title=""Payment rail"">payment rail</a>.<sup class=""reference"" id=""cite_ref-7""><a href=""#cite_note-7"">[7]</a></sup>
</p>, <p>Private blockchains have been proposed for business use. <i>Computerworld</i> called the marketing of such privatized blockchains without a proper security model ""<a class=""extiw"" href=""https://en.wiktionary.org/wiki/snake_oil"" title=""wikt:snake oil"">snake oil</a>"";<sup class=""reference"" id=""cite_ref-cw20160905_8-0""><a href=""#cite_note-cw20160905-8"">[8]</a></sup> however, others have argued that permissioned blockchains, if carefully designed, may be more decentralized and therefore more secure in practice than permissionless ones.<sup class=""reference"" id=""cite_ref-cryptocurrencytech_4-1""><a href=""#cite_note-cryptocurrencytech-4"">[4]</a></sup><sup class=""reference"" id=""cite_ref-auto_9-0""><a href=""#cite_note-auto-9"">[9]</a></sup>
</p>, <p>Cryptographer <a href=""/wiki/David_Chaum"" title=""David Chaum"">David Chaum</a> first proposed a blockchain-like protocol in his 1982 dissertation ""Computer Systems Established, Maintained, and Trusted by Mutually Suspicious Groups.""<sup class=""reference"" id=""cite_ref-10""><a href=""#cite_note-10"">[10]</a></sup> Further work on a cryptographically secured chain of blocks was described in 1991 by Stuart Haber and W. Scott Stornetta.<sup class=""reference"" id=""cite_ref-cryptocurrencytech_4-2""><a href=""#cite_note-cryptocurrencytech-4"">[4]</a></sup><sup class=""reference"" id=""cite_ref-11""><a href=""#cite_note-11"">[11]</a></sup> They wanted to implement a system wherein document timestamps could not be tampered with. In 1992, Haber, Stornetta, and <a href=""/wiki/Dave_Bayer"" title=""Dave Bayer"">Dave Bayer</a> incorporated <a href=""/wiki/Merkle_tree"" title=""Merkle tree"">Merkle trees</a> into the design, which improved its efficiency by allowing several document certificates to be collected into one block.<sup class=""reference"" id=""cite_ref-cryptocurrencytech_4-3""><a href=""#cite_note-cryptocurrencytech-4"">[4]</a></sup><sup class=""reference"" id=""cite_ref-12""><a href=""#cite_note-12"">[12]</a></sup> Under their company Surety, their document certificate hashes have been published in <i><a href=""/wiki/The_New_York_Times"" title=""The New York Times"">The New York Times</a></i> every week since 1995.<sup class=""reference"" id=""cite_ref-:0_6-1""><a href=""#cite_note-:0-6"">[6]</a></sup>
</p>, <p>The first decentralized blockchain was conceptualized by a person (or group of people) known as <a href=""/wiki/Satoshi_Nakamoto"" title=""Satoshi Nakamoto"">Satoshi Nakamoto</a> in 2008. Nakamoto improved the design in an important way using a <a href=""/wiki/Hashcash"" title=""Hashcash"">Hashcash</a>-like method to <a href=""/wiki/Timestamp-based_concurrency_control"" title=""Timestamp-based concurrency control"">timestamp</a> blocks without requiring them to be signed by a trusted party and introducing a difficulty parameter to stabilize the rate at which blocks are added to the chain.<sup class=""reference"" id=""cite_ref-cryptocurrencytech_4-4""><a href=""#cite_note-cryptocurrencytech-4"">[4]</a></sup> The design was implemented the following year by Nakamoto as a core component of the cryptocurrency <a href=""/wiki/Bitcoin"" title=""Bitcoin"">bitcoin</a>, where it serves as the public <a href=""/wiki/Ledger"" title=""Ledger"">ledger</a> for all transactions on the network.<sup class=""reference"" id=""cite_ref-te20151031_3-3""><a href=""#cite_note-te20151031-3"">[3]</a></sup>
</p>, <p>In August 2014, the bitcoin blockchain file size, containing records of all transactions that have occurred on the network, reached 20 GB (<a href=""/wiki/Gigabyte"" title=""Gigabyte"">gigabytes</a>).<sup class=""reference"" id=""cite_ref-13""><a href=""#cite_note-13"">[13]</a></sup> In January 2015, the size had grown to almost 30 GB, and from January 2016 to January 2017, the bitcoin blockchain grew from 50 GB to 100 GB in size.  The ledger size had exceeded 200 GB by early 2020.<sup class=""reference"" id=""cite_ref-14""><a href=""#cite_note-14"">[14]</a></sup>
</p>, <p>The words <i>block</i> and <i>chain</i> were used separately in Satoshi Nakamoto's original paper, but were eventually popularized as a single word, <i>blockchain,</i> by 2016.<sup class=""noprint Inline-Template Template-Fact"" style=""white-space:nowrap;"">[<i><a href=""/wiki/Wikipedia:Citation_needed"" title=""Wikipedia:Citation needed""><span title=""This claim needs references to reliable sources. (October 2021)"">citation needed</span></a></i>]</sup>
</p>, <p>According to <a href=""/wiki/Accenture"" title=""Accenture"">Accenture</a>, an application of the <a href=""/wiki/Diffusion_of_innovations"" title=""Diffusion of innovations"">diffusion of innovations</a> theory suggests that blockchains attained a 13.5% adoption rate within financial services in 2016, therefore reaching the <a href=""/wiki/Early_adopter"" title=""Early adopter"">early adopters</a>' phase.<sup class=""reference"" id=""cite_ref-15""><a href=""#cite_note-15"">[15]</a></sup> Industry trade groups joined to create the Global Blockchain Forum in 2016, an initiative of the <a href=""/wiki/Chamber_of_Digital_Commerce"" title=""Chamber of Digital Commerce"">Chamber of Digital Commerce</a>.
</p>, <p>In May 2018, <a href=""/wiki/Gartner"" title=""Gartner"">Gartner</a> found that only 1% of <a href=""/wiki/Chief_information_officer"" title=""Chief information officer"">CIOs</a> indicated any kind of blockchain adoption within their organisations, and only 8% of CIOs were in the short-term ""planning or [looking at] active experimentation with blockchain"".<sup class=""reference"" id=""cite_ref-16""><a href=""#cite_note-16"">[16]</a></sup> For the year 2019 Gartner reported 5% of CIOs believed blockchain technology was a 'game-changer' for their business.<sup class=""reference"" id=""cite_ref-17""><a href=""#cite_note-17"">[17]</a></sup>
</p>, <p>A blockchain is a <a href=""/wiki/Decentralized_computing"" title=""Decentralized computing"">decentralized</a>, <a href=""/wiki/Distributed_computing"" title=""Distributed computing"">distributed</a>, and oftentimes public, digital ledger consisting of records called <i>blocks</i> that are used to record transactions across many computers so that any involved block cannot be altered retroactively, without the alteration of all subsequent blocks.<sup class=""reference"" id=""cite_ref-te20151031_3-4""><a href=""#cite_note-te20151031-3"">[3]</a></sup><sup class=""reference"" id=""cite_ref-18""><a href=""#cite_note-18"">[18]</a></sup> This allows the participants to verify and audit transactions independently and relatively inexpensively.<sup class=""reference"" id=""cite_ref-19""><a href=""#cite_note-19"">[19]</a></sup> A blockchain database is managed autonomously using a <a href=""/wiki/Peer-to-peer"" title=""Peer-to-peer"">peer-to-peer</a> network and a distributed timestamping server. They are <a href=""/wiki/Authentication"" title=""Authentication"">authenticated</a> by <a href=""/wiki/Mass_collaboration"" title=""Mass collaboration"">mass collaboration</a> powered by <a href=""/wiki/Collective"" title=""Collective"">collective</a> <a href=""/wiki/Self-interest"" title=""Self-interest"">self-interests</a>.<sup class=""reference"" id=""cite_ref-20""><a href=""#cite_note-20"">[20]</a></sup> Such a design facilitates <a href=""/wiki/Robustness_(computer_science)"" title=""Robustness (computer science)"">robust</a> <a href=""/wiki/Workflow"" title=""Workflow"">workflow</a> where participants' uncertainty regarding data security is marginal. The use of a blockchain removes the characteristic of infinite <a href=""/wiki/Reproduction_(economics)"" title=""Reproduction (economics)"">reproducibility</a> from a digital asset. It confirms that each unit of value was transferred only once, solving the long-standing problem of <a class=""mw-redirect"" href=""/wiki/Double_spending"" title=""Double spending"">double-spending</a>. A blockchain has been described as a <i>value-exchange protocol</i>.<sup class=""reference"" id=""cite_ref-21""><a href=""#cite_note-21"">[21]</a></sup> A blockchain can maintain <a href=""/wiki/Title_(property)"" title=""Title (property)"">title rights</a> because, when properly set up to detail the exchange agreement, it provides a record that compels <a href=""/wiki/Offer_and_acceptance"" title=""Offer and acceptance"">offer and acceptance</a>.
</p>, <p>Logically, a blockchain can be seen as consisting of several layers:<sup class=""reference"" id=""cite_ref-22""><a href=""#cite_note-22"">[22]</a></sup>
</p>, <p>Blocks hold batches of valid <a href=""/wiki/Transaction_processing"" title=""Transaction processing"">transactions</a> that are hashed and encoded into a <a href=""/wiki/Merkle_tree"" title=""Merkle tree"">Merkle tree</a>.<sup class=""reference"" id=""cite_ref-te20151031_3-5""><a href=""#cite_note-te20151031-3"">[3]</a></sup> Each block includes the <a class=""mw-redirect"" href=""/wiki/Cryptographic_hash"" title=""Cryptographic hash"">cryptographic hash</a> of the prior block in the blockchain, linking the two. The linked blocks form a chain.<sup class=""reference"" id=""cite_ref-te20151031_3-6""><a href=""#cite_note-te20151031-3"">[3]</a></sup> This <a href=""/wiki/Iteration"" title=""Iteration"">iterative</a> process confirms the integrity of the previous block, all the way back to the initial block, which is known as the <i><b>genesis block</b></i>.<sup class=""reference"" id=""cite_ref-hadc_24-0""><a href=""#cite_note-hadc-24"">[24]</a></sup> To assure the integrity of a block and the data contained in it, the block is usually <a href=""/wiki/Digital_signature"" title=""Digital signature"">digitally signed</a>.<sup class=""reference"" id=""cite_ref-FOOTNOTEKnirschUnterwegerEngel20192_25-0""><a href=""#cite_note-FOOTNOTEKnirschUnterwegerEngel20192-25"">[25]</a></sup>
</p>, <p>Sometimes separate blocks can be produced concurrently, creating a temporary <a href=""/wiki/Fork_(blockchain)"" title=""Fork (blockchain)"">fork</a>. In addition to a secure <a href=""/wiki/Hash-based_cryptography"" title=""Hash-based cryptography"">hash-based</a> history, any blockchain has a specified algorithm for scoring different versions of the history so that one with a higher score can be selected over others. Blocks not selected for inclusion in the chain are called orphan blocks.<sup class=""reference"" id=""cite_ref-hadc_24-1""><a href=""#cite_note-hadc-24"">[24]</a></sup> Peers supporting the database have different versions of the history from time to time. They keep only the highest-scoring version of the database known to them. Whenever a peer receives a higher-scoring version (usually the old version with a single new block added) they extend or overwrite their own database and retransmit the improvement to their peers. There is never an absolute guarantee that any particular entry will remain in the best version of history forever. Blockchains are typically built to add the score of new blocks onto old blocks and are given incentives to extend with new blocks rather than overwrite old blocks. Therefore, the probability of an entry becoming superseded decreases exponentially<sup class=""reference"" id=""cite_ref-bsm_26-0""><a href=""#cite_note-bsm-26"">[26]</a></sup> as more blocks are built on top of it, eventually becoming very low.<sup class=""reference"" id=""cite_ref-te20151031_3-7""><a href=""#cite_note-te20151031-3"">[3]</a></sup><sup class=""reference"" id=""cite_ref-t12_27-0""><a href=""#cite_note-t12-27"">[27]</a></sup><sup class=""reference nowrap""><span title=""Page: ch. 08"">: ch. 08 </span></sup><sup class=""reference"" id=""cite_ref-paper_28-0""><a href=""#cite_note-paper-28"">[28]</a></sup> For example, bitcoin uses a <a class=""mw-redirect"" href=""/wiki/Proof-of-work_system"" title=""Proof-of-work system"">proof-of-work system</a>, where the chain with the most cumulative proof-of-work is considered the valid one by the network. There are a number of methods that can be used to demonstrate a sufficient level of <a href=""/wiki/Computation"" title=""Computation"">computation</a>. Within a blockchain the computation is carried out redundantly rather than in the traditional segregated and <a href=""/wiki/Parallel_computing"" title=""Parallel computing"">parallel</a> manner.<sup class=""reference"" id=""cite_ref-29""><a href=""#cite_note-29"">[29]</a></sup>
</p>, <p>The <i>block time</i> is the average time it takes for the network to generate one extra block in the blockchain. Some blockchains create a new block as frequently as every five seconds.<sup class=""reference"" id=""cite_ref-30""><a href=""#cite_note-30"">[30]</a></sup> By the time of block completion, the included data becomes verifiable. In cryptocurrency, this is practically when the transaction takes place, so a shorter block time means faster transactions. The block time for <a href=""/wiki/Ethereum"" title=""Ethereum"">Ethereum</a> is set to between 14 and 15 seconds, while for bitcoin it is on average 10 minutes.<sup class=""reference"" id=""cite_ref-31""><a href=""#cite_note-31"">[31]</a></sup>
</p>, <p>A <i>hard fork</i> is a rule change such that the software validating according to the old rules will see the blocks produced according to the new rules as invalid. In case of a hard fork, all nodes meant to work in accordance with the new rules need to upgrade their software. If one group of nodes continues to use the old software while the other nodes use the new software, a permanent split can occur. 
</p>, <p>For example, <a href=""/wiki/Ethereum"" title=""Ethereum"">Ethereum</a> was hard-forked in 2016 to ""make whole"" the investors in <a href=""/wiki/The_DAO_(organization)"" title=""The DAO (organization)"">The DAO</a>, which had been hacked by exploiting a vulnerability in its code. In this case, the fork resulted in a split creating <a href=""/wiki/Ethereum"" title=""Ethereum"">Ethereum</a> and <a href=""/wiki/Ethereum_Classic"" title=""Ethereum Classic"">Ethereum Classic</a> chains. In 2014 the <a href=""/wiki/Nxt"" title=""Nxt"">Nxt</a> community was asked to consider a hard fork that would have led to a rollback of the blockchain records to mitigate the effects of a theft of 50 million NXT from a major <a href=""/wiki/Cryptocurrency_exchange"" title=""Cryptocurrency exchange"">cryptocurrency exchange</a>. The hard fork proposal was rejected, and some of the funds were recovered after negotiations and ransom payment. Alternatively, to prevent a permanent split, a majority of nodes using the new software may return to the old rules, as was the case of bitcoin split on 12 March 2013.<sup class=""reference"" id=""cite_ref-Fork_(blockchain)_ArsFork_32-0""><a href=""#cite_note-Fork_(blockchain)_ArsFork-32"">[32]</a></sup>
</p>, <p>A more recent hard-fork example is of <a href=""/wiki/Bitcoin"" title=""Bitcoin"">Bitcoin</a> in 2017, which resulted in a split creating <a href=""/wiki/Bitcoin_Cash"" title=""Bitcoin Cash"">Bitcoin Cash</a>.<sup class=""reference"" id=""cite_ref-Fork_(blockchain)_bpr_33-0""><a href=""#cite_note-Fork_(blockchain)_bpr-33"">[33]</a></sup> The network split was mainly due to a disagreement in how to increase the transactions per second to accommodate for demand.<sup class=""reference"" id=""cite_ref-Fork_(blockchain)_CNN_2017-08-01_34-0""><a href=""#cite_note-Fork_(blockchain)_CNN_2017-08-01-34"">[34]</a></sup>
</p>, <p>By storing data across its <a href=""/wiki/Peer-to-peer"" title=""Peer-to-peer"">peer-to-peer network</a>, the blockchain eliminates a number of risks that come with data being held centrally.<sup class=""reference"" id=""cite_ref-te20151031_3-8""><a href=""#cite_note-te20151031-3"">[3]</a></sup> The decentralized blockchain may use <a href=""/wiki/Ad_hoc"" title=""Ad hoc"">ad hoc</a> <a href=""/wiki/Message_passing"" title=""Message passing"">message passing</a> and <a href=""/wiki/Distributed_networking"" title=""Distributed networking"">distributed networking</a>. One risk of a lack of decentralization is a so-called ""51% attack"" where a central entity can gain control of more than half of a network and can manipulate that specific blockchain record at will, allowing double-spending.<sup class=""reference"" id=""cite_ref-36""><a href=""#cite_note-36"">[36]</a></sup>
</p>, <p>Peer-to-peer blockchain networks lack centralized points of vulnerability that <a href=""/wiki/Security_hacker"" title=""Security hacker"">computer crackers</a> can exploit; likewise, it has no central point of <a href=""/wiki/Failure"" title=""Failure"">failure</a>. Blockchain security methods include the use of <a href=""/wiki/Public-key_cryptography"" title=""Public-key cryptography"">public-key cryptography</a>.<sup class=""reference"" id=""cite_ref-primer_37-0""><a href=""#cite_note-primer-37"">[37]</a></sup><sup class=""reference nowrap""><span title=""Page: 5"">: 5 </span></sup> A <i>public key</i> (a long, random-looking string of numbers) is an address on the blockchain. Value tokens sent across the network are recorded as belonging to that address. A <i>private key</i> is like a password that gives its owner access to their digital assets or the means to otherwise interact with the various capabilities that blockchains now support. Data stored on the blockchain is generally considered incorruptible.<sup class=""reference"" id=""cite_ref-te20151031_3-9""><a href=""#cite_note-te20151031-3"">[3]</a></sup>
</p>, <p>Every <a href=""/wiki/Node_(networking)"" title=""Node (networking)"">node</a> in a decentralized system has a copy of the blockchain. <a href=""/wiki/Data_quality"" title=""Data quality"">Data quality</a> is maintained by massive database <a href=""/wiki/Replication_(computing)"" title=""Replication (computing)"">replication</a><sup class=""reference"" id=""cite_ref-38""><a href=""#cite_note-38"">[38]</a></sup> and <a href=""/wiki/Computational_trust"" title=""Computational trust"">computational trust</a>. No centralized ""official"" copy exists and no user is ""trusted"" more than any other.<sup class=""reference"" id=""cite_ref-primer_37-1""><a href=""#cite_note-primer-37"">[37]</a></sup> Transactions are broadcast to the network using the software. Messages are delivered on a <a href=""/wiki/Best-effort_delivery"" title=""Best-effort delivery"">best-effort</a> basis. Mining nodes validate transactions,<sup class=""reference"" id=""cite_ref-hadc_24-2""><a href=""#cite_note-hadc-24"">[24]</a></sup> add them to the block they are building, and then <a href=""/wiki/Broadcasting_(networking)"" title=""Broadcasting (networking)"">broadcast</a> the completed block to other nodes.<sup class=""reference"" id=""cite_ref-t12_27-1""><a href=""#cite_note-t12-27"">[27]</a></sup><sup class=""reference nowrap""><span title=""Page: ch. 08"">: ch. 08 </span></sup> Blockchains use various time-stamping schemes, such as <a class=""mw-redirect"" href=""/wiki/Proof-of-work_system"" title=""Proof-of-work system"">proof-of-work</a>, to serialize changes.<sup class=""reference"" id=""cite_ref-kopstein_39-0""><a href=""#cite_note-kopstein-39"">[39]</a></sup> Alternative consensus methods include <a class=""mw-redirect"" href=""/wiki/Proof-of-stake"" title=""Proof-of-stake"">proof-of-stake</a>.<sup class=""reference"" id=""cite_ref-hadc_24-3""><a href=""#cite_note-hadc-24"">[24]</a></sup> The growth of a decentralized blockchain is accompanied by the risk of <a href=""/wiki/Centrality"" title=""Centrality"">centralization</a> because the computer resources required to process larger amounts of data become more expensive.<sup class=""reference"" id=""cite_ref-40""><a href=""#cite_note-40"">[40]</a></sup>
</p>, <p>Open blockchains are more <a href=""/wiki/Usability"" title=""Usability"">user-friendly</a> than some traditional ownership records, which, while open to the public, still require physical access to view. Because all early blockchains were permissionless, controversy has arisen over the blockchain definition. An issue in this ongoing debate is whether a private system with verifiers tasked and authorized (permissioned) by a central authority should be considered a blockchain.<sup class=""reference"" id=""cite_ref-t16_41-0""><a href=""#cite_note-t16-41"">[41]</a></sup><sup class=""reference"" id=""cite_ref-t10_42-0""><a href=""#cite_note-t10-42"">[42]</a></sup><sup class=""reference"" id=""cite_ref-t11_43-0""><a href=""#cite_note-t11-43"">[43]</a></sup><sup class=""reference"" id=""cite_ref-t8_44-0""><a href=""#cite_note-t8-44"">[44]</a></sup><sup class=""reference"" id=""cite_ref-t9_45-0""><a href=""#cite_note-t9-45"">[45]</a></sup> Proponents of permissioned or private chains argue that the term ""blockchain"" may be applied to any <b>data structure</b> that batches data into time-stamped blocks. These blockchains serve as a distributed version of <a href=""/wiki/Multiversion_concurrency_control"" title=""Multiversion concurrency control"">multiversion concurrency control</a> (MVCC) in databases.<sup class=""reference"" id=""cite_ref-t4_46-0""><a href=""#cite_note-t4-46"">[46]</a></sup> Just as MVCC prevents two transactions from concurrently modifying a single object in a database, blockchains prevent two transactions from spending the same single output in a blockchain.<sup class=""reference"" id=""cite_ref-tapscott201605_47-0""><a href=""#cite_note-tapscott201605-47"">[47]</a></sup><sup class=""reference nowrap""><span title=""Page / location: 30–31"">: 30–31 </span></sup> Opponents say that permissioned systems resemble traditional corporate databases, not supporting decentralized data verification, and that such systems are not hardened against operator tampering and revision.<sup class=""reference"" id=""cite_ref-t16_41-1""><a href=""#cite_note-t16-41"">[41]</a></sup><sup class=""reference"" id=""cite_ref-t11_43-1""><a href=""#cite_note-t11-43"">[43]</a></sup> Nikolai Hampton of <i><a href=""/wiki/Computerworld"" title=""Computerworld"">Computerworld</a></i> said that ""many in-house blockchain solutions will be nothing more than cumbersome databases,"" and ""without a clear security model, proprietary blockchains should be eyed with suspicion.""<sup class=""reference"" id=""cite_ref-cw20160905_8-1""><a href=""#cite_note-cw20160905-8"">[8]</a></sup><sup class=""reference"" id=""cite_ref-48""><a href=""#cite_note-48"">[48]</a></sup>
</p>, <p>An advantage to an open, permissionless, or public, blockchain network is that guarding against bad actors is not required and no <a href=""/wiki/Access_control"" title=""Access control"">access control</a> is needed.<sup class=""reference"" id=""cite_ref-bsm_26-1""><a href=""#cite_note-bsm-26"">[26]</a></sup> This means that applications can be added to the network without the approval or trust of others, using the blockchain as a <a href=""/wiki/Transport_layer"" title=""Transport layer"">transport layer</a>.<sup class=""reference"" id=""cite_ref-bsm_26-2""><a href=""#cite_note-bsm-26"">[26]</a></sup>
</p>, <p>Bitcoin and other cryptocurrencies currently secure their blockchain by requiring new entries to include proof of work. To prolong the blockchain, bitcoin uses <a href=""/wiki/Hashcash"" title=""Hashcash"">Hashcash</a> puzzles. While Hashcash was designed in 1997 by <a href=""/wiki/Adam_Back"" title=""Adam Back"">Adam Back</a>, the original idea was first proposed by <a href=""/wiki/Cynthia_Dwork"" title=""Cynthia Dwork"">Cynthia Dwork</a> and <a href=""/wiki/Moni_Naor"" title=""Moni Naor"">Moni Naor</a> and Eli Ponyatovski in their 1992 paper ""Pricing via Processing or Combatting Junk Mail"".
</p>, <p>In 2016, <a href=""/wiki/Venture_capital"" title=""Venture capital"">venture capital</a> investment for blockchain-related projects was weakening in the USA but increasing in China.<sup class=""reference"" id=""cite_ref-btt17_49-0""><a href=""#cite_note-btt17-49"">[49]</a></sup> Bitcoin and many other cryptocurrencies use open (public) blockchains. As of April 2018<sup class=""plainlinks noexcerpt noprint asof-tag update"" style=""display:none;""><a class=""external text"" href=""https://en.wikipedia.org/w/index.php?title=Blockchain&amp;action=edit"">[update]</a></sup>, bitcoin has the highest market capitalization.
</p>, <p>Permissioned blockchains use an access control layer to govern who has access to the network.<sup class=""reference"" id=""cite_ref-btit_50-0""><a href=""#cite_note-btit-50"">[50]</a></sup> In contrast to public blockchain networks, validators on private blockchain networks are vetted by the network owner. They do not rely on anonymous nodes to validate transactions nor do they benefit from the <a href=""/wiki/Network_effect"" title=""Network effect"">network effect</a>.<sup class=""noprint Inline-Template Template-Fact"" style=""white-space:nowrap;"">[<i><a href=""/wiki/Wikipedia:Citation_needed"" title=""Wikipedia:Citation needed""><span title=""This claim needs references to reliable sources. (May 2019)"">citation needed</span></a></i>]</sup> Permissioned blockchains can also go by the name of 'consortium' blockchains.<sup class=""noprint Inline-Template Template-Fact"" style=""white-space:nowrap;"">[<i><a href=""/wiki/Wikipedia:Citation_needed"" title=""Wikipedia:Citation needed""><span title=""This claim needs references to reliable sources. (February 2020)"">citation needed</span></a></i>]</sup> It has been argued that permissioned blockchains can guarantee a certain level of decentralization, if carefully designed, as opposed to permissionless blockchains, which are often centralized in practice.<sup class=""reference"" id=""cite_ref-auto_9-1""><a href=""#cite_note-auto-9"">[9]</a></sup>
</p>, <p>Nikolai Hampton pointed out in <i><a href=""/wiki/Computerworld"" title=""Computerworld"">Computerworld</a></i> that ""There is also no need for a '51 percent' attack on a private blockchain, as the private blockchain (most likely) already controls 100 percent of all block creation resources. If you could attack or damage the blockchain creation tools on a private corporate server, you could effectively control 100 percent of their network and alter transactions however you wished.""<sup class=""reference"" id=""cite_ref-cw20160905_8-2""><a href=""#cite_note-cw20160905-8"">[8]</a></sup> This has a set of particularly profound adverse implications during a <a href=""/wiki/Financial_crisis"" title=""Financial crisis"">financial crisis</a> or <a href=""/wiki/Debt_crisis"" title=""Debt crisis"">debt crisis</a> like the <a class=""mw-redirect"" href=""/wiki/Financial_crisis_of_2007%E2%80%9308"" title=""Financial crisis of 2007–08"">financial crisis of 2007–08</a>, where politically powerful actors may make decisions that favor some groups at the expense of others,<sup class=""reference"" id=""cite_ref-51""><a href=""#cite_note-51"">[51]</a></sup> and ""the bitcoin blockchain is protected by the massive group mining effort. It's unlikely that any private blockchain will try to protect records using <a class=""mw-redirect"" href=""/wiki/Gigawatts"" title=""Gigawatts"">gigawatts</a> of computing power — it's time-consuming and expensive.""<sup class=""reference"" id=""cite_ref-cw20160905_8-3""><a href=""#cite_note-cw20160905-8"">[8]</a></sup> He also said, ""Within a private blockchain there is also no 'race'; there's no incentive to use more power or discover blocks faster than competitors. This means that many in-house blockchain solutions will be nothing more than cumbersome databases.""<sup class=""reference"" id=""cite_ref-cw20160905_8-4""><a href=""#cite_note-cw20160905-8"">[8]</a></sup>
</p>, <p>The <a href=""/wiki/Blockchain_analysis"" title=""Blockchain analysis"">analysis of public blockchains</a> has become increasingly important with the popularity of <a href=""/wiki/Bitcoin"" title=""Bitcoin"">bitcoin</a>, <a href=""/wiki/Ethereum"" title=""Ethereum"">Ethereum</a>, <a href=""/wiki/Litecoin"" title=""Litecoin"">litecoin</a> and other <a class=""mw-redirect"" href=""/wiki/Cryptocurrencies"" title=""Cryptocurrencies"">cryptocurrencies</a>.<sup class=""reference"" id=""cite_ref-52""><a href=""#cite_note-52"">[52]</a></sup> A blockchain, if it is public, provides anyone who wants access to observe and analyse the chain data, given one has the know-how. The process of understanding and accessing the flow of crypto has been an issue for many cryptocurrencies, crypto exchanges and banks.<sup class=""reference"" id=""cite_ref-53""><a href=""#cite_note-53"">[53]</a></sup><sup class=""reference"" id=""cite_ref-54""><a href=""#cite_note-54"">[54]</a></sup> The reason for this is accusations of blockchain-enabled cryptocurrencies enabling illicit <a href=""/wiki/Darknet_market"" title=""Darknet market"">dark market</a> trade of drugs, weapons, money laundering, etc.<sup class=""reference"" id=""cite_ref-55""><a href=""#cite_note-55"">[55]</a></sup> A common belief has been that cryptocurrency is private and untraceable, thus leading many actors to use it for illegal purposes. This is changing and now specialised tech companies provide blockchain tracking services, making crypto exchanges, law-enforcement and banks more aware of what is happening with crypto funds and <a href=""/wiki/Fiat_money"" title=""Fiat money"">fiat</a>-crypto exchanges. The development, some argue, has led criminals to prioritise the use of new cryptos such as <a class=""mw-redirect"" href=""/wiki/Monero_(cryptocurrency)"" title=""Monero (cryptocurrency)"">Monero</a>.<sup class=""reference"" id=""cite_ref-56""><a href=""#cite_note-56"">[56]</a></sup><sup class=""reference"" id=""cite_ref-57""><a href=""#cite_note-57"">[57]</a></sup><sup class=""reference"" id=""cite_ref-58""><a href=""#cite_note-58"">[58]</a></sup> The question is about the public accessibility of blockchain data and the personal privacy of the very same data. It is a key debate in cryptocurrency and ultimately in the blockchain.<sup class=""reference"" id=""cite_ref-59""><a href=""#cite_note-59"">[59]</a></sup>
</p>, <p>In April 2016, <a href=""/wiki/Standards_Australia"" title=""Standards Australia"">Standards Australia</a> submitted a proposal to the <a href=""/wiki/International_Organization_for_Standardization"" title=""International Organization for Standardization"">International Organization for Standardization</a> to consider developing standards to support blockchain technology. This proposal resulted in the creation of ISO Technical Committee 307, Blockchain and Distributed Ledger Technologies.<sup class=""reference"" id=""cite_ref-Blockchain_60-0""><a href=""#cite_note-Blockchain-60"">[60]</a></sup> The technical committee has working groups relating to blockchain terminology, reference architecture, security and privacy, identity, smart contracts, governance and interoperability for blockchain and DLT, as well as standards specific to industry sectors and generic government requirements.<sup class=""reference"" id=""cite_ref-ISO/TC_307_Blockchain_and_distributed_ledger_technologies_61-0""><a href=""#cite_note-ISO/TC_307_Blockchain_and_distributed_ledger_technologies-61"">[61]</a></sup><sup class=""noprint Inline-Template noprint Template-Fact"" style=""white-space:nowrap;"">[<i><a href=""/wiki/Wikipedia:No_original_research#Primary,_secondary_and_tertiary_sources"" title=""Wikipedia:No original research""><span title=""This claim needs references to reliable secondary sources. (January 2022)"">non-primary source needed</span></a></i>]</sup> More than 50 countries are participating in the standardization process together with external liaisons such as the <a class=""mw-redirect"" href=""/wiki/Society_for_Worldwide_Interbank_Financial_Telecommunication"" title=""Society for Worldwide Interbank Financial Telecommunication"">Society for Worldwide Interbank Financial Telecommunication</a> (SWIFT), the <a href=""/wiki/European_Commission"" title=""European Commission"">European Commission</a>, the <a href=""/wiki/International_Federation_of_Surveyors"" title=""International Federation of Surveyors"">International Federation of Surveyors</a>, the <a href=""/wiki/International_Telecommunication_Union"" title=""International Telecommunication Union"">International Telecommunication Union</a> (ITU) and the <a href=""/wiki/United_Nations_Economic_Commission_for_Europe"" title=""United Nations Economic Commission for Europe"">United Nations Economic Commission for Europe</a> (UNECE).<sup class=""reference"" id=""cite_ref-ISO/TC_307_Blockchain_and_distributed_ledger_technologies_61-1""><a href=""#cite_note-ISO/TC_307_Blockchain_and_distributed_ledger_technologies-61"">[61]</a></sup>
</p>, <p>Many other national standards bodies and open standards bodies are also working on blockchain standards.<sup class=""reference"" id=""cite_ref-62""><a href=""#cite_note-62"">[62]</a></sup>  These include the <a href=""/wiki/National_Institute_of_Standards_and_Technology"" title=""National Institute of Standards and Technology"">National Institute of Standards and Technology</a><sup class=""reference"" id=""cite_ref-BLOCKCHAIN_Overview_63-0""><a href=""#cite_note-BLOCKCHAIN_Overview-63"">[63]</a></sup> (NIST), the <a href=""/wiki/European_Committee_for_Electrotechnical_Standardization"" title=""European Committee for Electrotechnical Standardization"">European Committee for Electrotechnical Standardization</a><sup class=""reference"" id=""cite_ref-CEN_and_CENELEC_publish_a_White_Paper_on_standards_in_Blockchain_&amp;_Distributed_Ledger_Technologies_64-0""><a href=""#cite_note-CEN_and_CENELEC_publish_a_White_Paper_on_standards_in_Blockchain_&amp;_Distributed_Ledger_Technologies-64"">[64]</a></sup> (CENELEC), the <a href=""/wiki/Institute_of_Electrical_and_Electronics_Engineers"" title=""Institute of Electrical and Electronics Engineers"">Institute of Electrical and Electronics Engineers</a><sup class=""reference"" id=""cite_ref-Standards_65-0""><a href=""#cite_note-Standards-65"">[65]</a></sup> (IEEE), the Organization for the Advancement of Structured Information Standards (<a href=""/wiki/OASIS_(organization)"" title=""OASIS (organization)"">OASIS</a>), and some individual participants in the  <a href=""/wiki/Internet_Engineering_Task_Force"" title=""Internet Engineering Task Force"">Internet Engineering Task Force</a><sup class=""reference"" id=""cite_ref-An_Interoperability_Architecture_for_Blockchain/DLT_Gateways_draft-hardjono-blockchain-interop-arch-02_66-0""><a href=""#cite_note-An_Interoperability_Architecture_for_Blockchain/DLT_Gateways_draft-hardjono-blockchain-interop-arch-02-66"">[66]</a></sup> (IETF).
</p>, <p>Currently, there are at least four types of blockchain networks — public blockchains, private blockchains, <a href=""/wiki/Consortium"" title=""Consortium"">consortium</a> blockchains and hybrid blockchains.
</p>, <p>A public blockchain has absolutely no access restrictions. Anyone with an <a href=""/wiki/Internet"" title=""Internet"">Internet</a> connection can send <a href=""/wiki/Financial_transaction"" title=""Financial transaction"">transactions</a> to it as well as become a <a href=""/wiki/Validator"" title=""Validator"">validator</a> (i.e., participate in the execution of a <a href=""/wiki/Consensus_(computer_science)"" title=""Consensus (computer science)"">consensus protocol</a>).<sup class=""reference"" id=""cite_ref-67""><a href=""#cite_note-67"">[67]</a></sup><sup class=""noprint Inline-Template"" style=""white-space:nowrap;"">[<i><a href=""/wiki/Wikipedia:Verifiability#Self-published_sources"" title=""Wikipedia:Verifiability""><span title=""The material near this tag may rely on a self-published source. (April 2018)"">self-published source?</span></a></i>]</sup> Usually, such networks offer <a href=""/wiki/Incentive"" title=""Incentive"">economic incentives</a> for those who secure them and utilize some type of a <a class=""mw-redirect"" href=""/wiki/Proof-of-stake"" title=""Proof-of-stake"">Proof of Stake</a> or <a class=""mw-redirect"" href=""/wiki/Proof-of-work_system"" title=""Proof-of-work system"">Proof of Work</a> algorithm.
</p>, <p>Some of the largest, most known public blockchains are the bitcoin blockchain and the Ethereum blockchain.
</p>, <p>A private blockchain is permissioned.<sup class=""reference"" id=""cite_ref-btit_50-1""><a href=""#cite_note-btit-50"">[50]</a></sup> One cannot join it unless invited by the network administrators. Participant and validator access is <a href=""/wiki/Closed_platform"" title=""Closed platform"">restricted</a>. To distinguish between open blockchains and other peer-to-peer decentralized database applications that are not open ad-hoc compute clusters, the terminology <a class=""mw-redirect"" href=""/wiki/Distributed_Ledger"" title=""Distributed Ledger"">Distributed Ledger</a> (DLT) is normally used for private blockchains.
</p>, <p>A hybrid blockchain has a combination of centralized and decentralized features.<sup class=""reference"" id=""cite_ref-68""><a href=""#cite_note-68"">[68]</a></sup> The exact workings of the chain can vary based on which portions of centralization and decentralization are used.
</p>, <p>A sidechain is a designation for a blockchain ledger that runs in parallel to a primary blockchain.<sup class=""reference"" id=""cite_ref-69""><a href=""#cite_note-69"">[69]</a></sup><sup class=""reference"" id=""cite_ref-70""><a href=""#cite_note-70"">[70]</a></sup> Entries from the primary blockchain (where said entries typically represent <a href=""/wiki/Digital_asset"" title=""Digital asset"">digital assets</a>) can be linked to and from the sidechain; this allows the sidechain to otherwise operate independently of the primary blockchain (e.g., by using an alternate means of record keeping, alternate <a href=""/wiki/Consensus_(computer_science)"" title=""Consensus (computer science)"">consensus algorithm</a>, etc.).<sup class=""reference"" id=""cite_ref-71""><a href=""#cite_note-71"">[71]</a></sup><sup class=""noprint Inline-Template noprint noexcerpt Template-Fact"" style=""white-space:nowrap;"">[<i><a class=""mw-redirect"" href=""/wiki/Wikipedia:NOTRS"" title=""Wikipedia:NOTRS""><span title=""This claim needs references to better sources. (November 2021)"">better source needed</span></a></i>]</sup>
</p>, <p>Blockchain technology can be integrated into multiple areas. The primary use of blockchains is as a <a href=""/wiki/Distributed_ledger"" title=""Distributed ledger"">distributed ledger</a> for <a href=""/wiki/Cryptocurrency"" title=""Cryptocurrency"">cryptocurrencies</a> such as <a href=""/wiki/Bitcoin"" title=""Bitcoin"">bitcoin</a>; there were also a few other operational products that had matured from <a href=""/wiki/Proof_of_concept"" title=""Proof of concept"">proof of concept</a> by late 2016.<sup class=""reference"" id=""cite_ref-btt17_49-1""><a href=""#cite_note-btt17-49"">[49]</a></sup> As of 2016, some businesses have been testing the technology and conducting low-level implementation to gauge blockchain's effects on organizational efficiency in their <a href=""/wiki/Back_office"" title=""Back office"">back office</a>.<sup class=""reference"" id=""cite_ref-72""><a href=""#cite_note-72"">[72]</a></sup>
</p>, <p>In 2019, it was estimated that around $2.9 billion were invested in blockchain technology, which represents an 89% increase from the year prior. Additionally, the International Data Corp has estimated that corporate investment into blockchain technology will reach $12.4 billion by 2022.<sup class=""reference"" id=""cite_ref-73""><a href=""#cite_note-73"">[73]</a></sup> Furthermore, According to <a href=""/wiki/PricewaterhouseCoopers"" title=""PricewaterhouseCoopers"">PricewaterhouseCoopers</a> (PwC), the second-largest professional services network in the world, blockchain technology has the potential to generate an annual business value of more than $3 trillion by 2030. PwC's estimate is further augmented by a 2018 study that they have conducted, in which PwC surveyed 600 business executives and determined that 84% have at least some exposure to utilizing blockchain technology, which indicts a significant demand and interest in blockchain technology.<sup class=""reference"" id=""cite_ref-74""><a href=""#cite_note-74"">[74]</a></sup>
</p>, <p>Individual use of blockchain technology has also greatly increased since 2016. According to statistics in 2020, there were more than 40 million blockchain wallets in 2020 in comparison to around 10 million blockchain wallets in 2016.<sup class=""reference"" id=""cite_ref-75""><a href=""#cite_note-75"">[75]</a></sup>
</p>, <p>Most cryptocurrencies use blockchain technology to record transactions. For example, the <a href=""/wiki/Bitcoin_network"" title=""Bitcoin network"">bitcoin network</a> and <a href=""/wiki/Ethereum"" title=""Ethereum"">Ethereum</a> network are both based on blockchain. On 8 May 2018 <a href=""/wiki/Facebook"" title=""Facebook"">Facebook</a> confirmed that it would open a new blockchain group<sup class=""reference"" id=""cite_ref-76""><a href=""#cite_note-76"">[76]</a></sup> which would be headed by <a href=""/wiki/David_A._Marcus"" title=""David A. Marcus"">David Marcus</a>, who previously was in charge of <a class=""mw-redirect"" href=""/wiki/Facebook_Messenger"" title=""Facebook Messenger"">Messenger</a>. Facebook's planned cryptocurrency platform, <a class=""mw-redirect"" href=""/wiki/Libra_(cryptocurrency)"" title=""Libra (cryptocurrency)"">Libra</a> (now known as Diem), was formally announced on June 18, 2019.<sup class=""reference"" id=""cite_ref-77""><a href=""#cite_note-77"">[77]</a></sup><sup class=""reference"" id=""cite_ref-78""><a href=""#cite_note-78"">[78]</a></sup>
</p>, <p>The criminal enterprise <a href=""/wiki/Silk_Road_(marketplace)"" title=""Silk Road (marketplace)"">Silk Road</a>, which operated on <a class=""mw-redirect"" href=""/wiki/Tor_(anonymity_network)"" title=""Tor (anonymity network)"">Tor</a>, utilized cryptocurrency for payments, some of which the <a class=""mw-redirect"" href=""/wiki/US_federal_government"" title=""US federal government"">US federal government</a> has seized through research on the blockchain and <a href=""/wiki/Forfeiture_(law)"" title=""Forfeiture (law)"">forfeiture.</a><sup class=""reference"" id=""cite_ref-79""><a href=""#cite_note-79"">[79]</a></sup>
</p>, <p><a class=""mw-redirect"" href=""/wiki/Legality_of_bitcoin_by_country_or_territory"" title=""Legality of bitcoin by country or territory"">Governments have mixed policies</a> on the legality of their citizens or banks
owning cryptocurrencies. China implements blockchain technology in several industries including a <a href=""/wiki/Central_bank_digital_currency"" title=""Central bank digital currency"">national digital currency</a> which launched in 2020.<sup class=""reference"" id=""cite_ref-80""><a href=""#cite_note-80"">[80]</a></sup> To strengthen their respective currencies, Western governments including the European Union and the United States have initiated similar projects.<sup class=""reference"" id=""cite_ref-81""><a href=""#cite_note-81"">[81]</a></sup>
</p>, <p>Blockchain-based <a href=""/wiki/Smart_contract"" title=""Smart contract"">smart contracts</a> are proposed contracts that can be partially or fully executed or enforced without human interaction.<sup class=""reference"" id=""cite_ref-82""><a href=""#cite_note-82"">[82]</a></sup> One of the main objectives of a smart contract is <a href=""/wiki/Automation"" title=""Automation"">automated</a> <a href=""/wiki/Escrow"" title=""Escrow"">escrow</a>. A key feature of smart contracts is that they do not need a trusted third party (such as a trustee) to act as an intermediary between contracting entities -the blockchain network executes the contract on its own. This may reduce friction between entities when transferring value and could subsequently open the door to a higher level of transaction automation.<sup class=""reference"" id=""cite_ref-83""><a href=""#cite_note-83"">[83]</a></sup> An <a href=""/wiki/International_Monetary_Fund"" title=""International Monetary Fund"">IMF</a> staff discussion from 2018 reported that smart contracts based on blockchain technology might reduce <a href=""/wiki/Moral_hazard"" title=""Moral hazard"">moral hazards</a> and optimize the use of contracts in general. But ""no viable smart contract systems have yet emerged."" Due to the lack of widespread use their legal status was unclear.<sup class=""reference"" id=""cite_ref-84""><a href=""#cite_note-84"">[84]</a></sup><sup class=""reference"" id=""cite_ref-85""><a href=""#cite_note-85"">[85]</a></sup>
</p>, <p>According to <i><a href=""/wiki/Reason_(magazine)"" title=""Reason (magazine)"">Reason</a></i>, many banks have expressed interest in implementing <a href=""/wiki/Distributed_ledger"" title=""Distributed ledger"">distributed ledgers</a> for use in <a class=""mw-redirect"" href=""/wiki/Banking"" title=""Banking"">banking</a> and are cooperating with companies creating private blockchains,<sup class=""reference"" id=""cite_ref-86""><a href=""#cite_note-86"">[86]</a></sup><sup class=""reference"" id=""cite_ref-87""><a href=""#cite_note-87"">[87]</a></sup><sup class=""reference"" id=""cite_ref-88""><a href=""#cite_note-88"">[88]</a></sup> and according to a September 2016 <a href=""/wiki/IBM"" title=""IBM"">IBM</a> study, this is occurring faster than expected.<sup class=""reference"" id=""cite_ref-89""><a href=""#cite_note-89"">[89]</a></sup>
</p>, <p>Banks are interested in this technology not least because it has the potential to speed up <a href=""/wiki/Back_office"" title=""Back office"">back office</a> settlement systems.<sup class=""reference"" id=""cite_ref-90""><a href=""#cite_note-90"">[90]</a></sup> Moreover, as the blockchain industry has reached early maturity institutional appreciation has grown that it is, practically speaking, the infrastructure of a whole new financial industry, with all the implications which that entails.<sup class=""reference"" id=""cite_ref-91""><a href=""#cite_note-91"">[91]</a></sup>
</p>, <p><a href=""/wiki/Bank"" title=""Bank"">Banks</a> such as <a href=""/wiki/UBS"" title=""UBS"">UBS</a> are opening new research labs dedicated to blockchain technology in order to explore how blockchain can be used in financial services to increase efficiency and reduce costs.<sup class=""reference"" id=""cite_ref-92""><a href=""#cite_note-92"">[92]</a></sup><sup class=""reference"" id=""cite_ref-93""><a href=""#cite_note-93"">[93]</a></sup>
</p>, <p><a href=""/wiki/Berenberg_Bank"" title=""Berenberg Bank"">Berenberg</a>, a German bank, believes that blockchain is an ""overhyped technology"" that has had a large number of ""proofs of concept"", but still has major challenges, and very few success stories.<sup class=""reference"" id=""cite_ref-94""><a href=""#cite_note-94"">[94]</a></sup>
</p>, <p>The blockchain has also given rise to <a href=""/wiki/Initial_coin_offering"" title=""Initial coin offering"">initial coin offerings</a> (ICOs) as well as a new category of digital asset called security token offerings (STOs), also sometimes referred to as digital security offerings (DSOs).<sup class=""reference"" id=""cite_ref-95""><a href=""#cite_note-95"">[95]</a></sup> STO/DSOs may be conducted privately or on public, regulated stock exchange and are used to tokenize traditional assets such as company shares as well as more innovative ones like intellectual property, real estate,<sup class=""reference"" id=""cite_ref-96""><a href=""#cite_note-96"">[96]</a></sup> art, or individual products. A number of companies are active in this space providing services for compliant tokenization, private STOs, and public STOs.
</p>, <p>Blockchain technology, such as cryptocurrencies and <a href=""/wiki/Non-fungible_token"" title=""Non-fungible token"">non-fungible tokens</a> (NFTs), has been used in video games for <a href=""/wiki/Video_game_monetization"" title=""Video game monetization"">monetization</a>. Many <a href=""/wiki/Games_as_a_service"" title=""Games as a service"">live-service games</a> offer in-game customization options, such as character skins or other in-game items, which the players can earn and trade with other players using in-game currency. Some games also allow for trading of virtual items using real-world currency, but this may be illegal in some countries where video games are seen as akin to gambling, and has led to <a class=""mw-redirect"" href=""/wiki/Gray_market"" title=""Gray market"">gray market</a> issues such as <a href=""/wiki/Skin_gambling"" title=""Skin gambling"">skin gambling</a>, and thus publishers typically have shied away from allowing players to earn real-world funds from games.<sup class=""reference"" id=""cite_ref-Verge-Valve_97-0""><a href=""#cite_note-Verge-Valve-97"">[97]</a></sup> Blockchain games typically allow players to trade these in-game items for cryptocurrency, which can then be exchanged for money.<sup class=""reference"" id=""cite_ref-inverse_blockchain_games_98-0""><a href=""#cite_note-inverse_blockchain_games-98"">[98]</a></sup>
</p>, <p>The first known game to use blockchain technologies was <i><a href=""/wiki/CryptoKitties"" title=""CryptoKitties"">CryptoKitties</a></i>, launched in November 2017, where the player would purchase NFTs with Ethereum cryptocurrency, each NFT consisting of a <a class=""mw-redirect"" href=""/wiki/Virtual_pet"" title=""Virtual pet"">virtual pet</a> that the player could breed with others to create offspring with combined traits as new NFTs.<sup class=""reference"" id=""cite_ref-99""><a href=""#cite_note-99"">[99]</a></sup><sup class=""reference"" id=""cite_ref-inverse_blockchain_games_98-1""><a href=""#cite_note-inverse_blockchain_games-98"">[98]</a></sup> The game made headlines in December 2017 when one virtual pet sold for more than <a href=""/wiki/United_States_dollar"" title=""United States dollar"">US$</a>100,000.<sup class=""reference"" id=""cite_ref-100""><a href=""#cite_note-100"">[100]</a></sup> <i>CryptoKitties</i> also illustrated scalability problems for games on Ethereum when it created significant congestion on the Ethereum network in early 2018 with approximately 30% of all Ethereum transactions<sup class=""noprint Inline-Template"" style=""margin-left:0.1em; white-space:nowrap;"">[<i><a href=""/wiki/Wikipedia:Please_clarify"" title=""Wikipedia:Please clarify""><span title=""The text near this tag may need clarification or removal of jargon. (October 2021)"">clarification needed</span></a></i>]</sup> being for the game.<sup class=""reference"" id=""cite_ref-101""><a href=""#cite_note-101"">[101]</a></sup><sup class=""reference"" id=""cite_ref-102""><a href=""#cite_note-102"">[102]</a></sup>
</p>, <p>By the early 2020s, there had not been a breakout success in video games using blockchain, as these games tend to focus on using blockchain for speculation instead of more traditional forms of gameplay, which offers limited appeal to most players. Such games also represent a high risk to investors as their revenues can be difficult to predict.<sup class=""reference"" id=""cite_ref-inverse_blockchain_games_98-2""><a href=""#cite_note-inverse_blockchain_games-98"">[98]</a></sup> However, limited successes of some games, such as <i><a href=""/wiki/Axie_Infinity"" title=""Axie Infinity"">Axie Infinity</a></i> during the <a href=""/wiki/COVID-19_pandemic"" title=""COVID-19 pandemic"">COVID-19 pandemic</a>, and corporate plans towards <a href=""/wiki/Metaverse"" title=""Metaverse"">metaverse</a> content, refueled interest in the area of GameFi, a term describing the intersection of video games and financing typically backed by blockchain currency, in the second half of 2021.<sup class=""reference"" id=""cite_ref-103""><a href=""#cite_note-103"">[103]</a></sup> Several major publishers, including <a href=""/wiki/Ubisoft"" title=""Ubisoft"">Ubisoft</a>, <a href=""/wiki/Electronic_Arts"" title=""Electronic Arts"">Electronic Arts</a>, and <a class=""mw-redirect"" href=""/wiki/Take_Two_Interactive"" title=""Take Two Interactive"">Take Two Interactive</a>, have stated that blockchain and NFT-based games are under serious consideration for their companies in the future.<sup class=""reference"" id=""cite_ref-104""><a href=""#cite_note-104"">[104]</a></sup>
</p>, <p>In October 2021, <a href=""/wiki/Valve_Corporation"" title=""Valve Corporation"">Valve Corporation</a> banned blockchain games, including those using cryptocurrency and NFTs, from being hosted on its <a href=""/wiki/Steam_(service)"" title=""Steam (service)"">Steam</a> digital storefront service, which is widely used for personal computer gaming, claiming that this was an extension of their policy banning games that offered in-game items with real-world value. Valve's prior history with <a href=""/wiki/Gambling"" title=""Gambling"">gambling</a>, specifically <a href=""/wiki/Skin_gambling"" title=""Skin gambling"">skin gambling</a>, was speculated to be a factor in the decision to ban blockchain games.<sup class=""reference"" id=""cite_ref-pcgamer_105-0""><a href=""#cite_note-pcgamer-105"">[105]</a></sup> Journalists and players responded positively to Valve's decision as blockchain and NFT games have a reputation for scams and fraud among most PC gamers,<sup class=""reference"" id=""cite_ref-Verge-Valve_97-1""><a href=""#cite_note-Verge-Valve-97"">[97]</a></sup><sup class=""reference"" id=""cite_ref-pcgamer_105-1""><a href=""#cite_note-pcgamer-105"">[105]</a></sup> <a href=""/wiki/Epic_Games"" title=""Epic Games"">Epic Games</a>, which runs the <a href=""/wiki/Epic_Games_Store"" title=""Epic Games Store"">Epic Games Store</a> in competition to Steam, said that they would be open to accepted blockchain games, in the wake of Valve's refusal.<sup class=""reference"" id=""cite_ref-106""><a href=""#cite_note-106"">[106]</a></sup>
</p>, <p>There have been several different efforts to employ blockchains in <a href=""/wiki/Supply_chain_management"" title=""Supply chain management"">supply chain management</a>.
</p>, <p>There are several different efforts to offer <a href=""/wiki/Domain_name"" title=""Domain name"">domain name</a> services via the blockchain. These domain names can be controlled by the use of a private key, which purports to allow for uncensorable websites. This would also bypass a registrar's ability to suppress domains used for fraud, abuse, or illegal content.<sup class=""reference"" id=""cite_ref-techrepublic_115-0""><a href=""#cite_note-techrepublic-115"">[115]</a></sup>
</p>, <p><a href=""/wiki/Namecoin"" title=""Namecoin"">Namecoin</a> is a cryptocurrency that supports the "".bit"" <a href=""/wiki/Top-level_domain"" title=""Top-level domain"">top-level domain</a> (TLD). Namecoin was forked from bitcoin in 2011. The .bit TLD is not sanctioned by <a href=""/wiki/ICANN"" title=""ICANN"">ICANN</a>, instead requiring an <a href=""/wiki/Alternative_DNS_root"" title=""Alternative DNS root"">alternative DNS root</a>.<sup class=""reference"" id=""cite_ref-techrepublic_115-1""><a href=""#cite_note-techrepublic-115"">[115]</a></sup> As of 2015, it was used by 28 websites, out of 120,000 registered names.<sup class=""reference"" id=""cite_ref-orcutt_116-0""><a href=""#cite_note-orcutt-116"">[116]</a></sup> Namecoin was dropped by <a href=""/wiki/OpenNIC"" title=""OpenNIC"">OpenNIC</a> in 2019, due to malware and potential other legal issues.<sup class=""reference"" id=""cite_ref-117""><a href=""#cite_note-117"">[117]</a></sup> Other blockchain alternatives to ICANN include The Handshake Network,<sup class=""reference"" id=""cite_ref-orcutt_116-1""><a href=""#cite_note-orcutt-116"">[116]</a></sup> EmerDNS, and Unstoppable Domains.<sup class=""reference"" id=""cite_ref-techrepublic_115-2""><a href=""#cite_note-techrepublic-115"">[115]</a></sup>
</p>, <p>Specific TLDs include "".eth"", "".luxe"", and "".kred"", which are associated with the Ethereum blockchain through the Ethereum Name Service (ENS). The .kred TLD also acts as an alternative to conventional <a href=""/wiki/Cryptocurrency_wallet"" title=""Cryptocurrency wallet"">cryptocurrency wallet</a> addresses, as a convenience for transferring cryptocurrency.<sup class=""reference"" id=""cite_ref-118""><a href=""#cite_note-118"">[118]</a></sup>
</p>, <p>Blockchain technology can be used to create a permanent, public, transparent ledger system for compiling data on sales, tracking digital use and payments to content creators, such as wireless users<sup class=""reference"" id=""cite_ref-119""><a href=""#cite_note-119"">[119]</a></sup> or musicians.<sup class=""reference"" id=""cite_ref-120""><a href=""#cite_note-120"">[120]</a></sup> The Gartner 2019 CIO Survey reported 2% of higher education respondents had launched blockchain projects and another 18% were planning academic projects in the next 24 months.<sup class=""reference"" id=""cite_ref-121""><a href=""#cite_note-121"">[121]</a></sup> In 2017, <a href=""/wiki/IBM"" title=""IBM"">IBM</a> partnered with <a class=""mw-redirect"" href=""/wiki/ASCAP"" title=""ASCAP"">ASCAP</a> and <a href=""/wiki/PRS_for_Music"" title=""PRS for Music"">PRS for Music</a> to adopt blockchain technology in music distribution.<sup class=""reference"" id=""cite_ref-122""><a href=""#cite_note-122"">[122]</a></sup> <a href=""/wiki/Imogen_Heap"" title=""Imogen Heap"">Imogen Heap</a>'s Mycelia service has also been proposed as a blockchain-based alternative ""that gives artists more control over how their songs and associated data circulate among fans and other musicians.""<sup class=""reference"" id=""cite_ref-123""><a href=""#cite_note-123"">[123]</a></sup><sup class=""reference"" id=""cite_ref-124""><a href=""#cite_note-124"">[124]</a></sup>
</p>, <p>New distribution methods are available for the <a href=""/wiki/Insurance"" title=""Insurance"">insurance</a> industry such as <a href=""/wiki/Peer-to-peer_insurance"" title=""Peer-to-peer insurance"">peer-to-peer insurance</a>, <a href=""/wiki/Parametric_insurance"" title=""Parametric insurance"">parametric insurance</a> and <a href=""/wiki/Microinsurance"" title=""Microinsurance"">microinsurance</a> following the adoption of blockchain.<sup class=""reference"" id=""cite_ref-125""><a href=""#cite_note-125"">[125]</a></sup><sup class=""reference"" id=""cite_ref-126""><a href=""#cite_note-126"">[126]</a></sup> The <a href=""/wiki/Sharing_economy"" title=""Sharing economy"">sharing economy</a> and <a class=""mw-redirect"" href=""/wiki/Internet_of_Things"" title=""Internet of Things"">IoT</a> are also set to benefit from blockchains because they involve many collaborating peers.<sup class=""reference"" id=""cite_ref-127""><a href=""#cite_note-127"">[127]</a></sup> The use of blockchain in libraries is being studied with a grant from the U.S. Institute of Museum and Library Services.<sup class=""reference"" id=""cite_ref-128""><a href=""#cite_note-128"">[128]</a></sup>
</p>, <p>Other blockchain designs include <a href=""/wiki/Hyperledger"" title=""Hyperledger"">Hyperledger</a>, a collaborative effort from the <a href=""/wiki/Linux_Foundation"" title=""Linux Foundation"">Linux Foundation</a> to support blockchain-based distributed ledgers, with projects under this initiative including Hyperledger Burrow (by Monax) and Hyperledger Fabric (spearheaded by IBM).<sup class=""reference"" id=""cite_ref-129""><a href=""#cite_note-129"">[129]</a></sup><sup class=""reference"" id=""cite_ref-130""><a href=""#cite_note-130"">[130]</a></sup><sup class=""reference"" id=""cite_ref-131""><a href=""#cite_note-131"">[131]</a></sup> Another is Quorum, a permissionable private blockchain by <a href=""/wiki/JPMorgan_Chase"" title=""JPMorgan Chase"">JPMorgan Chase</a> with private storage, used for contract applications.<sup class=""reference"" id=""cite_ref-132""><a href=""#cite_note-132"">[132]</a></sup>
</p>, <p>Blockchain is also being used in <a class=""mw-redirect"" href=""/wiki/Peer-to-peer_energy_trading"" title=""Peer-to-peer energy trading"">peer-to-peer energy trading</a>.<sup class=""reference"" id=""cite_ref-133""><a href=""#cite_note-133"">[133]</a></sup><sup class=""reference"" id=""cite_ref-134""><a href=""#cite_note-134"">[134]</a></sup><sup class=""reference"" id=""cite_ref-135""><a href=""#cite_note-135"">[135]</a></sup>
</p>, <p>Blockchain could be used in detecting counterfeits by associating unique identifiers to products, documents and shipments, and storing records associated with transactions that cannot be forged or altered.<sup class=""reference"" id=""cite_ref-136""><a href=""#cite_note-136"">[136]</a></sup><sup class=""reference"" id=""cite_ref-137""><a href=""#cite_note-137"">[137]</a></sup> It is however argued that blockchain technology needs to be supplemented with technologies that provide a strong binding between physical objects and blockchain systems.<sup class=""reference"" id=""cite_ref-138""><a href=""#cite_note-138"">[138]</a></sup> The <a href=""/wiki/European_Union_Intellectual_Property_Office"" title=""European Union Intellectual Property Office"">EUIPO</a> established an Anti-Counterfeiting Blockathon Forum, with the objective of ""defining, piloting and implementing"" an anti-counterfeiting infrastructure at the European level.<sup class=""reference"" id=""cite_ref-139""><a href=""#cite_note-139"">[139]</a></sup><sup class=""reference"" id=""cite_ref-140""><a href=""#cite_note-140"">[140]</a></sup> The Dutch Standardisation organisation NEN uses blockchain together with QR Codes to authenticate certificates.<sup class=""reference"" id=""cite_ref-141""><a href=""#cite_note-141"">[141]</a></sup>
</p>, <p>With the increasing number of blockchain systems appearing, even only those that support cryptocurrencies, blockchain interoperability is becoming a topic of major importance. The objective is to support transferring assets from one blockchain system to another blockchain system. Wegner<sup class=""reference"" id=""cite_ref-wegner_142-0""><a href=""#cite_note-wegner-142"">[142]</a></sup> stated that ""interoperability is the ability of two or more software components to cooperate despite differences in language, interface, and execution platform"". The objective of blockchain interoperability is therefore to support such cooperation among blockchain systems, despite those kinds of differences.
</p>, <p>There are already several blockchain interoperability solutions available.<sup class=""reference"" id=""cite_ref-survey_143-0""><a href=""#cite_note-survey-143"">[143]</a></sup> They can be classified into three categories: cryptocurrency interoperability approaches, blockchain engines, and blockchain connectors.
</p>, <p>Several individual IETF participants produced the draft of a blockchain interoperability architecture.<sup class=""reference"" id=""cite_ref-ietf-draft_144-0""><a href=""#cite_note-ietf-draft-144"">[144]</a></sup>
</p>, <p>Blockchain mining — the peer-to-peer computer computations by which transactions are validated and verified — requires a significant amount of energy. In June 2018 the <a href=""/wiki/Bank_for_International_Settlements"" title=""Bank for International Settlements"">Bank for International Settlements</a> criticized the use of public <a class=""mw-redirect"" href=""/wiki/Proof-of-work"" title=""Proof-of-work"">proof-of-work</a> blockchains for their high energy consumption.<sup class=""reference"" id=""cite_ref-145""><a href=""#cite_note-145"">[145]</a></sup><sup class=""reference"" id=""cite_ref-146""><a href=""#cite_note-146"">[146]</a></sup><sup class=""reference"" id=""cite_ref-147""><a href=""#cite_note-147"">[147]</a></sup> In 2021, a study by <a href=""/wiki/University_of_Cambridge"" title=""University of Cambridge"">Cambridge University</a> determined that Bitcoin (at 121 terawatt-hours per year) used more electricity than Argentina (at 121TWh) and the Netherlands (109TWh).<sup class=""reference"" id=""cite_ref-148""><a href=""#cite_note-148"">[148]</a></sup> According to Digiconomist, one bitcoin transaction required 708 kilowatt-hours of electrical energy, the amount an average U.S. household consumed in 24 days.<sup class=""reference"" id=""cite_ref-149""><a href=""#cite_note-149"">[149]</a></sup>
</p>, <p>In February 2021, U.S. Treasury secretary <a href=""/wiki/Janet_Yellen"" title=""Janet Yellen"">Janet Yellen</a> called Bitcoin ""an extremely inefficient way to conduct transactions"", saying ""the amount of energy consumed in processing those transactions is staggering.""<sup class=""reference"" id=""cite_ref-150""><a href=""#cite_note-150"">[150]</a></sup> In March 2021, <a href=""/wiki/Bill_Gates"" title=""Bill Gates"">Bill Gates</a> stated that ""Bitcoin uses more electricity per transaction than any other method known to mankind"", adding ""It's not a great climate thing.""<sup class=""reference"" id=""cite_ref-151""><a href=""#cite_note-151"">[151]</a></sup>
</p>, <p>Nicholas Weaver, of the <a href=""/wiki/International_Computer_Science_Institute"" title=""International Computer Science Institute"">International Computer Science Institute</a> at the <a href=""/wiki/University_of_California,_Berkeley"" title=""University of California, Berkeley"">University of California, Berkeley</a>, examined blockchain's online security, and the energy efficiency of proof-of-work public blockchains, and in both cases found it grossly inadequate.<sup class=""reference"" id=""cite_ref-152""><a href=""#cite_note-152"">[152]</a></sup><sup class=""reference"" id=""cite_ref-153""><a href=""#cite_note-153"">[153]</a></sup> The 31TWh–45TWh of electricity used for bitcoin in 2018 produced 17–22.9 million tonnes of CO2.<sup class=""reference"" id=""cite_ref-154""><a href=""#cite_note-154"">[154]</a></sup><sup class=""reference"" id=""cite_ref-155""><a href=""#cite_note-155"">[155]</a></sup> By 2022, the University of Cambridge and Digiconomist estimated that the two largest proof-of-work blockchains, Bitcoin and Ethereum, together used twice as much electricity in one year as the whole of Sweden, leading to the release of up to 120 million tonnes of CO2 each year.<sup class=""reference"" id=""cite_ref-156""><a href=""#cite_note-156"">[156]</a></sup>
</p>, <p>Inside the cryptocurrency industry, concern about high energy consumption has led some companies to consider moving from the <a href=""/wiki/Proof_of_work"" title=""Proof of work"">proof of work</a> blockchain model to the less energy-intensive <a href=""/wiki/Proof_of_stake"" title=""Proof of stake"">proof of stake</a> model.<sup class=""reference"" id=""cite_ref-157""><a href=""#cite_note-157"">[157]</a></sup> Academics and researchers have estimated that Bitcoin consumes 100,000 times as much energy as proof-of-stake networks.<sup class=""reference"" id=""cite_ref-158""><a href=""#cite_note-158"">[158]</a></sup><sup class=""reference"" id=""cite_ref-159""><a href=""#cite_note-159"">[159]</a></sup>
</p>, <p>In October 2014, the MIT Bitcoin Club, with funding from MIT alumni, provided undergraduate students at the <a href=""/wiki/Massachusetts_Institute_of_Technology"" title=""Massachusetts Institute of Technology"">Massachusetts Institute of Technology</a> access to $100 of bitcoin. The adoption rates, as studied by <a href=""/wiki/Christian_Catalini"" title=""Christian Catalini"">Catalini</a> and <a href=""/wiki/Catherine_Tucker"" title=""Catherine Tucker"">Tucker</a> (2016), revealed that when people who typically adopt technologies early are given delayed access, they tend to reject the technology.<sup class=""reference"" id=""cite_ref-160""><a href=""#cite_note-160"">[160]</a></sup> Many universities have founded departments focusing on crypto and blockchain, including <a href=""/wiki/Massachusetts_Institute_of_Technology"" title=""Massachusetts Institute of Technology"">MIT</a>, in 2017. In the same year, <a href=""/wiki/University_of_Edinburgh"" title=""University of Edinburgh"">Edinburgh</a> became ""one of the first big European universities to launch a blockchain course"", according to the <i>Financial Times</i>.<sup class=""reference"" id=""cite_ref-161""><a href=""#cite_note-161"">[161]</a></sup>
</p>, <p>Motivations for adopting blockchain technology (an aspect of <a href=""/wiki/Diffusion_of_innovations"" title=""Diffusion of innovations"">innovation adoptation</a>) have been investigated by researchers. For example, Janssen, et al. provided a framework for analysis,<sup class=""reference"" id=""cite_ref-162""><a href=""#cite_note-162"">[162]</a></sup> and Koens &amp; Poll pointed out that adoption could be heavily driven by non-technical factors.<sup class=""reference"" id=""cite_ref-163""><a href=""#cite_note-163"">[163]</a></sup> Based on behavioral models, Li<sup class=""reference"" id=""cite_ref-164""><a href=""#cite_note-164"">[164]</a></sup> has discussed the differences between adoption at the individual level and organizational levels.
</p>, <p>Scholars in business and management have started studying the role of blockchains to support collaboration.<sup class=""reference"" id=""cite_ref-165""><a href=""#cite_note-165"">[165]</a></sup><sup class=""reference"" id=""cite_ref-166""><a href=""#cite_note-166"">[166]</a></sup> It has been argued that blockchains can foster both cooperation (i.e., prevention of opportunistic behavior) and coordination (i.e., communication and information sharing). Thanks to reliability, transparency, traceability of records, and information immutability, blockchains facilitate collaboration in a way that differs both from the traditional use of contracts and from relational norms. Contrary to contracts, blockchains do not directly rely on the legal system to enforce agreements.<sup class=""reference"" id=""cite_ref-167""><a href=""#cite_note-167"">[167]</a></sup> In addition, contrary to the use of relational norms, blockchains do not require a trust or direct connections between collaborators.
</p>, <p>The need for internal audits to provide effective oversight of organizational efficiency will require a change in the way that  information  is accessed in new formats.<sup class=""reference"" id=""cite_ref-169""><a href=""#cite_note-169"">[169]</a></sup> Blockchain adoption requires a framework to identify the risk of exposure associated with transactions using blockchain. The <a href=""/wiki/Institute_of_Internal_Auditors"" title=""Institute of Internal Auditors"">Institute of Internal Auditors</a> has identified the need for internal auditors to address this transformational technology.  New methods are required to develop audit plans that identify threats and risks. The Internal Audit Foundation study, <i>Blockchain and Internal Audit,</i> assesses these factors.<sup class=""reference"" id=""cite_ref-170""><a href=""#cite_note-170"">[170]</a></sup> The <a href=""/wiki/American_Institute_of_Certified_Public_Accountants"" title=""American Institute of Certified Public Accountants"">American Institute of Certified Public Accountants</a> has outlined new roles for auditors as a result of blockchain.<sup class=""reference"" id=""cite_ref-171""><a href=""#cite_note-171"">[171]</a></sup>
</p>, <p>In September 2015, the first peer-reviewed academic journal dedicated to cryptocurrency and blockchain technology research, <i>Ledger</i>, was announced. The inaugural issue was published in December 2016.<sup class=""reference"" id=""cite_ref-172""><a href=""#cite_note-172"">[172]</a></sup> The journal covers aspects of <a href=""/wiki/Mathematics"" title=""Mathematics"">mathematics</a>, <a href=""/wiki/Computer_science"" title=""Computer science"">computer science</a>, <a href=""/wiki/Engineering"" title=""Engineering"">engineering</a>, <a href=""/wiki/Law"" title=""Law"">law</a>, <a href=""/wiki/Economics"" title=""Economics"">economics</a> and <a href=""/wiki/Philosophy"" title=""Philosophy"">philosophy</a> that relate to cryptocurrencies such as bitcoin.<sup class=""reference"" id=""cite_ref-173""><a href=""#cite_note-173"">[173]</a></sup><sup class=""reference"" id=""cite_ref-174""><a href=""#cite_note-174"">[174]</a></sup>
</p>, <p>The journal encourages authors to <a href=""/wiki/Digital_signature"" title=""Digital signature"">digitally sign</a> a <a href=""/wiki/Hash_function"" title=""Hash function"">file hash</a> of submitted papers, which are then <a href=""/wiki/Trusted_timestamping"" title=""Trusted timestamping"">timestamped</a> into the bitcoin blockchain. Authors are also asked to include a personal bitcoin address on the first page of their papers for non-repudiation purposes.<sup class=""reference"" id=""cite_ref-175""><a href=""#cite_note-175"">[175]</a></sup>
</p>]"
Internet of things,"[<p class=""mw-empty-elt"">
</p>, <p>The <b>Internet of things</b> (<b>IoT</b>) describes physical objects (or groups of such objects)  with <a href=""/wiki/Sensor"" title=""Sensor"">sensors</a>, processing ability, <a href=""/wiki/Software"" title=""Software"">software</a>, and other technologies that connect and exchange data with other devices and systems over the <a href=""/wiki/Internet"" title=""Internet"">Internet</a> or other communications networks.<sup class=""reference"" id=""cite_ref-Linux_Things_1-0""><a href=""#cite_note-Linux_Things-1"">[1]</a></sup><sup class=""reference"" id=""cite_ref-Linux_21OSP_2-0""><a href=""#cite_note-Linux_21OSP-2"">[2]</a></sup><sup class=""reference"" id=""cite_ref-IqTU_3-0""><a href=""#cite_note-IqTU-3"">[3]</a></sup><sup class=""reference"" id=""cite_ref-4""><a href=""#cite_note-4"">[4]</a></sup> Internet of things has been considered a <a href=""/wiki/Misnomer"" title=""Misnomer"">misnomer</a> because devices do not need to be connected to the public internet, they only need to be connected to a network and be individually addressable.<sup class=""reference"" id=""cite_ref-5""><a href=""#cite_note-5"">[5]</a></sup><sup class=""reference"" id=""cite_ref-6""><a href=""#cite_note-6"">[6]</a></sup>
</p>, <p>The field has evolved due to the convergence of multiple <a class=""mw-redirect"" href=""/wiki/Technologies"" title=""Technologies"">technologies</a>, including <a href=""/wiki/Ubiquitous_computing"" title=""Ubiquitous computing"">ubiquitous computing</a>, <a href=""/wiki/Commodity"" title=""Commodity"">commodity</a> <a class=""mw-redirect"" href=""/wiki/Sensors"" title=""Sensors"">sensors</a>, increasingly powerful <a href=""/wiki/Embedded_system"" title=""Embedded system"">embedded systems</a>, and <a href=""/wiki/Machine_learning"" title=""Machine learning"">machine learning</a>.<sup class=""reference"" id=""cite_ref-ast_7-0""><a href=""#cite_note-ast-7"">[7]</a></sup>  Traditional fields of <a href=""/wiki/Embedded_system"" title=""Embedded system"">embedded systems</a>, <a href=""/wiki/Wireless_sensor_network"" title=""Wireless sensor network"">wireless sensor networks</a>, control systems, <a href=""/wiki/Automation"" title=""Automation"">automation</a> (including <a href=""/wiki/Home_automation"" title=""Home automation"">home</a> and <a href=""/wiki/Building_automation"" title=""Building automation"">building automation</a>), independently and collectively enable the Internet of things.<sup class=""reference"" id=""cite_ref-aut_8-0""><a href=""#cite_note-aut-8"">[8]</a></sup>  In the consumer market, IoT technology is most synonymous with products pertaining to the concept of the ""<a class=""mw-redirect"" href=""/wiki/Smart_home_technology"" title=""Smart home technology"">smart home</a>"", including devices and <a href=""/wiki/Home_appliance"" title=""Home appliance"">appliances</a> (such as lighting fixtures, <a class=""mw-redirect"" href=""/wiki/Thermostats"" title=""Thermostats"">thermostats</a>, home <a class=""mw-redirect"" href=""/wiki/Security_systems"" title=""Security systems"">security systems</a>, cameras, and other home appliances) that support one or more common ecosystems, and can be controlled via devices associated with that ecosystem, such as <a href=""/wiki/Smartphone"" title=""Smartphone"">smartphones</a> and <a href=""/wiki/Smart_speaker"" title=""Smart speaker"">smart speakers</a>. IoT is also used in <a href=""/wiki/Health_system"" title=""Health system"">healthcare systems</a>.<sup class=""reference"" id=""cite_ref-9""><a href=""#cite_note-9"">[9]</a></sup>
</p>, <p>There are a number of concerns about the risks in the growth of IoT technologies and products, especially in the areas of <a href=""/wiki/Digital_privacy"" title=""Digital privacy"">privacy</a> and <a href=""/wiki/Digital_security"" title=""Digital security"">security</a>, and consequently, industry and governmental moves to address these concerns have begun, including the development of international and local standards, guidelines, and regulatory frameworks.<sup class=""reference"" id=""cite_ref-10""><a href=""#cite_note-10"">[10]</a></sup>
</p>, <p>The main concept of a network of <a href=""/wiki/Smart_device"" title=""Smart device"">smart devices</a> was discussed as early as 1982, with a modified <a href=""/wiki/Coca-Cola"" title=""Coca-Cola"">Coca-Cola</a> <a href=""/wiki/Vending_machine"" title=""Vending machine"">vending machine</a> at <a href=""/wiki/Carnegie_Mellon_University"" title=""Carnegie Mellon University"">Carnegie Mellon University</a> becoming the first <a href=""/wiki/ARPANET"" title=""ARPANET"">ARPANET</a>-connected appliance,<sup class=""reference"" id=""cite_ref-11""><a href=""#cite_note-11"">[11]</a></sup> able to report its inventory and whether newly loaded drinks were cold or not.<sup class=""reference"" id=""cite_ref-12""><a href=""#cite_note-12"">[12]</a></sup> <a href=""/wiki/Mark_Weiser"" title=""Mark Weiser"">Mark Weiser</a>'s 1991 paper on <a href=""/wiki/Ubiquitous_computing"" title=""Ubiquitous computing"">ubiquitous computing</a>, ""The Computer of the 21st Century"", as well as academic venues such as UbiComp and PerCom produced the contemporary vision of the IOT.<sup class=""reference"" id=""cite_ref-IoT_journal2_13-0""><a href=""#cite_note-IoT_journal2-13"">[13]</a></sup><sup class=""reference"" id=""cite_ref-UbiquitiousComputing_14-0""><a href=""#cite_note-UbiquitiousComputing-14"">[14]</a></sup> In 1994, Reza Raji described the concept in <i><a href=""/wiki/IEEE_Spectrum"" title=""IEEE Spectrum"">IEEE Spectrum</a></i> as ""[moving] small packets of data to a large set of nodes, so as to integrate and automate everything from home appliances to entire factories"".<sup class=""reference"" id=""cite_ref-15""><a href=""#cite_note-15"">[15]</a></sup> Between 1993 and 1997, several companies proposed solutions like <a href=""/wiki/Microsoft"" title=""Microsoft"">Microsoft</a>'s <a class=""mw-redirect"" href=""/wiki/At_Work"" title=""At Work"">at Work</a> or <a href=""/wiki/Novell"" title=""Novell"">Novell</a>'s <a href=""/wiki/Novell_Embedded_Systems_Technology"" title=""Novell Embedded Systems Technology"">NEST</a>. The field gained momentum when <a href=""/wiki/Bill_Joy"" title=""Bill Joy"">Bill Joy</a> envisioned <a href=""/wiki/Device-to-device"" title=""Device-to-device"">device-to-device</a> communication as a part of his ""Six Webs"" framework, presented at the World Economic Forum at Davos in 1999.<sup class=""reference"" id=""cite_ref-ETC:_Bill_Joy's_Six_Webs_16-0""><a href=""#cite_note-ETC:_Bill_Joy's_Six_Webs-16"">[16]</a></sup>
</p>, <p>The concept of the ""Internet of things"" and the term itself, first appeared in a speech by Peter T. Lewis, to the Congressional Black Caucus Foundation 15th Annual Legislative Weekend in Washington, D.C, published in September 1985.<sup class=""reference"" id=""cite_ref-iot:Lewis_17-0""><a href=""#cite_note-iot:Lewis-17"">[17]</a></sup> According to Lewis, ""The Internet of Things, or IoT, is the integration of people, processes and technology with connectable devices and sensors to enable remote monitoring, status, manipulation and evaluation of trends of such devices.""
</p>, <p>The term ""Internet of things"" was coined independently by <a href=""/wiki/Kevin_Ashton"" title=""Kevin Ashton"">Kevin Ashton</a> of <a href=""/wiki/Procter_%26_Gamble"" title=""Procter &amp; Gamble"">Procter &amp; Gamble</a>, later <a href=""/wiki/Massachusetts_Institute_of_Technology"" title=""Massachusetts Institute of Technology"">MIT</a>'s <a href=""/wiki/Auto-ID_Labs"" title=""Auto-ID Labs"">Auto-ID Center</a>, in 1999,<sup class=""reference"" id=""cite_ref-18""><a href=""#cite_note-18"">[18]</a></sup> though he prefers the phrase ""Internet <i>for</i> things"".<sup class=""reference"" id=""cite_ref-19""><a href=""#cite_note-19"">[19]</a></sup> At that point, he viewed <a href=""/wiki/Radio-frequency_identification"" title=""Radio-frequency identification"">radio-frequency identification</a> (RFID) as essential to the Internet of things,<sup class=""reference"" id=""cite_ref-gartner.com_20-0""><a href=""#cite_note-gartner.com-20"">[20]</a></sup> which would allow computers to manage all individual things.<sup class=""reference"" id=""cite_ref-gartner_21-0""><a href=""#cite_note-gartner-21"">[21]</a></sup><sup class=""reference"" id=""cite_ref-EC,_Action_plan_for_Europe_22-0""><a href=""#cite_note-EC,_Action_plan_for_Europe-22"">[22]</a></sup><sup class=""reference"" id=""cite_ref-23""><a href=""#cite_note-23"">[23]</a></sup> The main theme of the Internet of things is to embed short-range mobile transceivers in various gadgets and daily necessities to enable new forms of communication between people and things, and between things themselves.<sup class=""reference"" id=""cite_ref-24""><a href=""#cite_note-24"">[24]</a></sup>
</p>, <p>Defining the Internet of things as ""simply the point in time when more 'things or objects' were connected to the Internet than people"", <a class=""mw-redirect"" href=""/wiki/Cisco_Systems"" title=""Cisco Systems"">Cisco Systems</a> estimated that the IoT was ""born"" between 2008 and 2009, with the things/people ratio growing from 0.08 in 2003 to 1.84 in 2010.<sup class=""reference"" id=""cite_ref-25""><a href=""#cite_note-25"">[25]</a></sup>
</p>, <p>The extensive set of applications for IoT devices<sup class=""reference"" id=""cite_ref-26""><a href=""#cite_note-26"">[26]</a></sup> is often divided into consumer, commercial, industrial, and infrastructure spaces.<sup class=""reference"" id=""cite_ref-Business_Insider_27-0""><a href=""#cite_note-Business_Insider-27"">[27]</a></sup><sup class=""reference"" id=""cite_ref-28""><a href=""#cite_note-28"">[28]</a></sup>
</p>, <p>A growing portion of IoT devices are created for consumer use, including connected vehicles, <a href=""/wiki/Home_automation"" title=""Home automation"">home automation</a>, <a href=""/wiki/Wearable_technology"" title=""Wearable technology"">wearable technology</a>, connected health, and appliances with remote monitoring capabilities.<sup class=""reference"" id=""cite_ref-29""><a href=""#cite_note-29"">[29]</a></sup>
</p>, <p>IoT devices are a part of the larger concept of <a href=""/wiki/Home_automation"" title=""Home automation"">home automation</a>, which can include lighting, heating and air conditioning, media and security systems and camera systems.<sup class=""reference"" id=""cite_ref-30""><a href=""#cite_note-30"">[30]</a></sup><sup class=""reference"" id=""cite_ref-businessinsider.com_31-0""><a href=""#cite_note-businessinsider.com-31"">[31]</a></sup> Long-term benefits could include energy savings by automatically ensuring lights and electronics are turned off or by making the residents in the home aware of usage.<sup class=""reference"" id=""cite_ref-IoTEnergyKettle_32-0""><a href=""#cite_note-IoTEnergyKettle-32"">[32]</a></sup>
</p>, <p>A smart home or automated home could be based on a platform or hubs that control smart devices and appliances.<sup class=""reference"" id=""cite_ref-33""><a href=""#cite_note-33"">[33]</a></sup> For instance, using <a href=""/wiki/Apple_Inc."" title=""Apple Inc."">Apple</a>'s <a href=""/wiki/HomeKit"" title=""HomeKit"">HomeKit</a>, manufacturers can have their home products and accessories controlled by an application in <a href=""/wiki/IOS"" title=""IOS"">iOS</a> devices such as the <a href=""/wiki/IPhone"" title=""IPhone"">iPhone</a> and the <a href=""/wiki/Apple_Watch"" title=""Apple Watch"">Apple Watch</a>.<sup class=""reference"" id=""cite_ref-34""><a href=""#cite_note-34"">[34]</a></sup><sup class=""reference"" id=""cite_ref-35""><a href=""#cite_note-35"">[35]</a></sup> This could be a dedicated app or iOS native applications such as <a href=""/wiki/Siri"" title=""Siri"">Siri</a>.<sup class=""reference"" id=""cite_ref-:3_36-0""><a href=""#cite_note-:3-36"">[36]</a></sup> This can be demonstrated in the case of Lenovo's Smart Home Essentials, which is a line of smart home devices that are controlled through Apple's Home app or Siri without the need for a Wi-Fi bridge.<sup class=""reference"" id=""cite_ref-:3_36-1""><a href=""#cite_note-:3-36"">[36]</a></sup> There are also dedicated smart home hubs that are offered as standalone platforms to connect different smart home products and these include the <a href=""/wiki/Amazon_Echo"" title=""Amazon Echo"">Amazon Echo</a>, <a class=""mw-redirect"" href=""/wiki/Google_Home"" title=""Google Home"">Google Home</a>, Apple's <a href=""/wiki/HomePod"" title=""HomePod"">HomePod</a>, and Samsung's <a href=""/wiki/SmartThings"" title=""SmartThings"">SmartThings Hub</a>.<sup class=""reference"" id=""cite_ref-37""><a href=""#cite_note-37"">[37]</a></sup> In addition to the commercial systems, there are many non-proprietary, open source ecosystems; including Home Assistant, OpenHAB and Domoticz.<sup class=""reference"" id=""cite_ref-38""><a href=""#cite_note-38"">[38]</a></sup><sup class=""reference"" id=""cite_ref-39""><a href=""#cite_note-39"">[39]</a></sup>
</p>, <p>One key application of a smart home is to provide <a href=""/wiki/Home_automation_for_the_elderly_and_disabled"" title=""Home automation for the elderly and disabled"">assistance to elderly individuals and to those with disabilities</a>. These home systems use assistive technology to accommodate an owner's specific disabilities.<sup class=""reference"" id=""cite_ref-auto1_40-0""><a href=""#cite_note-auto1-40"">[40]</a></sup> <a class=""mw-redirect"" href=""/wiki/Voice_Control"" title=""Voice Control"">Voice control</a> can assist users with sight and mobility limitations while alert systems can be connected directly to <a href=""/wiki/Cochlear_implant"" title=""Cochlear implant"">cochlear implants</a> worn by hearing-impaired users.<sup class=""reference"" id=""cite_ref-41""><a href=""#cite_note-41"">[41]</a></sup> They can also be equipped with additional safety features, including sensors that monitor for medical emergencies such as falls or <a href=""/wiki/Seizure"" title=""Seizure"">seizures</a>.<sup class=""reference"" id=""cite_ref-42""><a href=""#cite_note-42"">[42]</a></sup> Smart home technology applied in this way can provide users with more freedom and a higher quality of life.<sup class=""reference"" id=""cite_ref-auto1_40-1""><a href=""#cite_note-auto1-40"">[40]</a></sup>
</p>, <p>The term ""Enterprise IoT"" refers to devices used in business and corporate settings. By 2019, it is estimated that the EIoT will account for 9.1 billion devices.<sup class=""reference"" id=""cite_ref-Business_Insider_27-1""><a href=""#cite_note-Business_Insider-27"">[27]</a></sup>
</p>, <p>The <b>Internet of Medical Things</b> (<b>IoMT</b>) is an application of the IoT for medical and health related purposes, data collection and analysis for research, and monitoring.<sup class=""reference"" id=""cite_ref-Wards_43-0""><a href=""#cite_note-Wards-43"">[43]</a></sup><sup class=""reference"" id=""cite_ref-Geron_44-0""><a href=""#cite_note-Geron-44"">[44]</a></sup><sup class=""reference"" id=""cite_ref-ClinLab_45-0""><a href=""#cite_note-ClinLab-45"">[45]</a></sup><sup class=""reference"" id=""cite_ref-46""><a href=""#cite_note-46"">[46]</a></sup><sup class=""reference"" id=""cite_ref-47""><a href=""#cite_note-47"">[47]</a></sup> The IoMT has been referenced as ""Smart Healthcare"",<sup class=""reference"" id=""cite_ref-auto2_48-0""><a href=""#cite_note-auto2-48"">[48]</a></sup> as the technology for creating a digitized healthcare system, connecting available medical resources and healthcare services.<sup class=""reference"" id=""cite_ref-49""><a href=""#cite_note-49"">[49]</a></sup><sup class=""reference"" id=""cite_ref-50""><a href=""#cite_note-50"">[50]</a></sup>
</p>, <p>IoT devices can be used to enable <a href=""/wiki/Remote_patient_monitoring"" title=""Remote patient monitoring"">remote health monitoring</a> and <a href=""/wiki/Emergency_notification_system"" title=""Emergency notification system"">emergency notification systems</a>. These health monitoring devices can range from blood pressure and heart rate monitors to advanced devices capable of monitoring specialized implants, such as pacemakers, Fitbit electronic wristbands, or advanced hearing aids.<sup class=""reference"" id=""cite_ref-CoMAN_51-0""><a href=""#cite_note-CoMAN-51"">[51]</a></sup> Some hospitals have begun implementing ""smart beds"" that can detect when they are occupied and when a patient is attempting to get up. It can also adjust itself to ensure appropriate pressure and support is applied to the patient without the manual interaction of nurses.<sup class=""reference"" id=""cite_ref-Wards_43-1""><a href=""#cite_note-Wards-43"">[43]</a></sup> A 2015 Goldman Sachs report indicated that healthcare IoT devices ""can save the United States more than $300 billion in annual healthcare expenditures by increasing revenue and decreasing cost.""<sup class=""reference"" id=""cite_ref-Engage16_52-0""><a href=""#cite_note-Engage16-52"">[52]</a></sup> Moreover, the use of mobile devices to support medical follow-up led to the creation of 'm-health', used analyzed health statistics.""<sup class=""reference"" id=""cite_ref-53""><a href=""#cite_note-53"">[53]</a></sup>
</p>, <p>Specialized sensors can also be equipped within living spaces to monitor the health and general well-being of senior citizens, while also ensuring that proper treatment is being administered and assisting people to regain lost mobility via therapy as well.<sup class=""reference"" id=""cite_ref-mHealth_54-0""><a href=""#cite_note-mHealth-54"">[54]</a></sup> These sensors create a network of intelligent sensors that are able to collect, process, transfer, and analyze valuable information in different environments, such as connecting in-home monitoring devices to hospital-based systems.<sup class=""reference"" id=""cite_ref-auto2_48-1""><a href=""#cite_note-auto2-48"">[48]</a></sup> Other consumer devices to encourage healthy living, such as connected scales or <a href=""/wiki/Wearable_technology"" title=""Wearable technology"">wearable heart monitors</a>, are also a possibility with the IoT.<sup class=""reference"" id=""cite_ref-SensorMania_55-0""><a href=""#cite_note-SensorMania-55"">[55]</a></sup> End-to-end health monitoring IoT platforms are also available for antenatal and chronic patients, helping one manage health vitals and recurring medication requirements.<sup class=""reference"" id=""cite_ref-56""><a href=""#cite_note-56"">[56]</a></sup>
</p>, <p>Advances in plastic and fabric electronics fabrication methods have enabled ultra-low cost, use-and-throw IoMT sensors. These sensors, along with the required RFID electronics, can be fabricated on <a href=""/wiki/Paper"" title=""Paper"">paper</a> or <a href=""/wiki/E-textiles"" title=""E-textiles"">e-textiles</a> for wireless powered disposable sensing devices.<sup class=""reference"" id=""cite_ref-57""><a href=""#cite_note-57"">[57]</a></sup> Applications have been established for <a href=""/wiki/Point-of-care_testing"" title=""Point-of-care testing"">point-of-care medical diagnostics</a>, where portability and low system-complexity is essential.<sup class=""reference"" id=""cite_ref-58""><a href=""#cite_note-58"">[58]</a></sup>
</p>, <p>As of 2018<sup class=""plainlinks noexcerpt noprint asof-tag update"" style=""display:none;""><a class=""external text"" href=""https://en.wikipedia.org/w/index.php?title=Internet_of_things&amp;action=edit"">[update]</a></sup> IoMT was not only being applied in the <a class=""mw-redirect"" href=""/wiki/Clinical_laboratory"" title=""Clinical laboratory"">clinical laboratory</a> industry,<sup class=""reference"" id=""cite_ref-ClinLab_45-1""><a href=""#cite_note-ClinLab-45"">[45]</a></sup> but also in the healthcare and health insurance industries. IoMT in the healthcare industry is now permitting doctors, patients, and others, such as guardians of patients, nurses, families, and similar, to be part of a system, where patient records are saved in a database, allowing doctors and the rest of the medical staff to have access to patient information.<sup class=""reference"" id=""cite_ref-59""><a href=""#cite_note-59"">[59]</a></sup> Moreover, IoT-based systems are patient-centered, which involves being flexible to the patient's medical conditions.<sup class=""noprint Inline-Template Template-Fact"" style=""white-space:nowrap;"">[<i><a href=""/wiki/Wikipedia:Citation_needed"" title=""Wikipedia:Citation needed""><span title=""This claim needs references to reliable sources. (November 2019)"">citation needed</span></a></i>]</sup> IoMT in the insurance industry provides access to better and new types of dynamic information. This includes sensor-based solutions such as biosensors, wearables, connected health devices, and mobile apps to track customer behavior. This can lead to more accurate underwriting and new pricing models.<sup class=""reference"" id=""cite_ref-60""><a href=""#cite_note-60"">[60]</a></sup>
</p>, <p>The application of the IoT in healthcare plays a fundamental role in managing chronic diseases and in disease prevention and control. Remote monitoring is made possible through the connection of powerful wireless solutions. The connectivity enables health practitioners to capture patient's data and applying complex algorithms in health data analysis.<sup class=""reference"" id=""cite_ref-61""><a href=""#cite_note-61"">[61]</a></sup>
</p>, <p>The IoT can assist in the integration of communications, control, and information processing across various <a href=""/wiki/Intelligent_transportation_system"" title=""Intelligent transportation system"">transportation systems</a>. Application of the IoT extends to all aspects of transportation systems (i.e. the vehicle,<sup class=""reference"" id=""cite_ref-62""><a href=""#cite_note-62"">[62]</a></sup> the infrastructure, and the driver or user). Dynamic interaction between these components of a transport system enables inter- and intra-vehicular communication,<sup class=""reference"" id=""cite_ref-XieTRB17_63-0""><a href=""#cite_note-XieTRB17-63"">[63]</a></sup> <a href=""/wiki/Smart_traffic_light"" title=""Smart traffic light"">smart traffic control</a>, smart parking, <a href=""/wiki/Electronic_toll_collection"" title=""Electronic toll collection"">electronic toll collection systems</a>, <a class=""mw-redirect"" href=""/wiki/Logistics_management"" title=""Logistics management"">logistics</a> and <a href=""/wiki/Fleet_management"" title=""Fleet management"">fleet management</a>, <a class=""mw-redirect"" href=""/wiki/Autonomous_cruise_control_system"" title=""Autonomous cruise control system"">vehicle control</a>, safety, and road assistance.<sup class=""reference"" id=""cite_ref-CoMAN_51-1""><a href=""#cite_note-CoMAN-51"">[51]</a></sup><sup class=""reference"" id=""cite_ref-SmartIoT2017_64-0""><a href=""#cite_note-SmartIoT2017-64"">[64]</a></sup>
</p>, <p>In <a href=""/wiki/Vehicular_communication_systems"" title=""Vehicular communication systems"">vehicular communication systems</a>, <a href=""/wiki/Vehicle-to-everything"" title=""Vehicle-to-everything"">vehicle-to-everything</a> communication (V2X), consists of three main components: vehicle to vehicle communication (V2V), vehicle to infrastructure communication (V2I) and vehicle to pedestrian communications (V2P). V2X is the first step to <a class=""mw-redirect"" href=""/wiki/Autonomous_car"" title=""Autonomous car"">autonomous driving</a> and connected road infrastructure.<sup class=""noprint Inline-Template Template-Fact"" style=""white-space:nowrap;"">[<i><a href=""/wiki/Wikipedia:Citation_needed"" title=""Wikipedia:Citation needed""><span title=""This claim needs references to reliable sources. (April 2019)"">citation needed</span></a></i>]</sup>
</p>, <p>IoT devices can be used to monitor and control the mechanical, electrical and electronic systems used in various types of buildings (e.g., public and private, industrial, institutions, or residential)<sup class=""reference"" id=""cite_ref-CoMAN_51-2""><a href=""#cite_note-CoMAN-51"">[51]</a></sup> in <a href=""/wiki/Home_automation"" title=""Home automation"">home automation</a> and <a href=""/wiki/Building_automation"" title=""Building automation"">building automation</a> systems. In this context, three main areas are being covered in literature:<sup class=""reference"" id=""cite_ref-:0_65-0""><a href=""#cite_note-:0-65"">[65]</a></sup>
</p>, <p>Also known as IIoT, industrial IoT devices acquire and analyze data from connected equipment, operational technology (OT), locations, and people. Combined with operational technology (OT) monitoring devices, IIoT helps regulate and monitor industrial systems. Also, the same implementation can be carried out for automated record updates of asset placement in industrial storage units as the size of the assets can vary from a small screw to the whole motor spare part, and misplacement of such assets can cause a loss of manpower time and money.
</p>, <p>The IoT can connect various manufacturing devices equipped with sensing, identification, processing, communication, actuation, and networking capabilities.<sup class=""reference"" id=""cite_ref-IoT-Manufacturing_Survey_66-0""><a href=""#cite_note-IoT-Manufacturing_Survey-66"">[66]</a></sup> Network control and management of <a class=""mw-redirect"" href=""/wiki/Reconfigurable_Manufacturing_System"" title=""Reconfigurable Manufacturing System"">manufacturing equipment</a>, <a href=""/wiki/Asset_management"" title=""Asset management"">asset</a> and situation management, or manufacturing <a href=""/wiki/Process_control"" title=""Process control"">process control</a> allow IoT to be used for industrial applications and smart manufacturing.<sup class=""reference"" id=""cite_ref-Butler-M2M_67-0""><a href=""#cite_note-Butler-M2M-67"">[67]</a></sup> IoT intelligent systems enable rapid manufacturing and optimization of new products, and rapid response to product demands.<sup class=""reference"" id=""cite_ref-CoMAN_51-3""><a href=""#cite_note-CoMAN-51"">[51]</a></sup>
</p>, <p><a href=""/wiki/Digital_control"" title=""Digital control"">Digital control systems</a> to automate process controls, operator tools and service information systems to optimize plant safety and security are within the purview of the <a href=""/wiki/Industrial_internet_of_things"" title=""Industrial internet of things"">IIoT</a>.<sup class=""reference"" id=""cite_ref-IoT-Survey_68-0""><a href=""#cite_note-IoT-Survey-68"">[68]</a></sup> IoT can also be applied to asset management via <a href=""/wiki/Predictive_maintenance"" title=""Predictive maintenance"">predictive maintenance</a>, <a href=""/wiki/Statistical_model"" title=""Statistical model"">statistical evaluation</a>, and measurements to maximize reliability.<sup class=""reference"" id=""cite_ref-Future-IoT_69-0""><a href=""#cite_note-Future-IoT-69"">[69]</a></sup> Industrial management systems can be integrated with <a href=""/wiki/Smart_grid"" title=""Smart grid"">smart grids</a>, enabling energy optimization. Measurements, automated controls, plant optimization, health and safety management, and other functions are provided by networked sensors.<sup class=""reference"" id=""cite_ref-CoMAN_51-4""><a href=""#cite_note-CoMAN-51"">[51]</a></sup>
</p>, <p>In addition to general manufacturing, IoT is also used for processes in the industrialization of construction.<sup class=""reference"" id=""cite_ref-70""><a href=""#cite_note-70"">[70]</a></sup>
</p>, <p>There are numerous IoT applications in farming<sup class=""reference"" id=""cite_ref-MeolaWhy16_71-0""><a href=""#cite_note-MeolaWhy16-71"">[71]</a></sup> such as collecting data on temperature, rainfall, humidity, wind speed, pest infestation, and soil content. This data can be used to automate farming techniques, take informed decisions to improve quality and quantity, minimize risk and waste, and reduce the effort required to manage crops. For example, farmers can now monitor soil temperature and moisture from afar, and even apply IoT-acquired data to precision fertilization programs.<sup class=""reference"" id=""cite_ref-ZhangPrecision15_72-0""><a href=""#cite_note-ZhangPrecision15-72"">[72]</a></sup> The overall goal is that data from sensors, coupled with the farmer's knowledge and intuition about his or her farm, can help increase farm productivity, and also help reduce costs.
</p>, <p>In August 2018, <a href=""/wiki/Toyota_Tsusho"" title=""Toyota Tsusho"">Toyota Tsusho</a> began a partnership with <a href=""/wiki/Microsoft"" title=""Microsoft"">Microsoft</a> to create <a href=""/wiki/Fish_farming"" title=""Fish farming"">fish farming</a> tools using the <a href=""/wiki/Microsoft_Azure"" title=""Microsoft Azure"">Microsoft Azure</a> application suite for IoT technologies related to water management. Developed in part by researchers from <a href=""/wiki/Kindai_University"" title=""Kindai University"">Kindai University</a>, the water pump mechanisms use <a href=""/wiki/Artificial_intelligence"" title=""Artificial intelligence"">artificial intelligence</a> to count the number of fish on a <a href=""/wiki/Conveyor_belt"" title=""Conveyor belt"">conveyor belt</a>, analyze the number of fish, and deduce the effectiveness of water flow from the data the fish provide.<sup class=""reference"" id=""cite_ref-73""><a href=""#cite_note-73"">[73]</a></sup> The FarmBeats project<sup class=""reference"" id=""cite_ref-74""><a href=""#cite_note-74"">[74]</a></sup> from Microsoft Research that uses TV white space to connect farms is also a part of the Azure Marketplace now.<sup class=""reference"" id=""cite_ref-75""><a href=""#cite_note-75"">[75]</a></sup>
</p>, <p>IoT devices are in use monitoring the environments and systems of boats and yachts.<sup class=""reference"" id=""cite_ref-76""><a href=""#cite_note-76"">[76]</a></sup> Many pleasure boats are left unattended for days in summer, and months in winter so such devices provide valuable early alerts of boat flooding, fire, and deep discharge of batteries. The use of global internet data networks such as <a href=""/wiki/Sigfox"" title=""Sigfox"">Sigfox</a>, combined with long-life batteries, and microelectronics allows the engine rooms, bilge, and batteries to be constantly monitored and reported to a connected Android &amp; Apple applications for example.
</p>, <p>Monitoring and controlling operations of sustainable urban and rural infrastructures like bridges, railway tracks and on- and offshore wind-farms is a key application of the IoT.<sup class=""reference"" id=""cite_ref-IoT-Survey_68-1""><a href=""#cite_note-IoT-Survey-68"">[68]</a></sup> The IoT infrastructure can be used for monitoring any events or changes in structural conditions that can compromise safety and increase risk. The IoT can benefit the construction industry by cost-saving, time reduction, better quality workday, paperless workflow and increase in productivity. It can help in taking faster decisions and save money with Real-Time Data Analytics. It can also be used for scheduling repair and maintenance activities in an efficient manner, by coordinating tasks between different service providers and users of these facilities.<sup class=""reference"" id=""cite_ref-CoMAN_51-5""><a href=""#cite_note-CoMAN-51"">[51]</a></sup> IoT devices can also be used to control critical infrastructure like bridges to provide access to ships. Usage of IoT devices for monitoring and operating infrastructure is likely to improve incident management and emergency response coordination, and <a href=""/wiki/Quality_of_service"" title=""Quality of service"">quality of service</a>, <a href=""/wiki/Uptime"" title=""Uptime"">up-times</a> and reduce costs of operation in all infrastructure related areas.<sup class=""reference"" id=""cite_ref-IoT-McKinsey_77-0""><a href=""#cite_note-IoT-McKinsey-77"">[77]</a></sup> Even areas such as waste management can benefit<sup class=""reference"" id=""cite_ref-SmartTrash_78-0""><a href=""#cite_note-SmartTrash-78"">[78]</a></sup> from <a href=""/wiki/Automation"" title=""Automation"">automation</a> and optimization that could be brought in by the IoT.<sup class=""noprint Inline-Template Template-Fact"" style=""white-space:nowrap;"">[<i><a href=""/wiki/Wikipedia:Citation_needed"" title=""Wikipedia:Citation needed""><span title=""This claim needs references to reliable sources. (June 2020)"">citation needed</span></a></i>]</sup>
</p>, <p>There are several planned or ongoing large-scale deployments of the IoT, to enable better management of cities and systems. For example, <a href=""/wiki/Songdo_International_Business_District"" title=""Songdo International Business District"">Songdo</a>, South Korea, the first of its kind fully equipped and wired <a href=""/wiki/Smart_city"" title=""Smart city"">smart city</a>, is gradually being built, with approximately 70 percent of the business district completed as of June 2018<sup class=""plainlinks noexcerpt noprint asof-tag update"" style=""display:none;""><a class=""external text"" href=""https://en.wikipedia.org/w/index.php?title=Internet_of_things&amp;action=edit"">[update]</a></sup>. Much of the city is planned to be wired and automated, with little or no human intervention.<sup class=""reference"" id=""cite_ref-PoonSleepy18_79-0""><a href=""#cite_note-PoonSleepy18-79"">[79]</a></sup>
</p>, <p>Another application is currently undergoing a project in <a href=""/wiki/Santander,_Spain"" title=""Santander, Spain"">Santander</a>, Spain. For this deployment, two approaches have been adopted. This city of 180,000 inhabitants has already seen 18,000 downloads of its city smartphone app. The app is connected to 10,000 sensors that enable services like parking search, environmental monitoring, digital city agenda, and more. City context information is used in this deployment so as to benefit merchants through a spark deals mechanism based on city behavior that aims at maximizing the impact of each notification.<sup class=""reference"" id=""cite_ref-80""><a href=""#cite_note-80"">[80]</a></sup>
</p>, <p>Other examples of large-scale deployments underway include the Sino-Singapore Guangzhou Knowledge City;<sup class=""reference"" id=""cite_ref-81""><a href=""#cite_note-81"">[81]</a></sup> work on improving air and water quality, reducing noise pollution, and increasing transportation efficiency in San Jose, California;<sup class=""reference"" id=""cite_ref-82""><a href=""#cite_note-82"">[82]</a></sup> and smart traffic management in western Singapore.<sup class=""reference"" id=""cite_ref-83""><a href=""#cite_note-83"">[83]</a></sup> Using its RPMA (Random Phase Multiple Access) technology, San Diego-based <a href=""/wiki/Ingenu"" title=""Ingenu"">Ingenu</a> has built a nationwide public network<sup class=""reference"" id=""cite_ref-Fortune_84-0""><a href=""#cite_note-Fortune-84"">[84]</a></sup> for low-bandwidth data transmissions using the same unlicensed 2.4 gigahertz spectrum as Wi-Fi. Ingenu's ""Machine Network"" covers more than a third of the US population across 35 major cities including San Diego and Dallas.<sup class=""reference"" id=""cite_ref-The_San_Diego_Union-Tribune_85-0""><a href=""#cite_note-The_San_Diego_Union-Tribune-85"">[85]</a></sup> French company, <a href=""/wiki/Sigfox"" title=""Sigfox"">Sigfox</a>, commenced building an <a href=""/wiki/Ultra_Narrowband"" title=""Ultra Narrowband"">Ultra Narrowband</a> wireless data network in the <a href=""/wiki/San_Francisco_Bay_Area"" title=""San Francisco Bay Area"">San Francisco Bay Area</a> in 2014, the first business to achieve such a deployment in the U.S.<sup class=""reference"" id=""cite_ref-EETimes_86-0""><a href=""#cite_note-EETimes-86"">[86]</a></sup><sup class=""reference"" id=""cite_ref-Fierce_Wireless_Tech_87-0""><a href=""#cite_note-Fierce_Wireless_Tech-87"">[87]</a></sup> It subsequently announced it would set up a total of 4000 <a href=""/wiki/Base_station"" title=""Base station"">base stations</a> to cover a total of 30 cities in the U.S. by the end of 2016, making it the largest IoT network coverage provider in the country thus far.<sup class=""reference"" id=""cite_ref-EETimes2_88-0""><a href=""#cite_note-EETimes2-88"">[88]</a></sup><sup class=""reference"" id=""cite_ref-Gigaom2_89-0""><a href=""#cite_note-Gigaom2-89"">[89]</a></sup> Cisco also participates in smart cities projects. Cisco has started deploying technologies for Smart Wi-Fi, Smart Safety &amp; Security, Smart Lighting, Smart Parking, Smart Transports, Smart Bus Stops, Smart Kiosks, Remote Expert for Government Services (REGS) and Smart Education in the five km area in the city of Vijaywada, India.<sup class=""reference"" id=""cite_ref-90""><a href=""#cite_note-90"">[90]</a></sup>
</p>, <p>Another example of a large deployment is the one completed by New York Waterways in New York City to connect all the city's vessels and be able to monitor them live 24/7. The network was designed and engineered by <a href=""/wiki/Fluidmesh"" title=""Fluidmesh"">Fluidmesh</a> Networks, a Chicago-based company developing wireless networks for critical applications. The NYWW network is currently providing coverage on the Hudson River, East River, and Upper New York Bay. With the wireless network in place, NY Waterway is able to take control of its fleet and passengers in a way that was not previously possible. New applications can include security, energy and fleet management, digital signage, public Wi-Fi, paperless ticketing and others.<sup class=""reference"" id=""cite_ref-91""><a href=""#cite_note-91"">[91]</a></sup>
</p>, <p>Significant numbers of energy-consuming devices (e.g. lamps, household appliances, motors, pumps, etc.) already integrate Internet connectivity, which can allow them to communicate with utilities not only to balance <a href=""/wiki/Electricity_generation"" title=""Electricity generation"">power generation</a> but also helps optimize the energy consumption as a whole.<sup class=""reference"" id=""cite_ref-CoMAN_51-6""><a href=""#cite_note-CoMAN-51"">[51]</a></sup> These devices allow for remote control by users, or central management via a <a href=""/wiki/Cloud_computing"" title=""Cloud computing"">cloud</a>-based interface, and enable functions like scheduling (e.g., remotely powering on or off heating systems, controlling ovens, changing lighting conditions etc.).<sup class=""reference"" id=""cite_ref-CoMAN_51-7""><a href=""#cite_note-CoMAN-51"">[51]</a></sup> The <a href=""/wiki/Smart_grid"" title=""Smart grid"">smart grid</a> is a utility-side IoT application; systems gather and act on energy and power-related information to improve the efficiency of the production and distribution of electricity.<sup class=""reference"" id=""cite_ref-EMAN_92-0""><a href=""#cite_note-EMAN-92"">[92]</a></sup> Using <a href=""/wiki/Smart_meter"" title=""Smart meter"">advanced metering infrastructure (AMI)</a> Internet-connected devices, electric utilities not only collect data from end-users, but also manage distribution automation devices like transformers.<sup class=""reference"" id=""cite_ref-CoMAN_51-8""><a href=""#cite_note-CoMAN-51"">[51]</a></sup>
</p>, <p><a href=""/wiki/Environmental_monitoring"" title=""Environmental monitoring"">Environmental monitoring</a> applications of the IoT typically use sensors to assist in environmental protection<sup class=""reference"" id=""cite_ref-93""><a href=""#cite_note-93"">[93]</a></sup> by monitoring <a class=""mw-redirect"" href=""/wiki/Air_quality"" title=""Air quality"">air</a> or <a href=""/wiki/Water_quality"" title=""Water quality"">water quality</a>,<sup class=""reference"" id=""cite_ref-MolluSCAN_eye_94-0""><a href=""#cite_note-MolluSCAN_eye-94"">[94]</a></sup> <a href=""/wiki/Air_pollution"" title=""Air pollution"">atmospheric</a> or <a class=""mw-redirect"" href=""/wiki/Soil_pollution"" title=""Soil pollution"">soil conditions</a>,<sup class=""reference"" id=""cite_ref-IoT-EnvProt_95-0""><a href=""#cite_note-IoT-EnvProt-95"">[95]</a></sup> and can even include areas like monitoring the <a href=""/wiki/Animal_migration_tracking"" title=""Animal migration tracking"">movements of wildlife</a> and their <a href=""/wiki/Habitat"" title=""Habitat"">habitats</a>.<sup class=""reference"" id=""cite_ref-FIT_96-0""><a href=""#cite_note-FIT-96"">[96]</a></sup> Development of resource-constrained devices connected to the Internet also means that other applications like <a href=""/wiki/Earthquake_warning_system"" title=""Earthquake warning system"">earthquake</a> or <a href=""/wiki/Tsunami_warning_system"" title=""Tsunami warning system"">tsunami early-warning systems</a> can also be used by emergency services to provide more effective aid. IoT devices in this application typically span a large geographic area and can also be mobile.<sup class=""reference"" id=""cite_ref-CoMAN_51-9""><a href=""#cite_note-CoMAN-51"">[51]</a></sup> It has been argued that the standardization that IoT brings to wireless sensing will revolutionize this area.<sup class=""reference"" id=""cite_ref-97""><a href=""#cite_note-97"">[97]</a></sup>
</p>, <p><b>Living Lab</b>
</p>, <p>Another example of integrating the IoT is Living Lab which integrates and combines research and innovation processes, establishing within a public-private-people-partnership.<sup class=""reference"" id=""cite_ref-Scuotto_357–367_98-0""><a href=""#cite_note-Scuotto_357–367-98"">[98]</a></sup> There are currently 320 Living Labs that use the IoT to collaborate and share knowledge between stakeholders to co-create innovative and technological products. For companies to implement and develop <a class=""external text"" href=""https://thinkpalm.com/technologies/internet-of-things/"" rel=""nofollow"">IoT services</a> for smart cities, they need to have incentives. The governments play key roles in smart city projects as changes in policies will help cities to implement the IoT which provides effectiveness, efficiency, and accuracy of the resources that are being used. For instance, the government provides tax incentives and cheap rent, improves public transports, and offers an environment where start-up companies, creative industries, and multinationals may co-create, share a common infrastructure and labor markets, and take advantage of locally embedded technologies, production process, and transaction costs.<sup class=""reference"" id=""cite_ref-Scuotto_357–367_98-1""><a href=""#cite_note-Scuotto_357–367-98"">[98]</a></sup> The relationship between the technology developers and governments who manage the city's assets, is key to provide open access to resources to users in an efficient way.
</p>, <p>The <a href=""/wiki/Internet_of_Military_Things"" title=""Internet of Military Things"">Internet of Military Things (IoMT)</a> is the application of IoT technologies in the military domain for the purposes of reconnaissance, surveillance, and other combat-related objectives. It is heavily influenced by the future prospects of warfare in an urban environment and involves the use of sensors, munitions, vehicles, robots, human-wearable biometrics, and other smart technology that is relevant on the battlefield.<sup class=""reference"" id=""cite_ref-99""><a href=""#cite_note-99"">[99]</a></sup>
</p>, <p>The <b>Internet of Battlefield Things</b> (<b>IoBT</b>) is a project initiated and executed by the <a href=""/wiki/United_States_Army_Research_Laboratory"" title=""United States Army Research Laboratory"">U.S. Army Research Laboratory (ARL)</a> that focuses on the basic science related to the IoT that enhance the capabilities of Army soldiers.<sup class=""reference"" id=""cite_ref-100""><a href=""#cite_note-100"">[100]</a></sup> In 2017, ARL launched the <a href=""/wiki/IoBT-CRA"" title=""IoBT-CRA"">Internet of Battlefield Things Collaborative Research Alliance (IoBT-CRA)</a>, establishing a working collaboration between industry, university, and Army researchers to advance the theoretical foundations of IoT technologies and their applications to Army operations.<sup class=""reference"" id=""cite_ref-101""><a href=""#cite_note-101"">[101]</a></sup><sup class=""reference"" id=""cite_ref-102""><a href=""#cite_note-102"">[102]</a></sup>
</p>, <p>The <b>Ocean of Things</b> project is a <a href=""/wiki/DARPA"" title=""DARPA"">DARPA</a>-led program designed to establish an Internet of things across large ocean areas for the purposes of collecting, monitoring, and analyzing environmental and vessel activity data. The project entails the deployment of about 50,000 floats that house a passive sensor suite that autonomously detect and track military and commercial vessels as part of a cloud-based network.<sup class=""reference"" id=""cite_ref-103""><a href=""#cite_note-103"">[103]</a></sup>
</p>, <p>There are several applications of smart or <a href=""/wiki/Active_packaging"" title=""Active packaging"">active packaging</a> in which a <a href=""/wiki/QR_code"" title=""QR code"">QR code</a> or <a class=""mw-redirect"" href=""/wiki/NFC_tag"" title=""NFC tag"">NFC tag</a> is affixed on a product or its packaging. The tag itself is passive, however, it contains a <a href=""/wiki/Unique_identifier"" title=""Unique identifier"">unique identifier</a> (typically a <a href=""/wiki/URL"" title=""URL"">URL</a>) which enables a user to access digital content about the product via a smartphone.<sup class=""reference"" id=""cite_ref-104""><a href=""#cite_note-104"">[104]</a></sup> Strictly speaking, such passive items are not part of the Internet of things, but they can be seen as enablers of digital interactions.<sup class=""reference"" id=""cite_ref-105""><a href=""#cite_note-105"">[105]</a></sup> The term ""Internet of Packaging"" has been coined to describe applications in which unique identifiers are used, to automate supply chains, and are scanned on large scale by consumers to access digital content.<sup class=""reference"" id=""cite_ref-106""><a href=""#cite_note-106"">[106]</a></sup> Authentication of the unique identifiers, and thereby of the product itself, is possible via a copy-sensitive <a class=""mw-redirect"" href=""/wiki/Digital_watermark"" title=""Digital watermark"">digital watermark</a> or <a href=""/wiki/Copy_detection_pattern"" title=""Copy detection pattern"">copy detection pattern</a> for scanning when scanning a QR code,<sup class=""reference"" id=""cite_ref-107""><a href=""#cite_note-107"">[107]</a></sup> while NFC tags can encrypt communication.<sup class=""reference"" id=""cite_ref-108""><a href=""#cite_note-108"">[108]</a></sup>
</p>, <p>The IoT's major significant trend in recent years is the explosive growth of devices connected and controlled by the Internet.<sup class=""reference"" id=""cite_ref-auto3_109-0""><a href=""#cite_note-auto3-109"">[109]</a></sup> The wide range of applications for IoT technology mean that the specifics can be very different from one device to the next but there are basic characteristics shared by most.
</p>, <p>The IoT creates opportunities for more direct integration of the physical world into computer-based systems, resulting in efficiency improvements, economic benefits, and reduced human exertions.<sup class=""reference"" id=""cite_ref-110""><a href=""#cite_note-110"">[110]</a></sup><sup class=""reference"" id=""cite_ref-111""><a href=""#cite_note-111"">[111]</a></sup><sup class=""reference"" id=""cite_ref-112""><a href=""#cite_note-112"">[112]</a></sup><sup class=""reference"" id=""cite_ref-113""><a href=""#cite_note-113"">[113]</a></sup>
</p>, <p>The number of IoT devices increased 31% year-over-year to 8.4 billion in the year 2017<sup class=""reference"" id=""cite_ref-faz.net_114-0""><a href=""#cite_note-faz.net-114"">[114]</a></sup> and it is estimated that there will be 30 billion devices by 2020.<sup class=""reference"" id=""cite_ref-auto3_109-1""><a href=""#cite_note-auto3-109"">[109]</a></sup> The global market value of the IoT is projected to reach $7.1 trillion by 2020.<sup class=""reference"" id=""cite_ref-115""><a href=""#cite_note-115"">[115]</a></sup>
</p>, <p><a href=""/wiki/Ambient_intelligence"" title=""Ambient intelligence"">Ambient intelligence</a> and autonomous control are not part of the original concept of the Internet of things. Ambient intelligence and autonomous control do not necessarily require Internet structures, either. However, there is a shift in research (by companies such as <a href=""/wiki/Intel"" title=""Intel"">Intel</a>) to integrate the concepts of the IoT and autonomous control, with initial outcomes towards this direction considering objects as the driving force for autonomous IoT.<sup class=""reference"" id=""cite_ref-GDRSmarter18_116-0""><a href=""#cite_note-GDRSmarter18-116"">[116]</a></sup> A promising approach in this context is <a href=""/wiki/Reinforcement_learning"" title=""Reinforcement learning"">deep reinforcement learning</a> where most of IoT systems provide a dynamic and interactive environment.<sup class=""reference"" id=""cite_ref-117""><a href=""#cite_note-117"">[117]</a></sup> Training an agent (i.e., IoT device) to behave smartly in such an environment cannot be addressed by conventional machine learning algorithms such as <a href=""/wiki/Supervised_learning"" title=""Supervised learning"">supervised learning</a>. By reinforcement learning approach, a learning agent can sense the environment's state (e.g., sensing home temperature), perform actions (e.g., turn <a class=""mw-redirect"" href=""/wiki/HVAC"" title=""HVAC"">HVAC</a> on or off) and learn through the maximizing accumulated rewards it receives in long term.
</p>, <p>IoT intelligence can be offered at three levels: IoT devices, <a href=""/wiki/Fog_computing"" title=""Fog computing"">Edge/Fog nodes</a>, and <a href=""/wiki/Cloud_computing"" title=""Cloud computing"">Cloud computing</a>.<sup class=""reference"" id=""cite_ref-deep_learning_iot_118-0""><a href=""#cite_note-deep_learning_iot-118"">[118]</a></sup> The need for intelligent control and decision at each level depends on the time sensitiveness of the IoT application. For example, an autonomous vehicle's camera needs to make real-time <a href=""/wiki/Obstacle_avoidance"" title=""Obstacle avoidance"">obstacle detection</a> to avoid an accident. This fast decision making would not be possible through transferring data from the vehicle to cloud instances and return the predictions back to the vehicle. Instead, all the operation should be performed locally in the vehicle. Integrating advanced machine learning algorithms including <a href=""/wiki/Deep_learning"" title=""Deep learning"">deep learning</a> into IoT devices is an active research area to make smart objects closer to reality. Moreover, it is possible to get the most value out of IoT deployments through analyzing IoT data, extracting hidden information, and predicting control decisions. A wide variety of machine learning techniques have been used in IoT domain ranging from traditional methods such as <a class=""extiw"" href=""https://en.wiktionary.org/wiki/regression"" title=""wikt:regression"">regression</a>, <a class=""mw-redirect"" href=""/wiki/Support_vector_machine"" title=""Support vector machine"">support vector machine</a>, and <a href=""/wiki/Random_forest"" title=""Random forest"">random forest</a> to advanced ones such as <a class=""mw-redirect"" href=""/wiki/Convolutional_neural_networks"" title=""Convolutional neural networks"">convolutional neural networks</a>, <a class=""mw-redirect"" href=""/wiki/LSTM"" title=""LSTM"">LSTM</a>, and <a href=""/wiki/Autoencoder"" title=""Autoencoder"">variational autoencoder</a>.<sup class=""reference"" id=""cite_ref-119""><a href=""#cite_note-119"">[119]</a></sup><sup class=""reference"" id=""cite_ref-deep_learning_iot_118-1""><a href=""#cite_note-deep_learning_iot-118"">[118]</a></sup>
</p>, <p>In the future, the Internet of things may be a non-deterministic and open network in which auto-organized or intelligent entities (<a href=""/wiki/Web_service"" title=""Web service"">web services</a>, <a href=""/wiki/Service-oriented_architecture"" title=""Service-oriented architecture"">SOA</a> components) and virtual objects (avatars) will be interoperable and able to act independently (pursuing their own objectives or shared ones) depending on the context, circumstances or environments. Autonomous behavior through the collection and reasoning of context information as well as the object's ability to detect changes in the environment (faults affecting sensors) and introduce suitable mitigation measures constitutes a major research trend,<sup class=""reference"" id=""cite_ref-Alippi2014_120-0""><a href=""#cite_note-Alippi2014-120"">[120]</a></sup> clearly needed to provide credibility to the IoT technology. Modern IoT products and solutions in the marketplace use a variety of different technologies to support such <a href=""/wiki/Context_awareness"" title=""Context awareness"">context-aware</a> automation, but more sophisticated forms of intelligence are requested to permit sensor units and intelligent cyber-physical systems to be deployed in real environments.<sup class=""reference"" id=""cite_ref-DelicatoSmart18_121-0""><a href=""#cite_note-DelicatoSmart18-121"">[121]</a></sup>
</p>, <p>IoT system architecture, in its simplistic view, consists of three tiers: Tier 1: Devices, Tier 2: the <a href=""/wiki/Edge_computing"" title=""Edge computing"">Edge</a> <a href=""/wiki/Gateway_(telecommunications)#IoT_gateway"" title=""Gateway (telecommunications)"">Gateway</a>, and Tier 3: the Cloud.<sup class=""reference"" id=""cite_ref-:4_122-0""><a href=""#cite_note-:4-122"">[122]</a></sup> Devices include networked things, such as the sensors and actuators found in IoT equipment, particularly those that use protocols such as <a href=""/wiki/Modbus"" title=""Modbus"">Modbus</a>, <a href=""/wiki/Bluetooth"" title=""Bluetooth"">Bluetooth</a>, <a class=""mw-redirect"" href=""/wiki/ZigBee"" title=""ZigBee"">Zigbee</a>, or proprietary protocols, to connect to an Edge Gateway.<sup class=""reference"" id=""cite_ref-:4_122-1""><a href=""#cite_note-:4-122"">[122]</a></sup> The Edge Gateway layer consists of sensor data aggregation systems called Edge Gateways that provide functionality, such as pre-processing of the data, securing connectivity to cloud, using systems such as WebSockets, the event hub, and, even in some cases, edge analytics or <a href=""/wiki/Fog_computing"" title=""Fog computing"">fog computing</a>.<sup class=""reference"" id=""cite_ref-:4_122-2""><a href=""#cite_note-:4-122"">[122]</a></sup> Edge Gateway layer is also required to give a common view of the devices to the upper layers to facilitate in easier management. The final tier includes the cloud application built for IoT using the microservices architecture, which are usually polyglot and inherently secure in nature using HTTPS/<a href=""/wiki/OAuth"" title=""OAuth"">OAuth</a>. It includes various <a href=""/wiki/Database"" title=""Database"">database</a> systems that store sensor data, such as time series databases or asset stores using backend data storage systems (e.g. Cassandra, PostgreSQL).<sup class=""reference"" id=""cite_ref-:4_122-3""><a href=""#cite_note-:4-122"">[122]</a></sup> The cloud tier in most cloud-based IoT system features event queuing and messaging system that handles communication that transpires in all tiers.<sup class=""reference"" id=""cite_ref-123""><a href=""#cite_note-123"">[123]</a></sup> Some experts classified the three-tiers in the IoT system as edge, platform, and enterprise and these are connected by proximity network, access network, and service network, respectively.<sup class=""reference"" id=""cite_ref-124""><a href=""#cite_note-124"">[124]</a></sup>
</p>, <p>Building on the Internet of things, the <a class=""mw-redirect"" href=""/wiki/Web_of_things"" title=""Web of things"">web of things</a> is an architecture for the application layer of the Internet of things looking at the convergence of data from IoT devices into Web applications to create innovative use-cases. In order to program and control the flow of information in the Internet of things, a predicted architectural direction is being called <a href=""/wiki/BPM_Everywhere"" title=""BPM Everywhere"">BPM Everywhere</a> which is a blending of traditional process management with process mining and special capabilities to automate the control of large numbers of coordinated devices.<sup class=""noprint Inline-Template Template-Fact"" style=""white-space:nowrap;"">[<i><a href=""/wiki/Wikipedia:Citation_needed"" title=""Wikipedia:Citation needed""><span title=""This claim needs references to reliable sources. (May 2017)"">citation needed</span></a></i>]</sup>
</p>, <p>The Internet of things requires huge scalability in the network space to handle the surge of devices.<sup class=""reference"" id=""cite_ref-125""><a href=""#cite_note-125"">[125]</a></sup> <a href=""/wiki/6LoWPAN"" title=""6LoWPAN"">IETF 6LoWPAN</a> would be used to connect devices to IP networks. With billions of devices<sup class=""reference"" id=""cite_ref-Gartner_126-0""><a href=""#cite_note-Gartner-126"">[126]</a></sup> being added to the Internet space, <a href=""/wiki/IPv6"" title=""IPv6"">IPv6</a> will play a major role in handling the network layer scalability. <a href=""/wiki/Constrained_Application_Protocol"" title=""Constrained Application Protocol"">IETF's Constrained Application Protocol</a>, <a class=""mw-redirect"" href=""/wiki/%C3%98MQ"" title=""ØMQ"">ZeroMQ</a>, and <a href=""/wiki/MQTT"" title=""MQTT"">MQTT</a> would provide lightweight data transport.
</p>, <p><a href=""/wiki/Fog_computing"" title=""Fog computing"">Fog computing</a> is a viable alternative to prevent such a large burst of data flow through the Internet.<sup class=""reference"" id=""cite_ref-MIST_127-0""><a href=""#cite_note-MIST-127"">[127]</a></sup> The <a href=""/wiki/Edge_device"" title=""Edge device"">edge devices</a>' computation power to analyse and process data is extremely limited. Limited processing power is a key attribute of IoT devices as their purpose is to supply data about physical objects while remaining autonomous. Heavy processing requirements use more battery power harming IoT's ability to operate. Scalability is easy because IoT devices simply supply data through the internet to a server with sufficient processing power.<sup class=""reference"" id=""cite_ref-128""><a href=""#cite_note-128"">[128]</a></sup>
</p>, <p>Decentralized Internet of things, or decentralized IoT, is a modified IoT. It utilizes <a class=""mw-redirect"" href=""/wiki/Fog_networking"" title=""Fog networking"">Fog Computing</a> to handle and balance requests of connected IoT devices in order to reduce loading on the cloud servers, and improve responsiveness for latency-sensitive IoT applications like vital signs monitoring of patients, vehicle-to-vehicle communication of autonomous driving, and critical failure detection of industrial devices.<sup class=""reference"" id=""cite_ref-129""><a href=""#cite_note-129"">[129]</a></sup>
</p>, <p>Conventional IoT is connected via a mesh network and led by a major head node (centralized controller).<sup class=""reference"" id=""cite_ref-130""><a href=""#cite_note-130"">[130]</a></sup> The head node decides how a data is created, stored, and transmitted.<sup class=""reference"" id=""cite_ref-131""><a href=""#cite_note-131"">[131]</a></sup> In contrast, decentralized IoT attempts to divide IoT systems into smaller divisions.<sup class=""reference"" id=""cite_ref-132""><a href=""#cite_note-132"">[132]</a></sup> The head node authorizes partial decision making power to lower level sub-nodes under mutual agreed policy.<sup class=""reference"" id=""cite_ref-133""><a href=""#cite_note-133"">[133]</a></sup> Performance is improved, especially for huge IoT systems with millions of nodes.<sup class=""reference"" id=""cite_ref-134""><a href=""#cite_note-134"">[134]</a></sup>
</p>, <p>Decentralized IoT attempts to address the limited bandwidth and hashing capacity of battery-powered or wireless IoT devices via <a href=""/wiki/Lightweight_blockchain"" title=""Lightweight blockchain"">lightweight blockchain</a>.<sup class=""reference"" id=""cite_ref-135""><a href=""#cite_note-135"">[135]</a></sup><sup class=""reference"" id=""cite_ref-136""><a href=""#cite_note-136"">[136]</a></sup><sup class=""reference"" id=""cite_ref-137""><a href=""#cite_note-137"">[137]</a></sup>
</p>, <p>Cyberattack identification can be done through early detection and mitigation at the edge nodes with traffic monitoring and evaluation.<sup class=""reference"" id=""cite_ref-138""><a href=""#cite_note-138"">[138]</a></sup>
</p>, <p>In semi-open or closed loops (i.e. value chains, whenever a global finality can be settled) the IoT will often be considered and studied as a <a href=""/wiki/Complex_system"" title=""Complex system"">complex system</a><sup class=""reference"" id=""cite_ref-Gautier_2011_139-0""><a href=""#cite_note-Gautier_2011-139"">[139]</a></sup> due to the huge number of different links, interactions between autonomous actors, and its capacity to integrate new actors. At the overall stage (full open loop) it will likely be seen as a <a href=""/wiki/Chaos_theory"" title=""Chaos theory"">chaotic</a> environment (since <a href=""/wiki/System"" title=""System"">systems</a> always have finality).
As a practical approach, not all elements in the Internet of things run in a global, public space. Subsystems are often implemented to mitigate the risks of privacy, control and reliability. For example, domestic robotics (domotics) running inside a smart home might only share data within and be available via a <a href=""/wiki/Local_area_network"" title=""Local area network"">local network</a>.<sup class=""reference"" id=""cite_ref-MargineanSDOMO16_140-0""><a href=""#cite_note-MargineanSDOMO16-140"">[140]</a></sup> Managing and controlling a high dynamic ad hoc IoT things/devices network is a tough task with the traditional networks architecture, Software Defined Networking (SDN) provides the agile dynamic solution that can cope with the special requirements of the diversity of innovative IoT applications.<sup class=""reference"" id=""cite_ref-141""><a href=""#cite_note-141"">[141]</a></sup><sup class=""reference"" id=""cite_ref-142""><a href=""#cite_note-142"">[142]</a></sup>
</p>, <p>The Internet of things would encode 50 to 100 trillion objects, and be able to follow the movement of those objects. Human beings in surveyed urban environments are each surrounded by 1000 to 5000 trackable objects.<sup class=""reference"" id=""cite_ref-Waldner,_2007_143-0""><a href=""#cite_note-Waldner,_2007-143"">[143]</a></sup> In 2015 there were 83 million smart devices in people's homes. This number is expected to grow to 193 million devices by 2020.<sup class=""reference"" id=""cite_ref-businessinsider.com_31-1""><a href=""#cite_note-businessinsider.com-31"">[31]</a></sup><sup class=""reference"" id=""cite_ref-144""><a href=""#cite_note-144"">[144]</a></sup>
</p>, <p>The figure of online capable devices grew 31% from 2016 to 2017 to reach 8.4 billion.<sup class=""reference"" id=""cite_ref-faz.net_114-1""><a href=""#cite_note-faz.net-114"">[114]</a></sup>
</p>, <p>In the Internet of things, the precise geographic location of a thing—and also the precise geographic dimensions of a thing—will be critical.<sup class=""reference"" id=""cite_ref-145""><a href=""#cite_note-145"">[145]</a></sup> Therefore, facts about a thing, such as its location in time and space, have been less critical to track because the person processing the information can decide whether or not that information was important to the action being taken, and if so, add the missing information (or decide to not take the action). (Note that some things in the Internet of things will be sensors, and sensor location is usually important.<sup class=""reference"" id=""cite_ref-146""><a href=""#cite_note-146"">[146]</a></sup>) The <a class=""mw-redirect"" href=""/wiki/GeoWeb"" title=""GeoWeb"">GeoWeb</a> and <a href=""/wiki/Digital_Earth"" title=""Digital Earth"">Digital Earth</a> are promising applications that become possible when things can become organized and connected by location. However, the challenges that remain include the constraints of variable spatial scales, the need to handle massive amounts of data, and an indexing for fast search and neighbour operations. In the Internet of things, if things are able to take actions on their own initiative, this human-centric mediation role is eliminated. Thus, the time-space context that we as humans take for granted must be given a central role in this information ecosystem. Just as standards play a key role in the Internet and the Web, geo-spatial standards will play a key role in the Internet of things.<sup class=""reference"" id=""cite_ref-MinteerAnalytics17_147-0""><a href=""#cite_note-MinteerAnalytics17-147"">[147]</a></sup><sup class=""reference"" id=""cite_ref-vanderZeeSpatial14_148-0""><a href=""#cite_note-vanderZeeSpatial14-148"">[148]</a></sup>
</p>, <p>Many IoT devices have the potential to take a piece of this market. <a href=""/wiki/Jean-Louis_Gass%C3%A9e"" title=""Jean-Louis Gassée"">Jean-Louis Gassée</a> (Apple initial alumni team, and BeOS co-founder) has addressed this topic in an article on <i>Monday Note</i>,<sup class=""reference"" id=""cite_ref-GasséeInternet14_149-0""><a href=""#cite_note-GasséeInternet14-149"">[149]</a></sup> where he predicts that the most likely problem will be what he calls the ""basket of remotes"" problem, where we'll have hundreds of applications to interface with hundreds of devices that don't share protocols for speaking with one another.<sup class=""reference"" id=""cite_ref-GasséeInternet14_149-1""><a href=""#cite_note-GasséeInternet14-149"">[149]</a></sup> For improved user interaction, some technology leaders are joining forces to create standards for communication between devices to solve this problem. Others are turning to the concept of predictive interaction of devices, ""where collected data is used to predict and trigger actions on the specific devices"" while making them work together.<sup class=""reference"" id=""cite_ref-deSousaInternet2015_150-0""><a href=""#cite_note-deSousaInternet2015-150"">[150]</a></sup>
</p>, <p>Social Internet of things (SIoT) is a new kind of IoT that focuses the importance of social interaction and relationship between IoT devices.<sup class=""reference"" id=""cite_ref-151""><a href=""#cite_note-151"">[151]</a></sup> SIoT is a pattern of how cross-domain IoT devices enabling application to application communication and collaboration without human intervention in order to serve their owners with autonomous services,<sup class=""reference"" id=""cite_ref-152""><a href=""#cite_note-152"">[152]</a></sup> and this only can be realized when gained low-level architecture support from both IoT software and hardware engineering.<sup class=""reference"" id=""cite_ref-:02_153-0""><a href=""#cite_note-:02-153"">[153]</a></sup>
</p>, <p>IoT defines a device with an identity like a citizen in a community, and connect them to the internet to provide services to its users.<sup class=""reference"" id=""cite_ref-154""><a href=""#cite_note-154"">[154]</a></sup> SIoT defines a social network for IoT devices only to interact with each other for different goals that to serve human.<sup class=""reference"" id=""cite_ref-155""><a href=""#cite_note-155"">[155]</a></sup>
</p>, <p>SIoT is different from the original IoT in terms of the collaboration characteristics. IoT is passive, it was set to serve for dedicated purposes with existing IoT devices in predetermined system. SIoT is active, it was programmed and managed by AI to serve for unplanned purposes with mix and match of potential IoT devices from different systems that benefit its users.<sup class=""reference"" id=""cite_ref-156""><a href=""#cite_note-156"">[156]</a></sup>
</p>, <p>IoT devices built-in with sociability will broadcast their abilities or functionalities, and at the same time discovers, navigates and groups with other IoT devices in the same or nearby network for useful service compositions in order to help its users proactively in every day's life especially during emergency.<sup class=""reference"" id=""cite_ref-157""><a href=""#cite_note-157"">[157]</a></sup>
</p>, <p>There are many technologies that enable the IoT. Crucial to the field is the network used to communicate between devices of an IoT installation, a role that several wireless or wired technologies may fulfill:<sup class=""reference"" id=""cite_ref-164""><a href=""#cite_note-164"">[164]</a></sup><sup class=""reference"" id=""cite_ref-165""><a href=""#cite_note-165"">[165]</a></sup><sup class=""reference"" id=""cite_ref-166""><a href=""#cite_note-166"">[166]</a></sup>
</p>, <p>The original idea of the <a href=""/wiki/Auto-ID_Labs"" title=""Auto-ID Labs"">Auto-ID Center</a> is based on RFID-tags and distinct identification through the <a href=""/wiki/Electronic_Product_Code"" title=""Electronic Product Code"">Electronic Product Code</a>. This has evolved into objects having an IP address or <a class=""mw-redirect"" href=""/wiki/Uniform_resource_identifier"" title=""Uniform resource identifier"">URI</a>.<sup class=""reference"" id=""cite_ref-HassanInternet18_167-0""><a href=""#cite_note-HassanInternet18-167"">[167]</a></sup> An alternative view, from the world of the <a href=""/wiki/Semantic_Web"" title=""Semantic Web"">Semantic Web</a><sup class=""reference"" id=""cite_ref-Brickley_et_al,_2001_168-0""><a href=""#cite_note-Brickley_et_al,_2001-168"">[168]</a></sup> focuses instead on making all things (not just those electronic, smart, or RFID-enabled) addressable by the existing naming protocols, such as <a class=""mw-redirect"" href=""/wiki/URI"" title=""URI"">URI</a>. The objects themselves do not converse, but they may now be referred to by other agents, such as powerful centralised servers acting for their human owners.<sup class=""reference"" id=""cite_ref-ShengManaging17_169-0""><a href=""#cite_note-ShengManaging17-169"">[169]</a></sup> Integration with the Internet implies that devices will use an <a href=""/wiki/IP_address"" title=""IP address"">IP address</a> as a distinct identifier. Due to the <a href=""/wiki/IPv4_address_exhaustion"" title=""IPv4 address exhaustion"">limited address space</a> of <a href=""/wiki/IPv4"" title=""IPv4"">IPv4</a> (which allows for 4.3 billion different addresses), objects in the IoT will have to use <a href=""/wiki/IPv6"" title=""IPv6"">the next generation</a> of the Internet protocol (<a href=""/wiki/IPv6"" title=""IPv6"">IPv6</a>) to scale to the extremely large address space required.<sup class=""reference"" id=""cite_ref-Waldner,_2008_170-0""><a href=""#cite_note-Waldner,_2008-170"">[170]</a></sup><sup class=""reference"" id=""cite_ref-6LoWPAN_171-0""><a href=""#cite_note-6LoWPAN-171"">[171]</a></sup><sup class=""reference"" id=""cite_ref-computerworld.com_172-0""><a href=""#cite_note-computerworld.com-172"">[172]</a></sup>
Internet-of-things devices additionally will benefit from the stateless address auto-configuration present in IPv6,<sup class=""reference"" id=""cite_ref-IPv6-autoconfiguration_173-0""><a href=""#cite_note-IPv6-autoconfiguration-173"">[173]</a></sup> as it reduces the configuration overhead on the hosts,<sup class=""reference"" id=""cite_ref-6LoWPAN_171-1""><a href=""#cite_note-6LoWPAN-171"">[171]</a></sup> and the <a href=""/wiki/6LoWPAN"" title=""6LoWPAN"">IETF 6LoWPAN</a> header compression. To a large extent, the future of the Internet of things will not be possible without the support of IPv6; and consequently, the global adoption of IPv6 in the coming years will be critical for the successful development of the IoT in the future.<sup class=""reference"" id=""cite_ref-computerworld.com_172-1""><a href=""#cite_note-computerworld.com-172"">[172]</a></sup>
</p>, <p>This is a list of <a href=""/wiki/Technical_standard"" title=""Technical standard"">technical standards</a> for the IoT, most of which are <a href=""/wiki/Open_standard"" title=""Open standard"">open standards</a>, and the <a href=""/wiki/Standards_organization"" title=""Standards organization"">standards organizations</a> that aspire to successfully setting them.<sup class=""reference"" id=""cite_ref-JingResearch12_176-0""><a href=""#cite_note-JingResearch12-176"">[176]</a></sup><sup class=""reference"" id=""cite_ref-MahmoodConnected18_177-0""><a href=""#cite_note-MahmoodConnected18-177"">[177]</a></sup>
</p>, <p>The GS1 digital link standard,<sup class=""reference"" id=""cite_ref-181""><a href=""#cite_note-181"">[181]</a></sup> first released in August 2018, allows the use QR Codes, GS1 Datamatrix, RFID and NFC to enable various types of business-to-business, as well as business-to-consumers interactions.
</p>, <p>Some scholars and activists argue that the IoT can be used to create new models of <a href=""/wiki/Civic_engagement"" title=""Civic engagement"">civic engagement</a> if device networks can be open to user control and inter-operable platforms. <a href=""/wiki/Philip_N._Howard"" title=""Philip N. Howard"">Philip N. Howard</a>, a professor and author, writes that political life in both democracies and authoritarian regimes will be shaped by the way the IoT will be used for civic engagement. For that to happen, he argues that any connected device should be able to divulge a list of the ""ultimate beneficiaries"" of its sensor data and that individual citizens should be able to add new organisations to the beneficiary list. In addition, he argues that civil society groups need to start developing their IoT strategy for making use of data and engaging with the public.<sup class=""reference"" id=""cite_ref-183""><a href=""#cite_note-183"">[183]</a></sup>
</p>, <p>One of the key drivers of the IoT is data. The success of the idea of connecting devices to make them more efficient is dependent upon access to and storage &amp; processing of data. For this purpose, companies working on the IoT collect data from multiple sources and store it in their cloud network for further processing. This leaves the door wide open for privacy and security dangers and single point vulnerability of multiple systems.<sup class=""reference"" id=""cite_ref-184""><a href=""#cite_note-184"">[184]</a></sup> The other issues pertain to consumer choice and ownership of data<sup class=""reference"" id=""cite_ref-185""><a href=""#cite_note-185"">[185]</a></sup> and how it is used. Though still in their infancy, regulations and governance regarding these issues of privacy, security, and data ownership continue to develop.<sup class=""reference"" id=""cite_ref-WeberInternet10_186-0""><a href=""#cite_note-WeberInternet10-186"">[186]</a></sup><sup class=""reference"" id=""cite_ref-HassanInternet18-2_187-0""><a href=""#cite_note-HassanInternet18-2-187"">[187]</a></sup><sup class=""reference"" id=""cite_ref-HassanInternet17_188-0""><a href=""#cite_note-HassanInternet17-188"">[188]</a></sup> IoT regulation depends on the country. Some examples of legislation that is relevant to privacy and data collection are: the US Privacy Act of 1974, OECD Guidelines on the Protection of Privacy and Transborder Flows of Personal Data of 1980, and the EU Directive 95/46/EC of 1995.<sup class=""reference"" id=""cite_ref-189""><a href=""#cite_note-189"">[189]</a></sup>
</p>, <p>Current regulatory environment:
</p>, <p>A report published by the <a href=""/wiki/Federal_Trade_Commission"" title=""Federal Trade Commission"">Federal Trade Commission</a> (FTC) in January 2015 made the following three recommendations:<sup class=""reference"" id=""cite_ref-190""><a href=""#cite_note-190"">[190]</a></sup>
</p>, <p>However, the FTC stopped at just making recommendations for now. According to an FTC analysis, the existing framework, consisting of the <a class=""mw-redirect"" href=""/wiki/FTC_Act"" title=""FTC Act"">FTC Act</a>, the <a href=""/wiki/Fair_Credit_Reporting_Act"" title=""Fair Credit Reporting Act"">Fair Credit Reporting Act</a>, and the <a href=""/wiki/Children%27s_Online_Privacy_Protection_Act"" title=""Children's Online Privacy Protection Act"">Children's Online Privacy Protection Act</a>, along with developing consumer education and business guidance, participation in multi-stakeholder efforts and advocacy to other agencies at the federal, state and local level, is sufficient to protect consumer rights.<sup class=""reference"" id=""cite_ref-192""><a href=""#cite_note-192"">[192]</a></sup>
</p>, <p>A resolution passed by the Senate in March 2015, is already being considered by the Congress.<sup class=""reference"" id=""cite_ref-193""><a href=""#cite_note-193"">[193]</a></sup> This resolution recognized the need for formulating a National Policy on IoT and the matter of privacy, security and spectrum. Furthermore, to provide an impetus to the IoT ecosystem, in March 2016, a bipartisan group of four Senators proposed a bill, The Developing Innovation and Growing the Internet of Things (DIGIT) Act, to direct the <a href=""/wiki/Federal_Communications_Commission"" title=""Federal Communications Commission"">Federal Communications Commission</a> to assess the need for more spectrum to connect IoT devices.
</p>, <p>Approved on 28 September 2018, California Senate Bill No. 327<sup class=""reference"" id=""cite_ref-194""><a href=""#cite_note-194"">[194]</a></sup> goes into effect on 1 January 2020. The bill requires ""<i>a manufacturer of a connected device, as those terms are defined, to equip the device with a reasonable security feature or features that are appropriate to the nature and function of the device, appropriate to the information it may collect, contain, or transmit, and designed to protect the device and any information contained therein from unauthorized access, destruction, use, modification, or disclosure,</i>""
</p>, <p>Several standards for the IoT industry are actually being established relating to automobiles because most concerns arising from use of connected cars apply to healthcare devices as well. In fact, the <a href=""/wiki/National_Highway_Traffic_Safety_Administration"" title=""National Highway Traffic Safety Administration"">National Highway Traffic Safety Administration</a> (NHTSA) is preparing cybersecurity guidelines and a database of best practices to make automotive computer systems more secure.<sup class=""reference"" id=""cite_ref-195""><a href=""#cite_note-195"">[195]</a></sup>
</p>, <p>A recent report from the World Bank examines the challenges and opportunities in government adoption of IoT.<sup class=""reference"" id=""cite_ref-196""><a href=""#cite_note-196"">[196]</a></sup> These include –
</p>, <p>In early December 2021, the U.K. government introduced the <a href=""/wiki/2021_State_Opening_of_Parliament#Announced_bills"" title=""2021 State Opening of Parliament"">Product Security and Telecommunications Infrastructure bill</a> (PST), an effort to legislate IoT distributors, manufacturers, and importers to meet certain <a href=""/wiki/IT_security_standards"" title=""IT security standards"">cybersecurity standards</a>. The bill also seeks to improve the security credentials of consumer IoT devices.<sup class=""reference"" id=""cite_ref-“page”_197-0""><a href=""#cite_note-“page”-197"">[197]</a></sup>
</p>, <p>The IoT suffers from <a class=""mw-redirect"" href=""/wiki/Platform_fragmentation"" title=""Platform fragmentation"">platform fragmentation</a>, lack of interoperability and common <a href=""/wiki/Technical_standard"" title=""Technical standard"">technical standards</a><sup class=""reference"" id=""cite_ref-198""><a href=""#cite_note-198"">[198]</a></sup><sup class=""reference"" id=""cite_ref-199""><a href=""#cite_note-199"">[199]</a></sup><sup class=""reference"" id=""cite_ref-200""><a href=""#cite_note-200"">[200]</a></sup><sup class=""reference"" id=""cite_ref-201""><a href=""#cite_note-201"">[201]</a></sup><sup class=""reference"" id=""cite_ref-202""><a href=""#cite_note-202"">[202]</a></sup><sup class=""reference"" id=""cite_ref-203""><a href=""#cite_note-203"">[203]</a></sup><sup class=""reference"" id=""cite_ref-204""><a href=""#cite_note-204"">[204]</a></sup><sup class=""noprint Inline-Template Template-Fact"" style=""white-space:nowrap;"">[<i><a href=""/wiki/Wikipedia:Citing_sources#Bundling_citations"" title=""Wikipedia:Citing sources""><span title=""This claim has too many footnotes for reading to be smooth. (April 2019)"">excessive citations</span></a></i>]</sup> a situation where the variety of IoT devices, in terms of both hardware variations and differences in the software running on them, makes the task of developing applications that work consistently between different inconsistent technology <a href=""/wiki/Ecosystem"" title=""Ecosystem"">ecosystems</a> hard.<sup class=""reference"" id=""cite_ref-Linux_Things_1-1""><a href=""#cite_note-Linux_Things-1"">[1]</a></sup> For example, wireless connectivity for IoT devices can be done using <a href=""/wiki/Bluetooth"" title=""Bluetooth"">Bluetooth</a>, <a href=""/wiki/Zigbee"" title=""Zigbee"">Zigbee</a>, <a href=""/wiki/Z-Wave"" title=""Z-Wave"">Z-Wave</a>, <a href=""/wiki/LoRa"" title=""LoRa"">LoRa</a>, <a href=""/wiki/Narrowband_IoT"" title=""Narrowband IoT"">NB-IoT</a>, <a class=""mw-redirect"" href=""/wiki/Cat-M1"" title=""Cat-M1"">Cat M1</a> as well as completely custom proprietary radios – each with its own advantages and disadvantages; and unique support ecosystem.<sup class=""reference"" id=""cite_ref-205""><a href=""#cite_note-205"">[205]</a></sup>
</p>, <p>The IoT's <a href=""/wiki/Amorphous_computing"" title=""Amorphous computing"">amorphous computing</a> nature is also a problem for security, since patches to bugs found in the core operating system often do not reach users of older and lower-price devices.<sup class=""reference"" id=""cite_ref-206""><a href=""#cite_note-206"">[206]</a></sup><sup class=""reference"" id=""cite_ref-Goodbye,_Android_207-0""><a href=""#cite_note-Goodbye,_Android-207"">[207]</a></sup><sup class=""reference"" id=""cite_ref-208""><a href=""#cite_note-208"">[208]</a></sup> One set of researchers say that the failure of vendors to support older devices with patches and updates leaves more than 87% of active Android devices vulnerable.<sup class=""reference"" id=""cite_ref-209""><a href=""#cite_note-209"">[209]</a></sup><sup class=""reference"" id=""cite_ref-210""><a href=""#cite_note-210"">[210]</a></sup>
</p>, <p><a href=""/wiki/Philip_N._Howard"" title=""Philip N. Howard"">Philip N. Howard</a>, a professor and author, writes that the Internet of things offers immense potential for empowering citizens, making government transparent, and broadening <a href=""/wiki/Information_access"" title=""Information access"">information access</a>. Howard cautions, however, that privacy threats are enormous, as is the potential for social control and political manipulation.<sup class=""reference"" id=""cite_ref-211""><a href=""#cite_note-211"">[211]</a></sup>
</p>, <p>Concerns about privacy have led many to consider the possibility that <a href=""/wiki/Big_data"" title=""Big data"">big data</a> infrastructures such as the Internet of things and <a href=""/wiki/Data_mining"" title=""Data mining"">data mining</a> are inherently incompatible with privacy.<sup class=""reference"" id=""cite_ref-212""><a href=""#cite_note-212"">[212]</a></sup> Key challenges of increased digitalization in the water, transport or energy sector are related to privacy and <a class=""mw-redirect"" href=""/wiki/Cybersecurity"" title=""Cybersecurity"">cybersecurity</a> which necessitate an adequate response from research and policymakers alike.<sup class=""reference"" id=""cite_ref-213""><a href=""#cite_note-213"">[213]</a></sup>
</p>, <p>Writer <a href=""/wiki/Adam_Greenfield"" title=""Adam Greenfield"">Adam Greenfield</a> claims that IoT technologies are not only an invasion of public space but are also being used to perpetuate normative behavior, citing an instance of billboards with hidden cameras that tracked the demographics of passersby who stopped to read the advertisement.
</p>, <p>The Internet of Things Council compared the increased prevalence of <a href=""/wiki/Surveillance"" title=""Surveillance"">digital surveillance</a> due to the Internet of things to the conceptual <a href=""/wiki/Panopticon"" title=""Panopticon"">panopticon</a> described by <a href=""/wiki/Jeremy_Bentham"" title=""Jeremy Bentham"">Jeremy Bentham</a> in the 18th Century.<sup class=""reference"" id=""cite_ref-214""><a href=""#cite_note-214"">[214]</a></sup> The assertion was defended by the works of French philosophers <a href=""/wiki/Michel_Foucault"" title=""Michel Foucault"">Michel Foucault</a> and <a href=""/wiki/Gilles_Deleuze"" title=""Gilles Deleuze"">Gilles Deleuze</a>. In <a href=""/wiki/Discipline_and_Punish"" title=""Discipline and Punish""><i>Discipline and Punish: The Birth of the Prison</i></a> Foucault asserts that the panopticon was a central element of the discipline society developed during the <a class=""mw-redirect"" href=""/wiki/Industrial_Era"" title=""Industrial Era"">Industrial Era</a>.<sup class=""reference"" id=""cite_ref-ccle.ucla.edu_215-0""><a href=""#cite_note-ccle.ucla.edu-215"">[215]</a></sup> Foucault also argued that the discipline systems established in factories and school reflected Bentham's vision of <a class=""mw-redirect"" href=""/wiki/Panopticism"" title=""Panopticism"">panopticism</a>.<sup class=""reference"" id=""cite_ref-ccle.ucla.edu_215-1""><a href=""#cite_note-ccle.ucla.edu-215"">[215]</a></sup> In his 1992 paper ""Postscripts on the Societies of Control,"" Deleuze wrote that the discipline society had transitioned into a control society, with the <a href=""/wiki/Computer"" title=""Computer"">computer</a> replacing the <a href=""/wiki/Panopticon"" title=""Panopticon"">panopticon</a> as an instrument of discipline and control while still maintaining the qualities similar to that of panopticism.<sup class=""reference"" id=""cite_ref-216""><a href=""#cite_note-216"">[216]</a></sup>
</p>, <p><a href=""/wiki/Peter-Paul_Verbeek"" title=""Peter-Paul Verbeek"">Peter-Paul Verbeek</a>, a professor of philosophy of technology at the <a href=""/wiki/University_of_Twente"" title=""University of Twente"">University of Twente</a>, Netherlands, writes that technology already influences our moral decision making, which in turn affects human agency, privacy and autonomy. He cautions against viewing technology merely as a human tool and advocates instead to consider it as an active agent.<sup class=""reference"" id=""cite_ref-217""><a href=""#cite_note-217"">[217]</a></sup>
</p>, <p>Justin Brookman, of the <a href=""/wiki/Center_for_Democracy_and_Technology"" title=""Center for Democracy and Technology"">Center for Democracy and Technology</a>, expressed concern regarding the impact of the IoT on <a href=""/wiki/Consumer_privacy"" title=""Consumer privacy"">consumer privacy</a>, saying that ""There are some people in the commercial space who say, 'Oh, <a href=""/wiki/Big_data"" title=""Big data"">big data</a> – well, let's collect everything, keep it around forever, we'll pay for somebody to think about security later.' The question is whether we want to have some sort of policy framework in place to limit that.""<sup class=""reference"" id=""cite_ref-218""><a href=""#cite_note-218"">[218]</a></sup>
</p>, <p><a href=""/wiki/Tim_O%27Reilly"" title=""Tim O'Reilly"">Tim O'Reilly</a> believes that the way companies sell the IoT devices on consumers are misplaced, disputing the notion that the IoT is about gaining efficiency from putting all kinds of devices online and postulating that the ""IoT is really about human augmentation. The applications are profoundly different when you have sensors and data driving the decision-making.""<sup class=""reference"" id=""cite_ref-219""><a href=""#cite_note-219"">[219]</a></sup>
</p>, <p>Editorials at <a class=""mw-redirect"" href=""/wiki/WIRED"" title=""WIRED"">WIRED</a> have also expressed concern, one stating ""What you're about to lose is your privacy. Actually, it's worse than that. You aren't just going to lose your privacy, you're going to have to watch the very concept of privacy be rewritten under your nose.""<sup class=""reference"" id=""cite_ref-220""><a href=""#cite_note-220"">[220]</a></sup>
</p>, <p>The <a href=""/wiki/American_Civil_Liberties_Union"" title=""American Civil Liberties Union"">American Civil Liberties Union</a> (ACLU) expressed concern regarding the ability of IoT to erode people's control over their own lives. The ACLU wrote that ""There's simply no way to forecast how these immense powers – disproportionately accumulating in the hands of corporations seeking financial advantage and governments craving ever more control – will be used. Chances are <a href=""/wiki/Big_data"" title=""Big data"">big data</a> and the Internet of Things will make it harder for us to control our own lives, as we grow increasingly transparent to powerful corporations and government institutions that are becoming more opaque to us.""<sup class=""reference"" id=""cite_ref-221""><a href=""#cite_note-221"">[221]</a></sup>
</p>, <p>In response to rising concerns about privacy and <a href=""/wiki/Smart_Technologies"" title=""Smart Technologies"">smart technology</a>, in 2007 the <a class=""mw-redirect"" href=""/wiki/British_Government"" title=""British Government"">British Government</a> stated it would follow formal <a class=""mw-redirect"" href=""/wiki/Privacy_by_Design"" title=""Privacy by Design"">Privacy by Design</a> principles when implementing their smart metering program. The program would lead to replacement of traditional <a class=""mw-redirect"" href=""/wiki/Electricity_Meter"" title=""Electricity Meter"">power meters</a> with smart power meters, which could track and manage energy usage more accurately.<sup class=""reference"" id=""cite_ref-222""><a href=""#cite_note-222"">[222]</a></sup> However the <a class=""mw-redirect"" href=""/wiki/British_Computer_Society"" title=""British Computer Society"">British Computer Society</a> is doubtful these principles were ever actually implemented.<sup class=""reference"" id=""cite_ref-bcs.org_223-0""><a href=""#cite_note-bcs.org-223"">[223]</a></sup> In 2009 the <a class=""mw-redirect"" href=""/wiki/Dutch_Parliament"" title=""Dutch Parliament"">Dutch Parliament</a> rejected a similar smart metering program, basing their decision on privacy concerns. The Dutch program later revised and passed in 2011.<sup class=""reference"" id=""cite_ref-bcs.org_223-1""><a href=""#cite_note-bcs.org-223"">[223]</a></sup>
</p>, <p>A challenge for producers of IoT applications is to <a href=""/wiki/Data_cleansing"" title=""Data cleansing"">clean</a>, process and interpret the vast amount of data which is gathered by the sensors. There is a solution proposed for the analytics of the information referred to as Wireless Sensor Networks.<sup class=""reference"" id=""cite_ref-:1_224-0""><a href=""#cite_note-:1-224"">[224]</a></sup> These networks share data among sensor nodes that are sent to a distributed system for the analytics of the sensory data.<sup class=""reference"" id=""cite_ref-AcharjyaRecognizing17_225-0""><a href=""#cite_note-AcharjyaRecognizing17-225"">[225]</a></sup>
</p>, <p>Another challenge is the storage of this bulk data. Depending on the application, there could be high data acquisition requirements, which in turn lead to high storage requirements. Currently the Internet is already responsible for 5% of the total energy generated,<sup class=""reference"" id=""cite_ref-:1_224-1""><a href=""#cite_note-:1-224"">[224]</a></sup> and a ""daunting challenge to power"" IoT devices to collect and even store data still remains.<sup class=""reference"" id=""cite_ref-HussainEnergy17_226-0""><a href=""#cite_note-HussainEnergy17-226"">[226]</a></sup>
</p>, <p>Security is the biggest concern in adopting Internet of things technology,<sup class=""reference"" id=""cite_ref-227""><a href=""#cite_note-227"">[227]</a></sup> with concerns that rapid development is happening without appropriate consideration of the profound security challenges involved<sup class=""reference"" id=""cite_ref-228""><a href=""#cite_note-228"">[228]</a></sup> and the regulatory changes that might be necessary.<sup class=""reference"" id=""cite_ref-forbes_229-0""><a href=""#cite_note-forbes-229"">[229]</a></sup><sup class=""reference"" id=""cite_ref-tinker_230-0""><a href=""#cite_note-tinker-230"">[230]</a></sup>
</p>, <p>Most of the technical security concerns are similar to those of conventional servers, workstations and smartphones.<sup class=""reference"" id=""cite_ref-LiSecuring17_231-0""><a href=""#cite_note-LiSecuring17-231"">[231]</a></sup> These concerns include using weak authentication, forgetting to change default credentials, unencrypted messages sent between devices, <a href=""/wiki/SQL_injection"" title=""SQL injection"">SQL injections</a>, <a href=""/wiki/Man-in-the-middle_attack"" title=""Man-in-the-middle attack"">Man-in-the-middle attacks</a>, and poor handling of security updates.<sup class=""reference"" id=""cite_ref-:5_232-0""><a href=""#cite_note-:5-232"">[232]</a></sup><sup class=""reference"" id=""cite_ref-233""><a href=""#cite_note-233"">[233]</a></sup> However, many IoT devices have severe operational limitations on the computational power available to them. These constraints often make them unable to directly use basic security measures such as implementing firewalls or using strong cryptosystems to encrypt their communications with other devices<sup class=""reference"" id=""cite_ref-234""><a href=""#cite_note-234"">[234]</a></sup> - and the low price and consumer focus of many devices makes a robust security patching system uncommon.<sup class=""reference"" id=""cite_ref-235""><a href=""#cite_note-235"">[235]</a></sup>
</p>, <p>Rather than conventional security vulnerabilities, fault injection attacks are on the rise and targeting IoT devices. A fault injection attack is a physical attack on a device to purposefully introduce faults in the system to change the intended behavior. Faults might happen unintentionally by environmental noises and electromagnetic fields. There are ideas stemmed from control-flow integrity (CFI) to prevent fault injection attacks and system recovery to a healthy state before the fault.<sup class=""reference"" id=""cite_ref-236""><a href=""#cite_note-236"">[236]</a></sup>
</p>, <p>Internet of things devices also have access to new areas of data, and can often control physical devices,<sup class=""reference"" id=""cite_ref-237""><a href=""#cite_note-237"">[237]</a></sup> so that even by 2014 it was possible to say that many Internet-connected appliances could already ""spy on people in their own homes"" including televisions, kitchen appliances,<sup class=""reference"" id=""cite_ref-238""><a href=""#cite_note-238"">[238]</a></sup> cameras, and thermostats.<sup class=""reference"" id=""cite_ref-239""><a href=""#cite_note-239"">[239]</a></sup> Computer-controlled devices in automobiles such as brakes, engine, locks, hood and trunk releases, horn, heat, and dashboard have been shown to be vulnerable to attackers who have access to the on-board network. In some cases, vehicle computer systems are Internet-connected, allowing them to be exploited remotely.<sup class=""reference"" id=""cite_ref-240""><a href=""#cite_note-240"">[240]</a></sup> By 2008 security researchers had shown the ability to remotely control pacemakers without authority. Later hackers demonstrated remote control of insulin pumps<sup class=""reference"" id=""cite_ref-241""><a href=""#cite_note-241"">[241]</a></sup> and implantable cardioverter defibrillators.<sup class=""reference"" id=""cite_ref-242""><a href=""#cite_note-242"">[242]</a></sup>
</p>, <p>Poorly secured Internet-accessible IoT devices can also be subverted to attack others. In 2016, a <a class=""mw-redirect"" href=""/wiki/Distributed_denial_of_service_attack"" title=""Distributed denial of service attack"">distributed denial of service attack</a> powered by Internet of things devices running the <a href=""/wiki/Mirai_(malware)"" title=""Mirai (malware)"">Mirai</a> malware <a class=""mw-redirect"" href=""/wiki/2016_Dyn_cyberattack"" title=""2016 Dyn cyberattack"">took down a DNS provider and major web sites</a>.<sup class=""reference"" id=""cite_ref-243""><a href=""#cite_note-243"">[243]</a></sup> The <a href=""/wiki/Mirai_(malware)"" title=""Mirai (malware)"">Mirai Botnet</a> had infected roughly 65,000 IoT devices within the first 20 hours.<sup class=""reference"" id=""cite_ref-:2_244-0""><a href=""#cite_note-:2-244"">[244]</a></sup> Eventually the infections increased to around 200,000 to 300,000 infections.<sup class=""reference"" id=""cite_ref-:2_244-1""><a href=""#cite_note-:2-244"">[244]</a></sup> Brazil, Colombia and Vietnam made up of 41.5% of the infections.<sup class=""reference"" id=""cite_ref-:2_244-2""><a href=""#cite_note-:2-244"">[244]</a></sup> The Mirai Botnet had singled out specific IoT devices that consisted of DVRs, IP cameras, routers and printers.<sup class=""reference"" id=""cite_ref-:2_244-3""><a href=""#cite_note-:2-244"">[244]</a></sup> Top vendors that contained the most infected devices were identified as Dahua, Huawei, ZTE, Cisco, ZyXEL and MikroTik<b>.</b><sup class=""reference"" id=""cite_ref-:2_244-4""><a href=""#cite_note-:2-244"">[244]</a></sup> In May 2017, <a href=""/wiki/Junade_Ali"" title=""Junade Ali"">Junade Ali</a>, a Computer Scientist at <a href=""/wiki/Cloudflare"" title=""Cloudflare"">Cloudflare</a> noted that native DDoS vulnerabilities exist in IoT devices due to a poor implementation of the <a href=""/wiki/Publish%E2%80%93subscribe_pattern"" title=""Publish–subscribe pattern"">Publish–subscribe pattern</a>.<sup class=""reference"" id=""cite_ref-245""><a href=""#cite_note-245"">[245]</a></sup><sup class=""reference"" id=""cite_ref-246""><a href=""#cite_note-246"">[246]</a></sup> These sorts of attacks have caused security experts to view IoT as a real threat to Internet services.<sup class=""reference"" id=""cite_ref-247""><a href=""#cite_note-247"">[247]</a></sup>
</p>, <p>The U.S. <a href=""/wiki/National_Intelligence_Council"" title=""National Intelligence Council"">National Intelligence Council</a> in an unclassified report maintains that it would be hard to deny ""access to networks of sensors and remotely-controlled objects by enemies of the United States, criminals, and mischief makers... An open market for aggregated sensor data could serve the interests of commerce and security no less than it helps criminals and spies identify vulnerable targets. Thus, massively parallel <a href=""/wiki/Sensor_fusion"" title=""Sensor fusion"">sensor fusion</a> may undermine social cohesion, if it proves to be fundamentally incompatible with Fourth-Amendment guarantees against unreasonable search.""<sup class=""reference"" id=""cite_ref-248""><a href=""#cite_note-248"">[248]</a></sup> In general, the intelligence community views the Internet of things as a rich source of data.<sup class=""reference"" id=""cite_ref-249""><a href=""#cite_note-249"">[249]</a></sup>
</p>, <p>On 31 January 2019, the Washington Post wrote an article regarding the security and ethical challenges that can occur with IoT doorbells and cameras: ""Last month, Ring got caught allowing its team in Ukraine to view and annotate certain user videos; the company says it only looks at publicly shared videos and those from Ring owners who provide consent. Just last week, a California family's Nest camera let a hacker take over and broadcast fake audio warnings about a missile attack, not to mention peer in on them, when they used a weak password""<sup class=""reference"" id=""cite_ref-250""><a href=""#cite_note-250"">[250]</a></sup>
</p>, <p>There have been a range of responses to concerns over security. The Internet of Things Security Foundation (IoTSF) was launched on 23 September 2015 with a mission to secure the Internet of things by promoting knowledge and best practice. Its founding board is made from technology providers and telecommunications companies. In addition, large IT companies are continually developing innovative solutions to ensure the security of IoT devices. In 2017, Mozilla launched <a class=""new"" href=""/w/index.php?title=Project_Things&amp;action=edit&amp;redlink=1"" title=""Project Things (page does not exist)"">Project Things</a>, which allows to route IoT devices through a safe Web of Things gateway.<sup class=""reference"" id=""cite_ref-251""><a href=""#cite_note-251"">[251]</a></sup> As per the estimates from KBV Research,<sup class=""reference"" id=""cite_ref-252""><a href=""#cite_note-252"">[252]</a></sup> the overall IoT security market<sup class=""reference"" id=""cite_ref-253""><a href=""#cite_note-253"">[253]</a></sup> would grow at 27.9% rate during 2016–2022 as a result of growing infrastructural concerns and diversified usage of Internet of things.<sup class=""reference"" id=""cite_ref-254""><a href=""#cite_note-254"">[254]</a></sup><sup class=""reference"" id=""cite_ref-255""><a href=""#cite_note-255"">[255]</a></sup>
</p>, <p>Governmental regulation is argued by some to be necessary to secure IoT devices and the wider Internet – as market incentives to secure IoT devices is insufficient.<sup class=""reference"" id=""cite_ref-256""><a href=""#cite_note-256"">[256]</a></sup><sup class=""reference"" id=""cite_ref-forbes_229-1""><a href=""#cite_note-forbes-229"">[229]</a></sup><sup class=""reference"" id=""cite_ref-tinker_230-1""><a href=""#cite_note-tinker-230"">[230]</a></sup> It was found that due to the nature of most of the IoT development boards, they generate predictable and weak keys which make it easy to be utilized by <a href=""/wiki/Man-in-the-middle_attack"" title=""Man-in-the-middle attack"">Man-in-the-middle</a> attack. However, various hardening approaches were proposed by many researchers to resolve the issue of SSH weak implementation and weak keys.<sup class=""reference"" id=""cite_ref-257""><a href=""#cite_note-257"">[257]</a></sup>
</p>, <p>IoT systems are typically controlled by event-driven smart apps that take as input either sensed data, user inputs, or other external triggers (from the Internet) and command one or more actuators towards providing different forms of automation.<sup class=""reference"" id=""cite_ref-IoTSan_258-0""><a href=""#cite_note-IoTSan-258"">[258]</a></sup> Examples of sensors include smoke detectors, motion sensors, and contact sensors. Examples of actuators include smart locks, smart power outlets, and door controls. Popular control platforms on which third-party developers can build smart apps that interact wirelessly with these sensors and actuators include Samsung's SmartThings,<sup class=""reference"" id=""cite_ref-259""><a href=""#cite_note-259"">[259]</a></sup> Apple's HomeKit,<sup class=""reference"" id=""cite_ref-260""><a href=""#cite_note-260"">[260]</a></sup> and Amazon's Alexa,<sup class=""reference"" id=""cite_ref-261""><a href=""#cite_note-261"">[261]</a></sup> among others.
</p>, <p>A problem specific to IoT systems is that buggy apps, unforeseen bad app interactions, or device/communication failures, can cause unsafe and dangerous physical states, e.g., ""unlock the entrance door when no one is at home"" or ""turn off the heater when the temperature is below 0 degrees Celsius and people are sleeping at night"".<sup class=""reference"" id=""cite_ref-IoTSan_258-1""><a href=""#cite_note-IoTSan-258"">[258]</a></sup> Detecting flaws that lead to such states, requires a holistic view of installed apps, component devices, their configurations, and more importantly, how they interact. Recently, researchers from the University of California Riverside have proposed IotSan, a novel practical system that uses model checking as a building block to reveal ""interaction-level"" flaws by identifying events that can lead the system to unsafe states.<sup class=""reference"" id=""cite_ref-IoTSan_258-2""><a href=""#cite_note-IoTSan-258"">[258]</a></sup> They have evaluated IotSan on the Samsung SmartThings platform. From 76 manually configured systems, IotSan detects 147 vulnerabilities (i.e., violations of safe physical states/properties).
</p>, <p>Given widespread recognition of the evolving nature of the design and management of the Internet of things, sustainable and secure deployment of IoT solutions must design for ""anarchic scalability.""<sup class=""reference"" id=""cite_ref-FieldingArch00_262-0""><a href=""#cite_note-FieldingArch00-262"">[262]</a></sup> Application of the concept of anarchic scalability can be extended to physical systems (i.e. controlled real-world objects), by virtue of those systems being designed to account for uncertain management futures. This hard anarchic scalability thus provides a pathway forward to fully realize the potential of Internet-of-things solutions by selectively constraining physical systems to allow for all management regimes without risking physical failure.<sup class=""reference"" id=""cite_ref-FieldingArch00_262-1""><a href=""#cite_note-FieldingArch00-262"">[262]</a></sup>
</p>, <p>Brown University computer scientist <a class=""mw-redirect"" href=""/wiki/Michael_Littman"" title=""Michael Littman"">Michael Littman</a> has argued that successful execution of the Internet of things requires consideration of the interface's usability as well as the technology itself. These interfaces need to be not only more user-friendly but also better integrated: ""If users need to learn different interfaces for their vacuums, their locks, their sprinklers, their lights, and their coffeemakers, it's tough to say that their lives have been made any easier.""<sup class=""reference"" id=""cite_ref-263""><a href=""#cite_note-263"">[263]</a></sup>
</p>, <p>A concern regarding Internet-of-things technologies pertains to the environmental impacts of the manufacture, use, and eventual disposal of all these semiconductor-rich devices.<sup class=""reference"" id=""cite_ref-The_Internet_of_Things_Could_Drown_Our_Environment_in_Gadgets_264-0""><a href=""#cite_note-The_Internet_of_Things_Could_Drown_Our_Environment_in_Gadgets-264"">[264]</a></sup> Modern electronics are replete with a wide variety of heavy metals and rare-earth metals, as well as highly toxic synthetic chemicals. This makes them extremely difficult to properly recycle. Electronic components are often incinerated or placed in regular landfills. Furthermore, the human and environmental cost of mining the rare-earth metals that are integral to modern electronic components continues to grow. This leads to societal questions concerning the environmental impacts of IoT devices over their lifetime.<sup class=""reference"" id=""cite_ref-RowlandDesigning15_265-0""><a href=""#cite_note-RowlandDesigning15-265"">[265]</a></sup>
</p>, <p>The <a href=""/wiki/Electronic_Frontier_Foundation"" title=""Electronic Frontier Foundation"">Electronic Frontier Foundation</a> has raised concerns that companies can use the technologies necessary to support connected devices to intentionally disable or ""<a href=""/wiki/Brick_(electronics)"" title=""Brick (electronics)"">brick</a>"" their customers' devices via a remote software update or by disabling a service necessary to the operation of the device. In one example, <a href=""/wiki/Home_automation"" title=""Home automation"">home automation</a> devices sold with the promise of a ""Lifetime Subscription"" were rendered useless after <a class=""mw-redirect"" href=""/wiki/Nest_Labs"" title=""Nest Labs"">Nest Labs</a> acquired Revolv and made the decision to shut down the central servers the Revolv devices had used to operate.<sup class=""reference"" id=""cite_ref-266""><a href=""#cite_note-266"">[266]</a></sup> As Nest is a company owned by <a href=""/wiki/Alphabet_Inc."" title=""Alphabet Inc."">Alphabet</a> (<a href=""/wiki/Google"" title=""Google"">Google's</a> parent company), the EFF argues this sets a ""terrible precedent for a company with ambitions to sell self-driving cars, medical devices, and other high-end gadgets that may be essential to a person's livelihood or physical safety.""<sup class=""reference"" id=""cite_ref-effrem_267-0""><a href=""#cite_note-effrem-267"">[267]</a></sup>
</p>, <p>Owners should be free to point their devices to a different server or collaborate on improved software. But such action violates the United States <a href=""/wiki/Digital_Millennium_Copyright_Act"" title=""Digital Millennium Copyright Act"">DMCA</a> section 1201, which only has an exemption for ""local use"". This forces tinkerers who want to keep using their own equipment into a legal grey area. EFF thinks buyers should refuse electronics and software that prioritize the manufacturer's wishes above their own.<sup class=""reference"" id=""cite_ref-effrem_267-1""><a href=""#cite_note-effrem-267"">[267]</a></sup>
</p>, <p>Examples of post-sale manipulations include <a class=""mw-redirect"" href=""/wiki/List_of_mergers_and_acquisitions_by_Google"" title=""List of mergers and acquisitions by Google"">Google Nest</a> Revolv, disabled privacy settings on <a href=""/wiki/Android_(operating_system)"" title=""Android (operating system)"">Android</a>, Sony disabling <a href=""/wiki/Linux"" title=""Linux"">Linux</a> on <a href=""/wiki/PlayStation_3"" title=""PlayStation 3"">PlayStation 3</a>, enforced <a href=""/wiki/End-user_license_agreement"" title=""End-user license agreement"">EULA</a> on <a href=""/wiki/Wii_U"" title=""Wii U"">Wii U</a>.<sup class=""reference"" id=""cite_ref-effrem_267-2""><a href=""#cite_note-effrem-267"">[267]</a></sup>
</p>, <p>Kevin Lonergan at <i>Information Age</i>, a business technology magazine, has referred to the terms surrounding the IoT as a ""terminology zoo"".<sup class=""reference"" id=""cite_ref-Vitesse_Media_Plc_268-0""><a href=""#cite_note-Vitesse_Media_Plc-268"">[268]</a></sup> The lack of clear terminology is not ""useful from a practical point of view"" and a ""source of confusion for the end user"".<sup class=""reference"" id=""cite_ref-Vitesse_Media_Plc_268-1""><a href=""#cite_note-Vitesse_Media_Plc-268"">[268]</a></sup> A company operating in the IoT space could be working in anything related to sensor technology, networking, embedded systems, or analytics.<sup class=""reference"" id=""cite_ref-Vitesse_Media_Plc_268-2""><a href=""#cite_note-Vitesse_Media_Plc-268"">[268]</a></sup> According to Lonergan, the term IoT was coined before smart phones, tablets, and devices as we know them today existed, and there is a long list of terms with varying degrees of overlap and <a href=""/wiki/Technological_convergence"" title=""Technological convergence"">technological convergence</a>: Internet of things, Internet of everything (IoE), Internet of goods (supply chain), industrial Internet, <a class=""mw-redirect"" href=""/wiki/Pervasive_computing"" title=""Pervasive computing"">pervasive computing</a>, pervasive sensing, <a href=""/wiki/Ubiquitous_computing"" title=""Ubiquitous computing"">ubiquitous computing</a>, <a href=""/wiki/Cyber-physical_system"" title=""Cyber-physical system"">cyber-physical systems</a> (CPS), <a href=""/wiki/Wireless_sensor_network"" title=""Wireless sensor network"">wireless sensor networks</a> (WSN), <a href=""/wiki/Smart_object"" title=""Smart object"">smart objects</a>, <a href=""/wiki/Digital_twin"" title=""Digital twin"">digital twin</a>, cyberobjects or avatars,<sup class=""reference"" id=""cite_ref-Gautier_2011_139-1""><a href=""#cite_note-Gautier_2011-139"">[139]</a></sup> cooperating objects, <a href=""/wiki/Machine_to_machine"" title=""Machine to machine"">machine to machine</a> (M2M), ambient intelligence (AmI), <a class=""mw-redirect"" href=""/wiki/Operational_Technology"" title=""Operational Technology"">Operational technology</a> (OT), and <a href=""/wiki/Information_technology"" title=""Information technology"">information technology</a> (IT).<sup class=""reference"" id=""cite_ref-Vitesse_Media_Plc_268-3""><a href=""#cite_note-Vitesse_Media_Plc-268"">[268]</a></sup> Regarding IIoT, an industrial sub-field of IoT, the <a href=""/wiki/Industrial_Internet_Consortium"" title=""Industrial Internet Consortium"">Industrial Internet Consortium</a>'s Vocabulary Task Group has created a ""common and reusable vocabulary of terms""<sup class=""reference"" id=""cite_ref-Technology_Working_Group_269-0""><a href=""#cite_note-Technology_Working_Group-269"">[269]</a></sup> to ensure ""consistent terminology""<sup class=""reference"" id=""cite_ref-Technology_Working_Group_269-1""><a href=""#cite_note-Technology_Working_Group-269"">[269]</a></sup><sup class=""reference"" id=""cite_ref-270""><a href=""#cite_note-270"">[270]</a></sup> across publications issued by the Industrial Internet Consortium. IoT One has created an IoT Terms Database including a New Term Alert<sup class=""reference"" id=""cite_ref-271""><a href=""#cite_note-271"">[271]</a></sup> to be notified when a new term is published. As of March 2020<sup class=""plainlinks noexcerpt noprint asof-tag update"" style=""display:none;""><a class=""external text"" href=""https://en.wikipedia.org/w/index.php?title=Internet_of_things&amp;action=edit"">[update]</a></sup>, this database aggregates 807 IoT-related terms, while keeping material ""transparent and comprehensive.""<sup class=""reference"" id=""cite_ref-272""><a href=""#cite_note-272"">[272]</a></sup><sup class=""reference"" id=""cite_ref-IoTONEGuide_273-0""><a href=""#cite_note-IoTONEGuide-273"">[273]</a></sup>
</p>, <p>Despite a shared belief in the potential of the IoT, industry leaders and consumers are facing barriers to adopt IoT technology more widely. Mike Farley argued in <a href=""/wiki/Forbes"" title=""Forbes"">Forbes</a> that while IoT solutions appeal to <a class=""mw-redirect"" href=""/wiki/Early_adopters"" title=""Early adopters"">early adopters</a>, they either lack interoperability or a clear use case for end-users.<sup class=""reference"" id=""cite_ref-Forbes_274-0""><a href=""#cite_note-Forbes-274"">[274]</a></sup> A study by Ericsson regarding the adoption of IoT among Danish companies suggests that many struggle ""to pinpoint exactly where the value of IoT lies for them"".<sup class=""reference"" id=""cite_ref-every_275-0""><a href=""#cite_note-every-275"">[275]</a></sup>
</p>, <p>As for IoT, especially in regards to consumer IoT, information about a user's daily routine is collected so that the “things” around the user can cooperate to provide better services that fulfill personal preference.<sup class=""reference"" id=""cite_ref-276""><a href=""#cite_note-276"">[276]</a></sup> When the collected information which describes a user in detail travels through multiple <a href=""/wiki/Hops"" title=""Hops"">hops</a> in a network, due to a diverse integration of services, devices and network, the information stored on a device is vulnerable to <a class=""mw-redirect"" href=""/wiki/Privacy_violation"" title=""Privacy violation"">privacy violation</a> by compromising nodes existing in an IoT network.<sup class=""reference"" id=""cite_ref-277""><a href=""#cite_note-277"">[277]</a></sup>
</p>, <p>For example, on 21 October 2016, a multiple <a class=""mw-redirect"" href=""/wiki/Distributed_denial_of_service"" title=""Distributed denial of service"">distributed denial of service</a> (DDoS) attacks systems operated by <a class=""mw-redirect"" href=""/wiki/Domain_name_system"" title=""Domain name system"">domain name system</a> provider Dyn, which caused the inaccessibility of several websites, such as <a href=""/wiki/GitHub"" title=""GitHub"">GitHub</a>, <a href=""/wiki/Twitter"" title=""Twitter"">Twitter</a>, and others. This attack is executed through a <a href=""/wiki/Botnet"" title=""Botnet"">botnet</a> consisting of a large number of IoT devices including IP cameras, <a href=""/wiki/Residential_gateway"" title=""Residential gateway"">gateways</a>, and even baby monitors.<sup class=""reference"" id=""cite_ref-278""><a href=""#cite_note-278"">[278]</a></sup>
</p>, <p>Fundamentally there are 4 security objectives that the IoT system requires: (1) data <a href=""/wiki/Confidentiality"" title=""Confidentiality"">confidentiality</a>: unauthorized parties cannot have access to the transmitted and stored data; (2) data <a href=""/wiki/Integrity"" title=""Integrity"">integrity</a>: intentional and unintentional <a href=""/wiki/Corruption"" title=""Corruption"">corruption</a> of transmitted and stored data must be detected; (3) <a href=""/wiki/Non-repudiation"" title=""Non-repudiation"">non-repudiation</a>: the sender cannot deny having sent a given message; (4) data availability: the transmitted and stored data should be available to authorized parties even with the <a class=""mw-redirect"" href=""/wiki/Denial-of-service"" title=""Denial-of-service"">denial-of-service</a> (DOS) attacks.<sup class=""reference"" id=""cite_ref-279""><a href=""#cite_note-279"">[279]</a></sup>
</p>, <p>Information privacy regulations also require organizations to practice ""reasonable security"". <a class=""external text"" href=""https://leginfo.legislature.ca.gov/faces/billNavClient.xhtml?bill_id=201720180SB327"" rel=""nofollow"">California's SB-327 Information privacy: connected devices</a> ""would require a manufacturer of a connected device, as those terms are defined, to equip the device with a reasonable security feature or features that are appropriate to the nature and function of the device, appropriate to the information it may collect, contain, or transmit, and designed to protect the device and any information contained therein from unauthorized access, destruction, use, modification, or disclosure, as specified.""<sup class=""reference"" id=""cite_ref-280""><a href=""#cite_note-280"">[280]</a></sup> As each organization's environment is unique, it can prove challenging to demonstrate what ""reasonable security"" is and what potential risks could be involved for the business. Oregon's <a class=""external text"" href=""https://olis.leg.state.or.us/liz/2019R1/Measures/Overview/HB2395"" rel=""nofollow"">HB 2395</a> also ""requires [a] <i>person that manufactures, sells or offers to sell connected device</i>] <b>manufacturer</b> to equip connected device with reasonable security features that protect <b>connected device and</b> information that connected device <i>collects, contains, stores or transmits</i>] <b>stores</b> from access, destruction, modification, use or disclosure that consumer does not authorize.""<sup class=""reference"" id=""cite_ref-281""><a href=""#cite_note-281"">[281]</a></sup>
</p>, <p>According to antivirus provider <a href=""/wiki/Kaspersky_Lab"" title=""Kaspersky Lab"">Kaspersky</a>, there were 639 million data breaches of IoT devices in 2020 and 1.5 billion breaches in the first six months of 2021.<sup class=""reference"" id=""cite_ref-“page”_197-1""><a href=""#cite_note-“page”-197"">[197]</a></sup>
</p>, <p>A study issued by Ericsson regarding the adoption of Internet of things among Danish companies identified a ""clash between IoT and companies' traditional <a href=""/wiki/Governance"" title=""Governance"">governance</a> structures, as IoT still presents both uncertainties and a lack of historical precedence.""<sup class=""reference"" id=""cite_ref-every_275-1""><a href=""#cite_note-every-275"">[275]</a></sup> Among the respondents interviewed, 60 percent stated that they ""do not believe they have the organizational capabilities, and three of four do not believe they have the processes needed, to capture the IoT opportunity.""<sup class=""reference"" id=""cite_ref-every_275-2""><a href=""#cite_note-every-275"">[275]</a></sup> This has led to a need to understand <a href=""/wiki/Organizational_culture"" title=""Organizational culture"">organizational culture</a> in order to facilitate <a href=""/wiki/Organizational_architecture"" title=""Organizational architecture"">organizational design</a> processes and to test new <a href=""/wiki/Innovation_management"" title=""Innovation management"">innovation management</a> practices. A lack of digital leadership in the age of <a href=""/wiki/Digital_transformation"" title=""Digital transformation"">digital transformation</a> has also stifled innovation and IoT adoption to a degree that many companies, in the face of uncertainty, ""were waiting for the market dynamics to play out"",<sup class=""reference"" id=""cite_ref-every_275-3""><a href=""#cite_note-every-275"">[275]</a></sup> or further action in regards to IoT ""was pending competitor moves, customer pull, or regulatory requirements.""<sup class=""reference"" id=""cite_ref-every_275-4""><a href=""#cite_note-every-275"">[275]</a></sup> Some of these companies risk being ""kodaked"" – ""Kodak was a market leader until digital disruption eclipsed film photography with digital photos"" – failing to ""see the disruptive forces affecting their industry""<sup class=""reference"" id=""cite_ref-kodak_282-0""><a href=""#cite_note-kodak-282"">[282]</a></sup> and ""to truly embrace the new business models the disruptive change opens up.""<sup class=""reference"" id=""cite_ref-kodak_282-1""><a href=""#cite_note-kodak-282"">[282]</a></sup> Scott Anthony has written in <a href=""/wiki/Harvard_Business_Review"" title=""Harvard Business Review"">Harvard Business Review</a> that Kodak ""created a digital camera, invested in the technology, and even understood that photos would be shared online""<sup class=""reference"" id=""cite_ref-kodak_282-2""><a href=""#cite_note-kodak-282"">[282]</a></sup> but ultimately failed to realize that ""online photo sharing <i>was</i> the new business, not just a way to expand the printing business.""<sup class=""reference"" id=""cite_ref-kodak_282-3""><a href=""#cite_note-kodak-282"">[282]</a></sup>
</p>, <p>According to 2018 study, 70–75% of IoT deployments were stuck in the pilot or prototype stage, unable to reach scale due in part to a lack of business planning.<sup class=""reference"" id=""cite_ref-283""><a href=""#cite_note-283"">[283]</a></sup><sup class=""noprint Inline-Template"" style=""white-space:nowrap;"">[<i><a href=""/wiki/Wikipedia:Citing_sources"" title=""Wikipedia:Citing sources""><span title=""This citation requires a reference to the specific page or range of pages in which the material appears. (August 2018)"">page needed</span></a></i>]</sup><sup class=""reference"" id=""cite_ref-284""><a href=""#cite_note-284"">[284]</a></sup>
</p>, <p>Even though scientists, engineers, and managers across the world are continuously working to create and exploit the benefits of IoT products, there are some flaws in the governance, management and implementation of such projects. Despite tremendous forward momentum in the field of information and other underlying technologies, IoT still remains a complex area and the problem of how IoT projects are managed still needs to be addressed. IoT projects must be run differently than simple and traditional IT, manufacturing or construction projects. Because IoT projects have longer project timelines, a lack of skilled resources and several security/legal issues, there is a need for new and specifically designed project processes. The following management techniques should improve the success rate of IoT projects:<sup class=""reference"" id=""cite_ref-285""><a href=""#cite_note-285"">[285]</a></sup>
</p>]"
Cloud computing,"[<p class=""mw-empty-elt"">
</p>, <p><b>Cloud computing</b><sup class=""reference"" id=""cite_ref-urlAn_Introduction_to_Dew_Computing:_Definition''',_Concept_and_Implications_-_IEEE_Journals_&amp;_Magazine_1-0""><a href=""#cite_note-urlAn_Introduction_to_Dew_Computing:_Definition''',_Concept_and_Implications_-_IEEE_Journals_&amp;_Magazine-1"">[1]</a></sup> is the on-demand availability of <a href=""/wiki/Computer"" title=""Computer"">computer</a> <a href=""/wiki/System_resource"" title=""System resource"">system resources</a>, especially data storage (<a href=""/wiki/Cloud_storage"" title=""Cloud storage"">cloud storage</a>) and <a class=""mw-redirect"" href=""/wiki/Computing_power"" title=""Computing power"">computing power</a>, without direct active management by the user.<sup class=""reference"" id=""cite_ref-2""><a href=""#cite_note-2"">[2]</a></sup> Large clouds often have functions <a href=""/wiki/Distributed_computing"" title=""Distributed computing"">distributed</a> over multiple locations, each location being a data center. Cloud computing relies on sharing of resources to achieve coherence and typically using a ""<a class=""mw-disambig"" href=""/wiki/Pay_as_you_go"" title=""Pay as you go"">pay-as-you-go</a>"" model which can help in reducing <a class=""mw-redirect"" href=""/wiki/Capital_expenses"" title=""Capital expenses"">capital expenses</a> but may also lead to unexpected <a href=""/wiki/Operating_expense"" title=""Operating expense"">operating expenses</a> for unaware users.<sup class=""reference"" id=""cite_ref-3""><a href=""#cite_note-3"">[3]</a></sup>
</p>, <p>Advocates of public and hybrid clouds claim that cloud computing allows companies to avoid or minimize up-front <a href=""/wiki/IT_infrastructure"" title=""IT infrastructure"">IT infrastructure</a> costs. Proponents also claim that cloud computing allows <a href=""/wiki/Company"" title=""Company"">enterprises</a> to get their applications up and running faster, with improved manageability and less maintenance, and that it enables IT teams to more rapidly adjust resources to meet fluctuating and unpredictable demand,<sup class=""reference"" id=""cite_ref-aws.amazon_4-0""><a href=""#cite_note-aws.amazon-4"">[4]</a></sup><sup class=""reference"" id=""cite_ref-5""><a href=""#cite_note-5"">[5]</a></sup><sup class=""reference"" id=""cite_ref-6""><a href=""#cite_note-6"">[6]</a></sup> providing <b>burst computing</b> capability: high computing power at certain periods of peak demand.<sup class=""reference"" id=""cite_ref-7""><a href=""#cite_note-7"">[7]</a></sup>
</p>, <p>The term <i>cloud</i> was used to refer to platforms for <a href=""/wiki/Distributed_computing"" title=""Distributed computing"">distributed computing</a> as early as 1993, when <a href=""/wiki/Apple_Inc."" title=""Apple Inc."">Apple</a> spin-off <a href=""/wiki/General_Magic"" title=""General Magic"">General Magic</a> and <a href=""/wiki/AT%26T"" title=""AT&amp;T"">AT&amp;T</a> used it in describing their (paired) <a href=""/wiki/Telescript_(programming_language)"" title=""Telescript (programming language)"">Telescript</a> and PersonaLink technologies.<sup class=""reference"" id=""cite_ref-8""><a href=""#cite_note-8"">[8]</a></sup>  In <i><a href=""/wiki/Wired_(magazine)"" title=""Wired (magazine)"">Wired's</a></i> April 1994 feature ""Bill and Andy's Excellent Adventure II"", <a href=""/wiki/Andy_Hertzfeld"" title=""Andy Hertzfeld"">Andy Hertzfeld</a> commented on Telescript, General Magic's distributed programming language:
</p>, <p>""The beauty of Telescript ... is that now, instead of just having a device to program, we now have the entire Cloud out there, where a single program can go and travel to many different sources of information and create a sort of a virtual service. No one had conceived that before. The example Jim White [the designer of Telescript, <a href=""/wiki/X.400"" title=""X.400"">X.400</a> and <a href=""/wiki/ASN.1"" title=""ASN.1"">ASN.1</a>] uses now is a date-arranging service where a software agent goes to the flower store and orders flowers and then goes to the ticket shop and gets the tickets for the show, and everything is communicated to both parties.""<sup class=""reference"" id=""cite_ref-9""><a href=""#cite_note-9"">[9]</a></sup></p>, <p>During the 1960s, the initial concepts of <a href=""/wiki/Time-sharing"" title=""Time-sharing"">time-sharing</a> became popularized via RJE (<a class=""mw-redirect"" href=""/wiki/Remote_Job_Entry"" title=""Remote Job Entry"">Remote Job Entry</a>);<sup class=""reference"" id=""cite_ref-10""><a href=""#cite_note-10"">[10]</a></sup> this terminology was mostly associated with large vendors such as <a href=""/wiki/IBM"" title=""IBM"">IBM</a> and <a href=""/wiki/Digital_Equipment_Corporation"" title=""Digital Equipment Corporation"">DEC</a>.  Full-time-sharing solutions were available by the early 1970s on such platforms as Multics (on GE hardware), Cambridge CTSS, and the earliest UNIX ports (on DEC hardware). Yet, the ""data center"" model where users submitted jobs to operators to run on IBM's mainframes was overwhelmingly predominant.
</p>, <p>In the 1990s, telecommunications companies, who previously offered primarily dedicated point-to-point data circuits, began offering <a href=""/wiki/Virtual_private_network"" title=""Virtual private network"">virtual private network</a> (VPN) services with comparable quality of service, but at a lower cost. By switching traffic as they saw fit to balance server use, they could use overall network bandwidth more effectively.<sup class=""noprint Inline-Template Template-Fact"" style=""white-space:nowrap;"">[<i><a href=""/wiki/Wikipedia:Citation_needed"" title=""Wikipedia:Citation needed""><span title=""This claim needs references to reliable sources. (May 2021)"">citation needed</span></a></i>]</sup>  They began to use the cloud symbol to denote the demarcation point between what the provider was responsible for and what users were responsible for. Cloud computing extended this boundary to cover all servers as well as the <a class=""mw-redirect"" href=""/wiki/Network_infrastructure"" title=""Network infrastructure"">network infrastructure</a>.<sup class=""reference"" id=""cite_ref-11""><a href=""#cite_note-11"">[11]</a></sup> As computers became more diffused, scientists and technologists explored ways to make large-scale computing power available to more users through time-sharing.<sup class=""noprint Inline-Template Template-Fact"" style=""white-space:nowrap;"">[<i><a href=""/wiki/Wikipedia:Citation_needed"" title=""Wikipedia:Citation needed""><span title=""This claim needs references to reliable sources. (May 2021)"">citation needed</span></a></i>]</sup> They experimented with algorithms to optimize the infrastructure, platform, and applications to prioritize CPUs and increase efficiency for end users.<sup class=""reference"" id=""cite_ref-MITCorbato_12-0""><a href=""#cite_note-MITCorbato-12"">[12]</a></sup>
</p>, <p>The use of the cloud metaphor for virtualized services dates at least to <a href=""/wiki/General_Magic"" title=""General Magic"">General Magic</a> in 1994, where it was used to describe the universe of ""places"" that <a href=""/wiki/Mobile_agent"" title=""Mobile agent"">mobile agents</a> in the <a href=""/wiki/Telescript_(programming_language)"" title=""Telescript (programming language)"">Telescript</a> environment could go. As described by 
<a href=""/wiki/Andy_Hertzfeld"" title=""Andy Hertzfeld"">Andy Hertzfeld</a>:
</p>, <p>""The beauty of <a href=""/wiki/Telescript_(programming_language)"" title=""Telescript (programming language)"">Telescript</a>,"" says <a href=""/wiki/Andy_Hertzfeld"" title=""Andy Hertzfeld"">Andy</a>, ""is that now, instead of just having a device to program, we now have the entire Cloud out there, where a single program can go and travel to many different sources of information and create a sort of a virtual service.""<sup class=""reference"" id=""cite_ref-13""><a href=""#cite_note-13"">[13]</a></sup></p>, <p>The use of the cloud metaphor is credited to General Magic communications employee <a class=""external text"" href=""http://www.whoisdavidhoffman.com/"" rel=""nofollow"">David Hoffman</a>, based on long-standing use in networking and telecom. In addition to use by General Magic itself, it was also used in promoting <a href=""/wiki/AT%26T"" title=""AT&amp;T"">AT&amp;T</a>'s associated PersonaLink Services.<sup class=""reference"" id=""cite_ref-14""><a href=""#cite_note-14"">[14]</a></sup>
</p>, <p>In July 2002, <a class=""mw-redirect"" href=""/wiki/Amazon.com"" title=""Amazon.com"">Amazon</a> created subsidiary <a href=""/wiki/Amazon_Web_Services"" title=""Amazon Web Services"">Amazon Web Services</a>, with the goal to ""enable developers to build innovative and entrepreneurial applications on their own."" In March 2006 Amazon introduced its <a href=""/wiki/Amazon_S3"" title=""Amazon S3"">Simple Storage Service</a> (S3), followed by <a href=""/wiki/Amazon_Elastic_Compute_Cloud"" title=""Amazon Elastic Compute Cloud"">Elastic Compute Cloud</a> (EC2) in August of the same year.<sup class=""reference"" id=""cite_ref-Amazon.com_15-0""><a href=""#cite_note-Amazon.com-15"">[15]</a></sup><sup class=""reference"" id=""cite_ref-company_technologies_16-0""><a href=""#cite_note-company_technologies-16"">[16]</a></sup> These products pioneered the usage of <a class=""mw-redirect"" href=""/wiki/Server_virtualization"" title=""Server virtualization"">server virtualization</a> to deliver <a class=""mw-redirect"" href=""/wiki/IaaS"" title=""IaaS"">IaaS</a> at a cheaper and on-demand pricing basis.
</p>, <p>In April 2008, <a href=""/wiki/Google"" title=""Google"">Google</a> released the beta version of <a href=""/wiki/Google_App_Engine"" title=""Google App Engine"">Google App Engine</a>.<sup class=""reference"" id=""cite_ref-17""><a href=""#cite_note-17"">[17]</a></sup> The App Engine was a <a class=""mw-redirect"" href=""/wiki/PaaS"" title=""PaaS"">PaaS</a> (one of the first of its kind) which provided fully maintained infrastructure and a deployment platform for users to create web applications using common languages/technologies such as <a href=""/wiki/Python_(programming_language)"" title=""Python (programming language)"">Python</a>, <a href=""/wiki/Node.js"" title=""Node.js"">Node.js</a> and <a href=""/wiki/PHP"" title=""PHP"">PHP</a>. The goal was to eliminate the need for some administrative tasks typical of an <a class=""mw-redirect"" href=""/wiki/IaaS"" title=""IaaS"">IaaS</a> model, while creating a platform where users could easily deploy such applications and <a href=""/wiki/Scalability"" title=""Scalability"">scale</a> them to demand.<sup class=""reference"" id=""cite_ref-18""><a href=""#cite_note-18"">[18]</a></sup>
</p>, <p>In early 2008, <a href=""/wiki/NASA"" title=""NASA"">NASA</a>'s <a href=""/wiki/Nebula_(computing_platform)"" title=""Nebula (computing platform)"">Nebula</a>,<sup class=""reference"" id=""cite_ref-19""><a href=""#cite_note-19"">[19]</a></sup> enhanced in the RESERVOIR European Commission-funded project, became the first open-source software for deploying private and hybrid clouds, and for the federation of clouds.<sup class=""reference"" id=""cite_ref-20""><a href=""#cite_note-20"">[20]</a></sup>
</p>, <p>By mid-2008, Gartner saw an opportunity for cloud computing ""to shape the relationship among consumers of IT services, those who use IT services and those who sell them""<sup class=""reference"" id=""cite_ref-21""><a href=""#cite_note-21"">[21]</a></sup> and observed that ""organizations are switching from company-owned hardware and software assets to per-use service-based models"" so that the ""projected shift to computing ... will result in dramatic growth in IT products in some areas and significant reductions in other areas.""<sup class=""reference"" id=""cite_ref-22""><a href=""#cite_note-22"">[22]</a></sup>
</p>, <p>In 2008, the U.S. <a href=""/wiki/National_Science_Foundation"" title=""National Science Foundation"">National Science Foundation</a> began the <a href=""/wiki/Cluster_Exploratory"" title=""Cluster Exploratory"">Cluster Exploratory</a> program to fund academic research using <a href=""/wiki/Google"" title=""Google"">Google</a>-<a href=""/wiki/IBM"" title=""IBM"">IBM</a> cluster technology to analyze massive amounts of data.<sup class=""reference"" id=""cite_ref-23""><a href=""#cite_note-23"">[23]</a></sup>
</p>, <p>In 2009, the government of France announced Project Andromède to create a ""sovereign cloud"" or national cloud computing, with the government to spend €285 million.<sup class=""reference"" id=""cite_ref-24""><a href=""#cite_note-24"">[24]</a></sup><sup class=""reference"" id=""cite_ref-25""><a href=""#cite_note-25"">[25]</a></sup>  The initiative failed badly and <a href=""/wiki/Cloudwatt"" title=""Cloudwatt"">Cloudwatt</a> was shut down on 1 February 2020.<sup class=""reference"" id=""cite_ref-26""><a href=""#cite_note-26"">[26]</a></sup><sup class=""reference"" id=""cite_ref-27""><a href=""#cite_note-27"">[27]</a></sup>
</p>, <p>In February 2010, <a href=""/wiki/Microsoft"" title=""Microsoft"">Microsoft</a> released <a href=""/wiki/Microsoft_Azure"" title=""Microsoft Azure"">Microsoft Azure</a>, which was announced in October 2008.<sup class=""reference"" id=""cite_ref-Azure_28-0""><a href=""#cite_note-Azure-28"">[28]</a></sup>
</p>, <p>In July 2010, <a class=""mw-redirect"" href=""/wiki/Rackspace"" title=""Rackspace"">Rackspace Hosting</a> and <a href=""/wiki/NASA"" title=""NASA"">NASA</a> jointly launched an <a href=""/wiki/Open_source"" title=""Open source"">open-source</a> cloud-software initiative known as <a href=""/wiki/OpenStack"" title=""OpenStack"">OpenStack</a>. The OpenStack project intended to help organizations offering cloud-computing services running on standard hardware. The early code came from NASA's <a href=""/wiki/Nebula_(computing_platform)"" title=""Nebula (computing platform)"">Nebula platform</a> as well as from <a href=""/wiki/Rackspace_Cloud#Cloud_Files"" title=""Rackspace Cloud"">Rackspace's Cloud Files</a> platform. As an open-source offering and along with other open-source solutions such as CloudStack, Ganeti, and OpenNebula, it has attracted attention by several key communities. Several studies aim at comparing these open source offerings based on a set of criteria.<sup class=""reference"" id=""cite_ref-29""><a href=""#cite_note-29"">[29]</a></sup><sup class=""reference"" id=""cite_ref-30""><a href=""#cite_note-30"">[30]</a></sup><sup class=""reference"" id=""cite_ref-31""><a href=""#cite_note-31"">[31]</a></sup><sup class=""reference"" id=""cite_ref-32""><a href=""#cite_note-32"">[32]</a></sup><sup class=""reference"" id=""cite_ref-33""><a href=""#cite_note-33"">[33]</a></sup><sup class=""reference"" id=""cite_ref-34""><a href=""#cite_note-34"">[34]</a></sup><sup class=""reference"" id=""cite_ref-35""><a href=""#cite_note-35"">[35]</a></sup>
</p>, <p>On March 1, 2011, IBM announced the <a class=""mw-redirect"" href=""/wiki/IBM_SmartCloud"" title=""IBM SmartCloud"">IBM SmartCloud</a> framework to support <a href=""/wiki/Smarter_Planet"" title=""Smarter Planet"">Smarter Planet</a>.<sup class=""reference"" id=""cite_ref-36""><a href=""#cite_note-36"">[36]</a></sup> Among the various components of the <a class=""mw-redirect"" href=""/wiki/Smarter_Computing"" title=""Smarter Computing"">Smarter Computing</a> foundation, cloud computing is a critical part. On June 7, 2012, Oracle announced the <a href=""/wiki/Oracle_Cloud"" title=""Oracle Cloud"">Oracle Cloud</a>.<sup class=""reference"" id=""cite_ref-37""><a href=""#cite_note-37"">[37]</a></sup> This cloud offering is poised to be the first to provide users with access to an integrated set of IT solutions, including the Applications (<a class=""mw-redirect"" href=""/wiki/SaaS"" title=""SaaS"">SaaS</a>), Platform (<a class=""mw-redirect"" href=""/wiki/PaaS"" title=""PaaS"">PaaS</a>), and Infrastructure (<a class=""mw-redirect"" href=""/wiki/IaaS"" title=""IaaS"">IaaS</a>) layers.<sup class=""reference"" id=""cite_ref-38""><a href=""#cite_note-38"">[38]</a></sup><sup class=""reference"" id=""cite_ref-39""><a href=""#cite_note-39"">[39]</a></sup><sup class=""reference"" id=""cite_ref-40""><a href=""#cite_note-40"">[40]</a></sup>
</p>, <p>In May 2012, <a href=""/wiki/Google_Compute_Engine"" title=""Google Compute Engine"">Google Compute Engine</a> was released in preview, before being rolled out into General Availability in December 2013.<sup class=""reference"" id=""cite_ref-41""><a href=""#cite_note-41"">[41]</a></sup>
</p>, <p>In 2019, Linux was the most common OS used on <a href=""/wiki/Microsoft_Azure"" title=""Microsoft Azure"">Microsoft Azure</a>.<sup class=""reference"" id=""cite_ref-Linux_on_Azure_42-0""><a href=""#cite_note-Linux_on_Azure-42"">[42]</a></sup> In December 2019, <a href=""/wiki/Amazon_(company)"" title=""Amazon (company)"">Amazon</a> announced <a href=""/wiki/Amazon_Web_Services"" title=""Amazon Web Services"">AWS</a> Outposts, which is a fully managed service that extends AWS infrastructure, AWS services, APIs, and tools to virtually any customer datacenter, co-location space, or on-premises facility for a truly consistent hybrid experience<sup class=""reference"" id=""cite_ref-43""><a href=""#cite_note-43"">[43]</a></sup>
</p>, <p>The goal of cloud computing is to allow users to take benefit from all of these technologies, without the need for deep knowledge about or expertise with each one of them. The cloud aims to cut costs and helps the users focus on their core business instead of being impeded by IT obstacles.<sup class=""reference"" id=""cite_ref-HAM2012_44-0""><a href=""#cite_note-HAM2012-44"">[44]</a></sup> The main enabling technology for cloud computing is <a href=""/wiki/Virtualization"" title=""Virtualization"">virtualization</a>. Virtualization software separates a physical computing device into one or more ""virtual"" devices, each of which can be easily used and managed to perform computing tasks. With <a class=""mw-redirect"" href=""/wiki/Operating_system%E2%80%93level_virtualization"" title=""Operating system–level virtualization"">operating system–level virtualization</a> essentially creating a scalable system of multiple independent computing devices, idle computing resources can be allocated and used more efficiently. Virtualization provides the agility required to speed up IT operations and reduces cost by increasing infrastructure <a href=""/wiki/Rental_utilization"" title=""Rental utilization"">utilization</a>. Autonomic computing automates the process through which the user can provision resources <a href=""/wiki/Code_on_demand"" title=""Code on demand"">on-demand</a>. By minimizing user involvement, automation speeds up the process, reduces labor costs and reduces the possibility of human errors.<sup class=""reference"" id=""cite_ref-HAM2012_44-1""><a href=""#cite_note-HAM2012-44"">[44]</a></sup>
</p>, <p>Cloud computing uses concepts from utility computing to provide <a class=""mw-redirect"" href=""/wiki/Performance_metric"" title=""Performance metric"">metrics</a> for the services used. Cloud computing attempts to address QoS (quality of service) and <a href=""/wiki/Reliability_(computer_networking)"" title=""Reliability (computer networking)"">reliability</a> problems of other <a href=""/wiki/Grid_computing"" title=""Grid computing"">grid computing</a> models.<sup class=""reference"" id=""cite_ref-HAM2012_44-2""><a href=""#cite_note-HAM2012-44"">[44]</a></sup>
</p>, <p>Cloud computing shares characteristics with:
</p>, <p>Cloud computing exhibits the following key characteristics:
</p>, <p>The <a href=""/wiki/National_Institute_of_Standards_and_Technology"" title=""National Institute of Standards and Technology"">National Institute of Standards and Technology</a>'s definition of cloud computing identifies ""five essential characteristics"":
</p>, <p><i>On-demand self-service.</i> A consumer can unilaterally provision computing capabilities, such as server time and network storage, as needed automatically without requiring human interaction with each service provider.
</p>, <p><i>Broad network access.</i> Capabilities are available over the network and accessed through standard mechanisms that promote use by heterogeneous thin or thick client platforms (e.g., mobile phones, tablets, <a href=""/wiki/Laptop"" title=""Laptop"">laptops</a>, and workstations).
</p>, <p><i><a href=""/wiki/Pooling_(resource_management)"" title=""Pooling (resource management)"">Resource pooling</a>.</i> The provider's computing resources are pooled to serve multiple consumers using a multi-tenant model, with different physical and virtual resources dynamically assigned and reassigned according to consumer demand. 
</p>, <p><i>Rapid elasticity.</i> Capabilities can be elastically provisioned and released, in some cases automatically, to scale rapidly outward and inward commensurate with demand. To the consumer, the capabilities available for provisioning often appear unlimited and can be appropriated in any quantity at any time.
</p>, <p><i>Measured service.</i> Cloud systems automatically control and optimize resource use by leveraging a metering capability at some level of abstraction appropriate to the type of service (e.g., storage, processing, bandwidth, and active user accounts). Resource usage can be monitored, controlled, and reported, providing transparency for both the provider and consumer of the utilized service.</p>, <p>Though <a href=""/wiki/Service-oriented_architecture"" title=""Service-oriented architecture"">service-oriented architecture</a> advocates ""Everything as a service"" (with the acronyms <b>EaaS</b> or <b>XaaS</b>,<sup class=""reference"" id=""cite_ref-67""><a href=""#cite_note-67"">[67]</a></sup> or simply <b><a href=""/wiki/As_a_service"" title=""As a service"">aas</a></b>), cloud-computing providers offer their ""services"" according to different models, of which the three standard models per <a class=""mw-redirect"" href=""/wiki/NIST"" title=""NIST"">NIST</a> are Infrastructure as a Service (IaaS), Platform as a Service (PaaS), and Software as a Service (SaaS).<sup class=""reference"" id=""cite_ref-nist_66-1""><a href=""#cite_note-nist-66"">[66]</a></sup> These models offer increasing abstraction; they are thus often portrayed as <i>layers</i> in a <a href=""/wiki/Solution_stack"" title=""Solution stack"">stack</a>: infrastructure-, platform- and software-as-a-service, but these need not be related. For example, one can provide SaaS implemented on physical machines (bare metal), without using underlying PaaS or IaaS layers, and conversely one can run a program on IaaS and access it directly, without wrapping it as SaaS.
</p>, <p>""Infrastructure as a service"" (IaaS) refers to online services that provide high-level <a class=""mw-redirect"" href=""/wiki/Api"" title=""Api"">APIs</a> used to <a href=""/wiki/Abstraction_(computer_science)"" title=""Abstraction (computer science)"">abstract</a> various low-level details of underlying network infrastructure like physical computing resources, location, data partitioning, scaling, security, backup, etc. A <a href=""/wiki/Hypervisor"" title=""Hypervisor"">hypervisor</a> runs the virtual machines as guests. Pools of hypervisors within the cloud operational system can support large numbers of virtual machines and the ability to scale services up and down according to customers' varying requirements. Linux containers run in isolated partitions of a single <a href=""/wiki/Linux_kernel"" title=""Linux kernel"">Linux kernel</a> running directly on the physical hardware. Linux <a href=""/wiki/Cgroups"" title=""Cgroups"">cgroups</a> and namespaces are the underlying Linux kernel technologies used to isolate, secure and manage the containers. Containerisation offers higher performance than virtualization because there is no hypervisor overhead.  IaaS clouds often offer additional resources such as a virtual-machine <a href=""/wiki/Disk_image"" title=""Disk image"">disk-image</a> library, raw <a class=""mw-redirect"" href=""/wiki/Block_storage"" title=""Block storage"">block storage</a>, file or <a href=""/wiki/Object_storage"" title=""Object storage"">object storage</a>, firewalls, load balancers, <a href=""/wiki/IP_address"" title=""IP address"">IP addresses</a>, <a href=""/wiki/VLAN"" title=""VLAN"">virtual local area networks</a> (VLANs), and software bundles.<sup class=""reference"" id=""cite_ref-DHAC_68-0""><a href=""#cite_note-DHAC-68"">[68]</a></sup>
</p>, <p>The <a class=""mw-redirect"" href=""/wiki/NIST"" title=""NIST"">NIST</a>'s definition of cloud computing describes IaaS as ""where the consumer is able to deploy and run arbitrary software, which can include operating systems and applications. The consumer does not manage or control the underlying cloud infrastructure but has control over operating systems, storage, and deployed applications; and possibly limited control of select networking components (e.g., host firewalls).""<sup class=""reference"" id=""cite_ref-nist_66-2""><a href=""#cite_note-nist-66"">[66]</a></sup>
</p>, <p>IaaS-cloud providers supply these resources on-demand from their large pools of equipment installed in <a class=""mw-redirect"" href=""/wiki/Data_centers"" title=""Data centers"">data centers</a>. For <a href=""/wiki/Wide_area_network"" title=""Wide area network"">wide-area</a> connectivity, customers can use either the Internet or <a href=""/wiki/Carrier_cloud"" title=""Carrier cloud"">carrier clouds</a> (dedicated <a href=""/wiki/Virtual_private_network"" title=""Virtual private network"">virtual private networks</a>). To deploy their applications, cloud users install operating-system images and their application software on the cloud infrastructure. In this model, the cloud user patches and maintains the operating systems and the application software. Cloud providers typically bill IaaS services on a utility computing basis: cost reflects the amount of resources allocated and consumed.<sup class=""noprint Inline-Template Template-Fact"" style=""white-space:nowrap;"">[<i><a href=""/wiki/Wikipedia:Citation_needed"" title=""Wikipedia:Citation needed""><span title=""This claim needs references to reliable sources. (May 2021)"">citation needed</span></a></i>]</sup>
</p>, <p>The <a class=""mw-redirect"" href=""/wiki/NIST"" title=""NIST"">NIST</a>'s definition of cloud computing defines Platform as a Service as:<sup class=""reference"" id=""cite_ref-nist_66-3""><a href=""#cite_note-nist-66"">[66]</a></sup>
</p>, <p>The capability provided to the consumer is to deploy onto the cloud infrastructure consumer-created or acquired applications created using programming languages, libraries, services, and tools supported by the provider. The consumer does not manage or control the underlying cloud infrastructure including network, servers, operating systems, or storage, but has control over the deployed applications and possibly configuration settings for the application-hosting environment.</p>, <p>PaaS vendors offer a development environment to application developers. The provider typically develops toolkit and standards for development and channels for distribution and payment. In the PaaS models, cloud providers deliver a <a href=""/wiki/Computing_platform"" title=""Computing platform"">computing platform</a>, typically including operating system, programming-language execution environment, database, and web server. Application developers develop and run their software on a cloud platform instead of directly buying and managing the underlying hardware and software layers. With some PaaS, the underlying computer and storage resources scale automatically to match application demand so that the cloud user does not have to allocate resources manually.<sup class=""reference"" id=""cite_ref-69""><a href=""#cite_note-69"">[69]</a></sup><sup class=""noprint Inline-Template"" style=""white-space:nowrap;"">[<i><a href=""/wiki/Wikipedia:Verifiability"" title=""Wikipedia:Verifiability""><span title=""Quotation needed from source to verify. (July 2015)"">need quotation to verify</span></a></i>]</sup>
</p>, <p>Some integration and data management providers also use specialized applications of PaaS as delivery models for data. Examples include <b>iPaaS (Integration Platform as a Service)</b> and <b>dPaaS (Data Platform as a Service)</b>. iPaaS enables customers to develop, execute and govern integration flows.<sup class=""reference"" id=""cite_ref-GartnerGlossary_70-0""><a href=""#cite_note-GartnerGlossary-70"">[70]</a></sup> Under the iPaaS integration model, customers drive the development and deployment of integrations without installing or managing any hardware or middleware.<sup class=""reference"" id=""cite_ref-GartnerReferenceModel_71-0""><a href=""#cite_note-GartnerReferenceModel-71"">[71]</a></sup> dPaaS delivers integration—and data-management—products as a fully managed service.<sup class=""reference"" id=""cite_ref-ITBusinessEdge_72-0""><a href=""#cite_note-ITBusinessEdge-72"">[72]</a></sup> Under the dPaaS model, the PaaS provider, not the customer, manages the development and execution of programs by building data applications for the customer. dPaaS users access data through <a class=""mw-redirect"" href=""/wiki/Data_visualization"" title=""Data visualization"">data-visualization</a> tools.<sup class=""reference"" id=""cite_ref-EnterpriseCIOForum_73-0""><a href=""#cite_note-EnterpriseCIOForum-73"">[73]</a></sup>
</p>, <p>The <a class=""mw-redirect"" href=""/wiki/NIST"" title=""NIST"">NIST</a>'s definition of cloud computing defines Software as a Service as:<sup class=""reference"" id=""cite_ref-nist_66-4""><a href=""#cite_note-nist-66"">[66]</a></sup>
</p>, <p>The capability provided to the consumer is to use the provider's applications running on a <a class=""mw-redirect"" href=""/wiki/Cloud_infrastructure"" title=""Cloud infrastructure"">cloud infrastructure</a>. The applications are accessible from various client devices through either a thin client interface, such as a web browser (e.g., web-based email), or a program interface. The consumer does not manage or control the underlying cloud infrastructure including network, servers, operating systems, storage, or even individual application capabilities, with the possible exception of limited user-specific application configuration settings.</p>, <p>In the software as a service (SaaS) model, users gain access to application software and <a class=""mw-redirect"" href=""/wiki/Databases"" title=""Databases"">databases</a>. Cloud providers manage the infrastructure and platforms that run the applications. SaaS is sometimes referred to as ""on-demand software"" and is usually priced on a pay-per-use basis or using a subscription fee.<sup class=""reference"" id=""cite_ref-74""><a href=""#cite_note-74"">[74]</a></sup> In the SaaS model, cloud providers install and operate application software in the cloud and cloud users access the software from cloud clients. Cloud users do not manage the cloud infrastructure and platform where the application runs. This eliminates the need to install and run the application on the cloud user's own computers, which simplifies maintenance and support. Cloud applications differ from other applications in their scalability—which can be achieved by cloning tasks onto multiple <a class=""mw-redirect"" href=""/wiki/Virtual_machines"" title=""Virtual machines"">virtual machines</a> at run-time to meet changing work demand.<sup class=""reference"" id=""cite_ref-hamdaqa_75-0""><a href=""#cite_note-hamdaqa-75"">[75]</a></sup> <a class=""mw-redirect"" href=""/wiki/Load_balancer"" title=""Load balancer"">Load balancers</a> distribute the work over the set of virtual machines. This process is transparent to the cloud user, who sees only a single <a class=""new"" href=""/w/index.php?title=Access-point&amp;action=edit&amp;redlink=1"" title=""Access-point (page does not exist)"">access-point</a>. To accommodate a large number of cloud users, cloud applications can be <i><a class=""mw-redirect"" href=""/wiki/Multitenant"" title=""Multitenant"">multitenant</a></i>, meaning that any machine may serve more than one cloud-user organization.
</p>, <p>The pricing model for SaaS applications is typically a monthly or yearly flat fee per user,<sup class=""reference"" id=""cite_ref-Chou_76-0""><a href=""#cite_note-Chou-76"">[76]</a></sup> so prices become scalable and adjustable if users are added or removed at any point. It may also be free.<sup class=""reference"" id=""cite_ref-77""><a href=""#cite_note-77"">[77]</a></sup> Proponents claim that SaaS gives a <a href=""/wiki/Business"" title=""Business"">business</a> the potential to reduce IT operational costs by <a href=""/wiki/Outsourcing"" title=""Outsourcing"">outsourcing</a> hardware and software maintenance and support to the cloud provider. This enables the business to reallocate IT operations costs away from hardware/software spending and from personnel expenses, towards meeting other goals. In addition, with applications hosted centrally, updates can be released without the need for users to install new software. One drawback of SaaS comes with storing the users' data on the cloud provider's server. As a result,<sup class=""noprint Inline-Template Template-Fact"" style=""white-space:nowrap;"">[<i><a href=""/wiki/Wikipedia:Citation_needed"" title=""Wikipedia:Citation needed""><span title=""This claim needs references to reliable sources. (July 2015)"">citation needed</span></a></i>]</sup> there could be <a class=""new"" href=""/w/index.php?title=Unauthorized_access&amp;action=edit&amp;redlink=1"" title=""Unauthorized access (page does not exist)"">unauthorized access</a> to the data.<sup class=""reference"" id=""cite_ref-78""><a href=""#cite_note-78"">[78]</a></sup> Examples of applications offered as SaaS are <a href=""/wiki/Cloud_gaming"" title=""Cloud gaming"">games</a> and productivity software like Google Docs and Word Online. SaaS applications may be integrated with <a href=""/wiki/Cloud_storage"" title=""Cloud storage"">cloud storage</a> or <a href=""/wiki/File_hosting_service"" title=""File hosting service"">File hosting services</a>, which is the case with Google Docs being integrated with <a href=""/wiki/Google_Drive"" title=""Google Drive"">Google Drive</a> and Word Online being integrated with Onedrive.<sup class=""noprint Inline-Template Template-Fact"" style=""white-space:nowrap;"">[<i><a href=""/wiki/Wikipedia:Citation_needed"" title=""Wikipedia:Citation needed""><span title=""This claim needs references to reliable sources. (May 2020)"">citation needed</span></a></i>]</sup>
</p>, <p>In the mobile ""backend"" as a service (m) model, also known as <b>backend as a service (BaaS)</b>, <a class=""mw-redirect"" href=""/wiki/Web_app"" title=""Web app"">web app</a> and <a href=""/wiki/Mobile_app"" title=""Mobile app"">mobile app</a> developers are provided with a way to link their applications to <a href=""/wiki/Cloud_storage"" title=""Cloud storage"">cloud storage</a> and cloud computing services with <a class=""mw-redirect"" href=""/wiki/Application_programming_interface"" title=""Application programming interface"">application programming interfaces</a> (APIs) exposed to their applications and custom <a href=""/wiki/Software_development_kit"" title=""Software development kit"">software development kits</a> (SDKs). Services include user management, <a href=""/wiki/Push_technology"" title=""Push technology"">push notifications</a>, integration with <a href=""/wiki/Social_networking_service"" title=""Social networking service"">social networking services</a><sup class=""reference"" id=""cite_ref-PandoDailyAP_79-0""><a href=""#cite_note-PandoDailyAP-79"">[79]</a></sup> and more. This is a relatively recent model in cloud computing,<sup class=""reference"" id=""cite_ref-Williams1_80-0""><a href=""#cite_note-Williams1-80"">[80]</a></sup> with most BaaS <a href=""/wiki/Startup_company"" title=""Startup company"">startups</a> dating from 2011 or later<sup class=""reference"" id=""cite_ref-Tan12_81-0""><a href=""#cite_note-Tan12-81"">[81]</a></sup><sup class=""reference"" id=""cite_ref-Rowinski11_82-0""><a href=""#cite_note-Rowinski11-82"">[82]</a></sup><sup class=""reference"" id=""cite_ref-Mishra_83-0""><a href=""#cite_note-Mishra-83"">[83]</a></sup> but trends indicate that these services are gaining significant mainstream traction with enterprise consumers.<sup class=""reference"" id=""cite_ref-built.io_84-0""><a href=""#cite_note-built.io-84"">[84]</a></sup>
</p>, <p>Serverless computing is a cloud computing code <a href=""/wiki/Execution_(computing)"" title=""Execution (computing)"">execution</a> model in which the cloud provider fully manages starting and stopping <a class=""mw-redirect"" href=""/wiki/Virtual_machines"" title=""Virtual machines"">virtual machines</a> as necessary to serve requests, and requests are billed by an abstract measure of the resources required to satisfy the request, rather than per virtual machine, per hour.<sup class=""reference"" id=""cite_ref-techcrunch-lambda_85-0""><a href=""#cite_note-techcrunch-lambda-85"">[85]</a></sup> Despite the name, it does not actually involve running code without servers.<sup class=""reference"" id=""cite_ref-techcrunch-lambda_85-1""><a href=""#cite_note-techcrunch-lambda-85"">[85]</a></sup> Serverless computing is so named because the business or person that owns the system does not have to purchase, rent or provide servers or virtual machines for the <a href=""/wiki/Back-end_database"" title=""Back-end database"">back-end</a> code to run on.
</p>, <p>Function as a service (FaaS) is a service-hosted remote procedure call that leverages serverless computing to enable the deployment of individual functions in the cloud that run in response to events.<sup class=""reference"" id=""cite_ref-86""><a href=""#cite_note-86"">[86]</a></sup> FaaS is considered by some to come under the umbrella of <i><a href=""/wiki/Serverless_computing"" title=""Serverless computing"">serverless computing</a></i>, while some others use the terms interchangeably.<sup class=""reference"" id=""cite_ref-87""><a href=""#cite_note-87"">[87]</a></sup>
</p>, <p>Private cloud is cloud infrastructure operated solely for a single organization, whether managed internally or by a third party, and hosted either internally or externally.<sup class=""reference"" id=""cite_ref-nist_66-5""><a href=""#cite_note-nist-66"">[66]</a></sup> Undertaking a private cloud project requires significant engagement to virtualize the business environment, and requires the organization to reevaluate decisions about existing resources. It can improve business, but every step in the project raises security issues that must be addressed to prevent serious vulnerabilities. Self-run <a href=""/wiki/Data_center"" title=""Data center"">data centers</a><sup class=""reference"" id=""cite_ref-88""><a href=""#cite_note-88"">[88]</a></sup> are generally capital intensive. They have a significant physical footprint, requiring allocations of space, hardware, and environmental controls. These assets have to be refreshed periodically, resulting in additional capital expenditures. They have attracted criticism because users ""still have to buy, build, and manage them"" and thus do not benefit from less hands-on management,<sup class=""reference"" id=""cite_ref-iwpc_89-0""><a href=""#cite_note-iwpc-89"">[89]</a></sup> essentially ""[lacking] the economic model that makes cloud computing such an intriguing concept"".<sup class=""reference"" id=""cite_ref-90""><a href=""#cite_note-90"">[90]</a></sup><sup class=""reference"" id=""cite_ref-91""><a href=""#cite_note-91"">[91]</a></sup>
</p>, <p>Cloud services are considered ""public"" when they are delivered over the public Internet, and they may be offered as a paid subscription, or free of charge.<sup class=""reference"" id=""cite_ref-92""><a href=""#cite_note-92"">[92]</a></sup> Architecturally, there are few differences between public- and private-cloud services, but security concerns increase substantially when services (applications, storage, and other resources) are shared by multiple customers. Most public-cloud providers offer direct-connection services that allow customers to securely link their legacy data centers to their cloud-resident applications.<sup class=""reference"" id=""cite_ref-idc_49-3""><a href=""#cite_note-idc-49"">[49]</a></sup><sup class=""reference"" id=""cite_ref-93""><a href=""#cite_note-93"">[93]</a></sup>
</p>, <p>Several factors like the functionality of the solutions, <a class=""mw-redirect"" href=""/wiki/Costing"" title=""Costing"">cost</a>, integrational and <a href=""/wiki/Organization"" title=""Organization"">organizational</a> aspects as well as <a href=""/wiki/Security"" title=""Security"">safety &amp; security</a> are influencing the decision of enterprises and organizations to choose a public cloud or <a href=""/wiki/On-premises_software"" title=""On-premises software"">on-premises</a> solution.<sup class=""reference"" id=""cite_ref-94""><a href=""#cite_note-94"">[94]</a></sup>
</p>, <p><b>Hybrid</b> cloud is a composition of a public cloud and a private environment, such as a private cloud or on-premises resources,<sup class=""reference"" id=""cite_ref-95""><a href=""#cite_note-95"">[95]</a></sup><sup class=""reference"" id=""cite_ref-96""><a href=""#cite_note-96"">[96]</a></sup> that remain distinct entities but are bound together, offering the benefits of multiple deployment models. Hybrid cloud can also mean the ability to connect collocation, managed and/or dedicated services with cloud resources.<sup class=""reference"" id=""cite_ref-nist_66-6""><a href=""#cite_note-nist-66"">[66]</a></sup> <a href=""/wiki/Gartner"" title=""Gartner"">Gartner</a> defines a hybrid cloud service as a cloud computing service that is composed of some combination of private, public and community cloud services, from different service providers.<sup class=""reference"" id=""cite_ref-97""><a href=""#cite_note-97"">[97]</a></sup> A hybrid cloud service crosses isolation and provider boundaries so that it can't be simply put in one category of private, public, or community cloud service. It allows one to extend either the capacity or the capability of a cloud service, by aggregation, integration or customization with another cloud service.
</p>, <p>Varied use cases for hybrid cloud composition exist. For example, an organization may store sensitive client data in house on a private cloud application, but interconnect that application to a business intelligence application provided on a public cloud as a software service.<sup class=""reference"" id=""cite_ref-98""><a href=""#cite_note-98"">[98]</a></sup> This example of hybrid cloud extends the capabilities of the enterprise to deliver a specific business service through the addition of externally available public cloud services. Hybrid cloud adoption depends on a number of factors such as data security and compliance requirements, level of control needed over data, and the applications an organization uses.<sup class=""reference"" id=""cite_ref-99""><a href=""#cite_note-99"">[99]</a></sup>
</p>, <p>Another example of hybrid cloud is one where <a href=""/wiki/Information_technology"" title=""Information technology"">IT</a> organizations use public cloud computing resources to meet temporary capacity needs that can not be met by the private cloud.<sup class=""reference"" id=""cite_ref-100""><a href=""#cite_note-100"">[100]</a></sup> This capability enables hybrid clouds to employ cloud bursting for scaling across clouds.<sup class=""reference"" id=""cite_ref-nist_66-7""><a href=""#cite_note-nist-66"">[66]</a></sup> Cloud bursting is an application deployment model in which an application runs in a private cloud or data center and ""bursts"" to a public cloud when the demand for computing capacity increases. A primary advantage of cloud bursting and a hybrid cloud model is that an organization pays for extra compute resources only when they are needed.<sup class=""reference"" id=""cite_ref-101""><a href=""#cite_note-101"">[101]</a></sup> Cloud bursting enables data centers to create an in-house IT infrastructure that supports average workloads, and use cloud resources from public or private clouds, during spikes in processing demands.<sup class=""reference"" id=""cite_ref-102""><a href=""#cite_note-102"">[102]</a></sup> The specialized model of hybrid cloud, which is built atop heterogeneous hardware, is called ""Cross-platform Hybrid Cloud"". A cross-platform hybrid cloud is usually powered by different CPU architectures, for example, x86-64 and ARM, underneath. Users can transparently deploy and scale applications without knowledge of the cloud's hardware diversity.<sup class=""reference"" id=""cite_ref-103""><a href=""#cite_note-103"">[103]</a></sup> This kind of cloud emerges from the rise of ARM-based system-on-chip for server-class computing.
</p>, <p>Hybrid cloud infrastructure essentially serves to eliminate limitations inherent to the multi-access relay characteristics of private cloud networking. The advantages include enhanced runtime flexibility and adaptive memory processing unique to virtualized interface models.<sup class=""reference"" id=""cite_ref-104""><a href=""#cite_note-104"">[104]</a></sup>
</p>, <p><a href=""/wiki/Community_cloud"" title=""Community cloud"">Community cloud</a> shares infrastructure between several organizations from a specific community with common concerns (security, compliance, jurisdiction, etc.), whether managed internally or by a third-party, and either hosted internally or externally. The costs are spread over fewer users than a public cloud (but more than a private cloud), so only some of the cost savings potential of cloud computing are realized.<sup class=""reference"" id=""cite_ref-nist_66-8""><a href=""#cite_note-nist-66"">[66]</a></sup>
</p>, <p>A cloud computing platform can be assembled from a distributed set of machines in different locations, connected to a single network or hub service. It is possible to distinguish between two types of distributed clouds: public-resource computing and volunteer cloud.
</p>, <p>Multicloud is the use of multiple cloud computing services in a single heterogeneous architecture to reduce reliance on single vendors, increase flexibility through choice, mitigate against disasters, etc. It differs from hybrid cloud in that it refers to multiple cloud services, rather than multiple deployment modes (public, private, legacy).<sup class=""reference"" id=""cite_ref-rouse_106-0""><a href=""#cite_note-rouse-106"">[106]</a></sup><sup class=""reference"" id=""cite_ref-king_107-0""><a href=""#cite_note-king-107"">[107]</a></sup><sup class=""reference"" id=""cite_ref-108""><a href=""#cite_note-108"">[108]</a></sup>
</p>, <p>Poly cloud refers to the use of multiple public clouds for the purpose of leveraging specific services that each provider offers. It differs from Multi cloud in that it is not designed to increase flexibility or mitigate against failures but is rather used to allow an organization to achieve more that could be done with a single provider.<sup class=""reference"" id=""cite_ref-109""><a href=""#cite_note-109"">[109]</a></sup>
</p>, <p>The issues of transferring large amounts of data to the cloud as well as data security once the data is in the cloud initially hampered adoption of cloud for <a href=""/wiki/Big_data"" title=""Big data"">big data</a>, but now that much data originates in the cloud and with the advent of <a href=""/wiki/Bare-metal_server"" title=""Bare-metal server"">bare-metal servers</a>, the cloud has become<sup class=""reference"" id=""cite_ref-110""><a href=""#cite_note-110"">[110]</a></sup> a solution for use cases including business <a href=""/wiki/Analytics"" title=""Analytics"">analytics</a> and <a class=""mw-redirect"" href=""/wiki/Geospatial_analysis"" title=""Geospatial analysis"">geospatial analysis</a>.<sup class=""reference"" id=""cite_ref-YANG_111-0""><a href=""#cite_note-YANG-111"">[111]</a></sup>
</p>, <p>HPC cloud refers to the use of cloud computing services and infrastructure to execute <a href=""/wiki/High-performance_computing"" title=""High-performance computing"">high-performance computing</a> (HPC) applications.<sup class=""reference"" id=""cite_ref-112""><a href=""#cite_note-112"">[112]</a></sup> These applications consume considerable amount of computing power and memory and are traditionally executed on <a href=""/wiki/Computer_cluster"" title=""Computer cluster"">clusters</a> of computers. In 2016 a handful of companies, including R-HPC, <a href=""/wiki/Amazon_Web_Services"" title=""Amazon Web Services"">Amazon Web Services</a>, <a href=""/wiki/Univa"" title=""Univa"">Univa</a>, <a href=""/wiki/Silicon_Graphics_International"" title=""Silicon Graphics International"">Silicon Graphics International</a>, Sabalcore, Gomput, and <a href=""/wiki/Penguin_Computing"" title=""Penguin Computing"">Penguin Computing</a> offered a high performance computing cloud. The Penguin On Demand (POD) cloud was one of the first non-virtualized remote HPC services offered on a <a href=""/wiki/Prepayment_for_service"" title=""Prepayment for service"">pay-as-you-go</a> basis.<sup class=""reference"" id=""cite_ref-113""><a href=""#cite_note-113"">[113]</a></sup><sup class=""reference"" id=""cite_ref-114""><a href=""#cite_note-114"">[114]</a></sup> Penguin Computing launched its HPC cloud in 2016 as alternative to Amazon's EC2 Elastic Compute Cloud, which uses virtualized computing nodes.<sup class=""reference"" id=""cite_ref-115""><a href=""#cite_note-115"">[115]</a></sup><sup class=""reference"" id=""cite_ref-116""><a href=""#cite_note-116"">[116]</a></sup>
</p>, <p><b><a href=""/wiki/Cloud_computing_architecture"" title=""Cloud computing architecture"">Cloud architecture</a></b>,<sup class=""reference"" id=""cite_ref-117""><a href=""#cite_note-117"">[117]</a></sup> the <a href=""/wiki/Systems_architecture"" title=""Systems architecture"">systems architecture</a> of the <a class=""mw-redirect"" href=""/wiki/Software_systems"" title=""Software systems"">software systems</a> involved in the delivery of cloud computing, typically involves multiple <i>cloud components</i> communicating with each other over a loose coupling mechanism such as a messaging queue. Elastic provision implies intelligence in the use of tight or loose coupling as applied to mechanisms such as these and others.
</p>, <p><b><a href=""/wiki/Cloud_engineering"" title=""Cloud engineering"">Cloud engineering</a></b> is the application of <a href=""/wiki/Engineering"" title=""Engineering"">engineering</a> disciplines of cloud computing. It brings a systematic approach to the high-level concerns of commercialization, standardization and governance in conceiving, developing, operating and maintaining cloud computing systems. It is a multidisciplinary method encompassing contributions from diverse areas such as <a href=""/wiki/Systems_engineering"" title=""Systems engineering"">systems</a>, <a href=""/wiki/Software_engineering"" title=""Software engineering"">software</a>, <a href=""/wiki/Web_engineering"" title=""Web engineering"">web</a>, <a href=""/wiki/Performance_engineering"" title=""Performance engineering"">performance</a>, <a class=""mw-redirect"" href=""/wiki/Information_technology_engineering"" title=""Information technology engineering"">information technology engineering</a>, <a href=""/wiki/Security_engineering"" title=""Security engineering"">security</a>, <a class=""mw-redirect"" href=""/wiki/Platform_engineering"" title=""Platform engineering"">platform</a>, <a href=""/wiki/Risk_analysis_(engineering)"" title=""Risk analysis (engineering)"">risk</a>, and <a href=""/wiki/Quality_control"" title=""Quality control"">quality</a> engineering.
</p>, <p>Cloud computing poses privacy concerns because the service provider can access the data that is in the cloud at any time.  It could accidentally or deliberately alter or delete information.<sup class=""reference"" id=""cite_ref-ryan_118-0""><a href=""#cite_note-ryan-118"">[118]</a></sup> Many cloud providers can share information with third parties if necessary for purposes of law and order without a warrant. That is permitted in their privacy policies, which users must agree to before they start using cloud services. Solutions to privacy include policy and legislation as well as end-users' choices for how data is stored.<sup class=""reference"" id=""cite_ref-ryan_118-1""><a href=""#cite_note-ryan-118"">[118]</a></sup> Users can encrypt data that is processed or stored within the cloud to prevent unauthorized access.<sup class=""reference"" id=""cite_ref-cloudid_119-0""><a href=""#cite_note-cloudid-119"">[119]</a></sup><sup class=""reference"" id=""cite_ref-ryan_118-2""><a href=""#cite_note-ryan-118"">[118]</a></sup> <a class=""mw-redirect"" href=""/wiki/Identity_management_systems"" title=""Identity management systems"">Identity management systems</a> can also provide practical solutions to privacy concerns in cloud computing. These systems distinguish between authorized and unauthorized users and determine the amount of data that is accessible to each entity.<sup class=""reference"" id=""cite_ref-120""><a href=""#cite_note-120"">[120]</a></sup> The systems work by creating and describing identities, recording activities, and getting rid of unused identities.
</p>, <p>According to the Cloud Security Alliance, the top three threats in the cloud are <i>Insecure Interfaces and APIs</i>, <i>Data Loss &amp; Leakage</i>, and <i>Hardware Failure</i>—which accounted for 29%, 25% and 10% of all cloud security outages respectively. Together, these form shared technology vulnerabilities. In a cloud provider platform being shared by different users, there may be a possibility that information belonging to different customers resides on the same data server. Additionally, <a href=""/wiki/Eugene_Schultz"" title=""Eugene Schultz"">Eugene Schultz</a>, chief technology officer at Emagined Security, said that hackers are spending substantial time and effort looking for ways to penetrate the cloud. ""There are some real Achilles' heels in the cloud infrastructure that are making big holes for the bad guys to get into"". Because data from hundreds or thousands of companies can be stored on large cloud servers, hackers can theoretically gain control of huge stores of information through a single attack—a process he called ""hyperjacking"". Some examples of this include the Dropbox security breach, and iCloud 2014 leak.<sup class=""reference"" id=""cite_ref-psg.hitachi-solutions.com_121-0""><a href=""#cite_note-psg.hitachi-solutions.com-121"">[121]</a></sup> Dropbox had been breached in October 2014, having over 7 million of its users passwords stolen by hackers in an effort to get monetary value from it by Bitcoins (BTC). By having these passwords, they are able to read <a href=""/wiki/Information_privacy"" title=""Information privacy"">private data</a> as well as have this data be indexed by search engines (making the information public).<sup class=""reference"" id=""cite_ref-psg.hitachi-solutions.com_121-1""><a href=""#cite_note-psg.hitachi-solutions.com-121"">[121]</a></sup>
</p>, <p>There is the problem of legal ownership of the data (If a user stores some data in the cloud, can the cloud provider profit from it?). Many Terms of Service agreements are silent on the question of ownership.<sup class=""reference"" id=""cite_ref-122""><a href=""#cite_note-122"">[122]</a></sup> Physical control of the computer equipment (private cloud) is more secure than having the equipment off-site and under someone else's control (public cloud). This delivers great incentive to public cloud computing service providers to prioritize building and maintaining strong management of secure services.<sup class=""reference"" id=""cite_ref-123""><a href=""#cite_note-123"">[123]</a></sup> Some small businesses that don't have expertise in <a href=""/wiki/Information_technology"" title=""Information technology"">IT</a> security could find that it's more secure for them to use a public cloud. There is the risk that end users do not understand the issues involved when signing on to a cloud service (persons sometimes don't read the many pages of the terms of service agreement, and just click ""Accept"" without reading). This is important now that cloud computing is becoming popular and required for some services to work, for example for an <a class=""mw-redirect"" href=""/wiki/Intelligent_personal_assistant"" title=""Intelligent personal assistant"">intelligent personal assistant</a> (Apple's <a href=""/wiki/Siri"" title=""Siri"">Siri</a> or <a href=""/wiki/Google_Now"" title=""Google Now"">Google Now</a>). Fundamentally, private cloud is seen as more secure with higher levels of control for the owner, however public cloud is seen to be more flexible and requires less time and money investment from the user.<sup class=""reference"" id=""cite_ref-124""><a href=""#cite_note-124"">[124]</a></sup>
</p>, <p>According to <a href=""/wiki/Bruce_Schneier"" title=""Bruce Schneier"">Bruce Schneier</a>, ""The downside is that you will have limited customization options. Cloud computing is cheaper because of <a href=""/wiki/Economies_of_scale"" title=""Economies of scale"">economics of scale</a>, and—like any outsourced task—you tend to get what you want. A restaurant with a limited menu is cheaper than a personal chef who can cook anything you want. Fewer options at a much cheaper price: it's a feature, not a bug."" He also suggests that ""the cloud provider might not meet your legal needs"" and that businesses need to weigh the benefits of cloud computing against the risks.<sup class=""reference"" id=""cite_ref-125""><a href=""#cite_note-125"">[125]</a></sup>
In cloud computing, the control of the back end infrastructure is limited to the cloud vendor only. Cloud providers often decide on the management policies, which moderates what the cloud users are able to do with their deployment.<sup class=""reference"" id=""cite_ref-126""><a href=""#cite_note-126"">[126]</a></sup> Cloud users are also limited to the control and management of their applications, data and services.<sup class=""reference"" id=""cite_ref-The_real_limits_of_cloud_computing_127-0""><a href=""#cite_note-The_real_limits_of_cloud_computing-127"">[127]</a></sup> This includes <a href=""/wiki/Data_cap"" title=""Data cap"">data caps</a>, which are placed on cloud users by the cloud vendor allocating a certain amount of bandwidth for each customer and are often shared among other cloud users.<sup class=""reference"" id=""cite_ref-The_real_limits_of_cloud_computing_127-1""><a href=""#cite_note-The_real_limits_of_cloud_computing-127"">[127]</a></sup>
</p>, <p>Privacy and <a href=""/wiki/Confidentiality"" title=""Confidentiality"">confidentiality</a> are big concerns in some activities. For instance, sworn translators working under the stipulations of an <a href=""/wiki/Non-disclosure_agreement"" title=""Non-disclosure agreement"">NDA</a>, might face problems regarding <a class=""mw-redirect"" href=""/wiki/Sensitive_data"" title=""Sensitive data"">sensitive data</a> that are not <a class=""mw-redirect"" href=""/wiki/Encrypt"" title=""Encrypt"">encrypted</a>.<sup class=""reference"" id=""cite_ref-128""><a href=""#cite_note-128"">[128]</a></sup> Due to the use of the internet, confidential information such as employee data and user data can be easily available to third-party organisations and people in Cloud Computing.<sup class=""reference"" id=""cite_ref-129""><a href=""#cite_note-129"">[129]</a></sup>
</p>, <p>Cloud computing has some limitations for smaller business operations, particularly regarding security and downtime. Technical outages are inevitable and occur sometimes when cloud service providers (CSPs) become overwhelmed in the process of serving their clients. This may result in temporary business suspension. Since this technology's systems rely on the Internet, an individual cannot access their applications, server, or data from the cloud during an outage.<sup class=""reference"" id=""cite_ref-130""><a href=""#cite_note-130"">[130]</a></sup>
</p>, <p>Cloud computing is still a subject of research.<sup class=""reference"" id=""cite_ref-ghc_131-0""><a href=""#cite_note-ghc-131"">[131]</a></sup> A driving factor in the evolution of cloud computing has been <a href=""/wiki/Chief_technology_officer"" title=""Chief technology officer"">chief technology officers</a> seeking to minimize risk of internal outages and mitigate the complexity of housing network and computing hardware in-house.<sup class=""reference"" id=""cite_ref-132""><a href=""#cite_note-132"">[132]</a></sup> They are also looking to share information to workers located in diverse areas in near and real-time, to enable teams to work seamlessly, no matter where they are located. Since the global pandemic of 2020, cloud technology jumped ahead in popularity due to the level of security of data and the flexibility of working options for all employees, notably remote workers. For example, Zoom grew over 160% in 2020 alone.<sup class=""reference"" id=""cite_ref-133""><a href=""#cite_note-133"">[133]</a></sup>
</p>, <p>The issue of carrying out investigations where the cloud storage devices cannot be physically accessed has generated a number of changes to the way that digital evidence is located and collected.<sup class=""reference"" id=""cite_ref-134""><a href=""#cite_note-134"">[134]</a></sup> New process models have been developed to formalize collection.<sup class=""reference"" id=""cite_ref-135""><a href=""#cite_note-135"">[135]</a></sup>
</p>, <p>In some scenarios existing digital forensics tools can be employed to access cloud storage as networked drives (although this is a slow process generating a large amount of internet traffic).<sup class=""noprint Inline-Template Template-Fact"" style=""white-space:nowrap;"">[<i><a href=""/wiki/Wikipedia:Citation_needed"" title=""Wikipedia:Citation needed""><span title=""This claim needs references to reliable sources. (June 2018)"">citation needed</span></a></i>]</sup>
</p>, <p>An alternative approach is to deploy a tool that processes in the cloud itself.<sup class=""reference"" id=""cite_ref-136""><a href=""#cite_note-136"">[136]</a></sup>
</p>, <p>For organizations using Office 365 with an 'E5' subscription, there is the option to use Microsoft's built-in e-discovery resources, although these do not provide all the functionality that is typically required for a forensic process.<sup class=""reference"" id=""cite_ref-137""><a href=""#cite_note-137"">[137]</a></sup>
</p>]"
Amazon,"[<p><b>Amazon</b> most often refers to:
</p>, <p><b>Amazon</b> or <b>Amazone</b> may also refer to:
</p>, <p><span class=""anchor"" id=""Film""></span><span class=""anchor"" id=""Television""></span>
</p>]"
Laptop,"[<p class=""mw-empty-elt"">
</p>, <p>A <b>laptop</b>, <b>laptop computer</b>, or <b>notebook computer</b> is a small, portable <a href=""/wiki/Personal_computer"" title=""Personal computer"">personal computer</a> (PC) with a screen and <a class=""mw-redirect"" href=""/wiki/Alphanumeric_keyboard"" title=""Alphanumeric keyboard"">alphanumeric keyboard</a>. Laptops typically have a <a class=""mw-redirect"" href=""/wiki/Flip_(form)"" title=""Flip (form)"">clam shell</a> form factor with the <a class=""mw-redirect"" href=""/wiki/Computer_screen"" title=""Computer screen"">screen</a> mounted on the inside of the upper lid and the keyboard on the inside of the lower lid, although <a href=""/wiki/2-in-1_PC"" title=""2-in-1 PC"">2-in-1 PCs</a> with a detachable <a href=""/wiki/Computer_keyboard"" title=""Computer keyboard"">keyboard</a> are often marketed as laptops or as having a laptop mode. Laptops are folded shut for transportation, and thus are suitable for <a href=""/wiki/Mobile_computing"" title=""Mobile computing"">mobile use</a>.<sup class=""reference"" id=""cite_ref-Wikipedia_1-0""><a href=""#cite_note-Wikipedia-1"">[1]</a></sup> Its name comes from <a href=""/wiki/Lap"" title=""Lap"">lap</a>, as it was deemed practical to be placed on a person's lap when being used. Today, laptops are used in a variety of settings, such as at work, in education, for <a href=""/wiki/PC_game"" title=""PC game"">playing games</a>, <a href=""/wiki/Web_browser"" title=""Web browser"">web browsing</a>, for personal <a href=""/wiki/Multimedia"" title=""Multimedia"">multimedia</a>, and for general home computer use.
</p>, <p>As of 2021, in <a href=""/wiki/American_English"" title=""American English"">American English</a>, the terms <i>laptop computer</i> and <i>notebook computer</i> are used interchangeably;<sup class=""reference"" id=""cite_ref-2""><a href=""#cite_note-2"">[2]</a></sup> in other dialects of English, one or the other may be preferred. Although the terms <i>notebook computers</i> or <i>notebooks</i> originally referred to a specific size of laptop (originally smaller and lighter than mainstream laptops of the time),<sup class=""reference"" id=""cite_ref-Buzzle_3-0""><a href=""#cite_note-Buzzle-3"">[3]</a></sup> the terms have come to mean the same thing and <i>notebook</i> no longer refers to any specific size.
</p>, <p>Laptops combine all the <a href=""/wiki/Input/output"" title=""Input/output"">input/output</a> components and capabilities of a <a href=""/wiki/Desktop_computer"" title=""Desktop computer"">desktop computer</a>, including the display screen, <a href=""/wiki/Computer_speakers"" title=""Computer speakers"">small speakers</a>, a keyboard, <a href=""/wiki/Data_storage"" title=""Data storage"">data storage</a> device, sometimes an <a href=""/wiki/Optical_disc_drive"" title=""Optical disc drive"">optical disc drive</a>, pointing devices (such as a <a href=""/wiki/Touchpad"" title=""Touchpad"">touch pad</a> or <a href=""/wiki/Pointing_stick"" title=""Pointing stick"">pointing stick</a>), with an <a href=""/wiki/Operating_system"" title=""Operating system"">operating system</a>, a <a href=""/wiki/Processor_(computing)"" title=""Processor (computing)"">processor</a> and <a href=""/wiki/Computer_memory"" title=""Computer memory"">memory</a> into a single unit. Most modern laptops feature integrated <a href=""/wiki/Webcam"" title=""Webcam"">webcams</a> and built-in <a href=""/wiki/Microphone"" title=""Microphone"">microphones</a>, while many also have <a href=""/wiki/Touchscreen"" title=""Touchscreen"">touchscreens</a>. Laptops can be powered either from an internal <a href=""/wiki/Electric_battery"" title=""Electric battery"">battery</a> or by an external <a href=""/wiki/Mains_electricity"" title=""Mains electricity"">power supply</a> from an <a href=""/wiki/AC_adapter"" title=""AC adapter"">AC adapter</a>. Hardware specifications, such as the processor speed and memory capacity, significantly vary between different types, models and <a href=""/wiki/Price_point"" title=""Price point"">price points</a>.
</p>, <p>Design elements, form factor and construction can also vary significantly between models depending on the intended use. Examples of specialized models of laptops include <a href=""/wiki/Rugged_computer"" title=""Rugged computer"">rugged notebooks</a> for use in construction or <a class=""mw-redirect"" href=""/wiki/Military_applications"" title=""Military applications"">military applications</a>, as well as <a href=""/wiki/Cost_of_goods_sold"" title=""Cost of goods sold"">low production cost</a> laptops such as those from the <a href=""/wiki/One_Laptop_per_Child"" title=""One Laptop per Child"">One Laptop per Child</a> (OLPC) organization, which incorporate features like <a href=""/wiki/Solar_power"" title=""Solar power"">solar charging</a> and semi-flexible components not found on most laptop computers. <a href=""/wiki/Portable_computer"" title=""Portable computer"">Portable computers</a>, which later developed into modern laptops, were originally considered to be a small <a href=""/wiki/Niche_market"" title=""Niche market"">niche market</a>, mostly for specialized field applications, such as in the military, for accountants, or traveling sales representatives. As portable computers evolved into modern laptops, they became widely used for a variety of purposes.<sup class=""reference"" id=""cite_ref-NPD_4-0""><a href=""#cite_note-NPD-4"">[4]</a></sup>
</p>, <p>As the personal computer (PC) became feasible in 1971, the idea of a portable personal computer soon followed. A ""personal, portable information manipulator"" was imagined by <a href=""/wiki/Alan_Kay"" title=""Alan Kay"">Alan Kay</a> at <a class=""mw-redirect"" href=""/wiki/Xerox_PARC"" title=""Xerox PARC"">Xerox PARC</a> in 1968,<sup class=""reference"" id=""cite_ref-5""><a href=""#cite_note-5"">[5]</a></sup> and described in his 1972 paper as the ""<a href=""/wiki/Dynabook"" title=""Dynabook"">Dynabook</a>"".<sup class=""reference"" id=""cite_ref-6""><a href=""#cite_note-6"">[6]</a></sup> The IBM Special Computer APL Machine Portable (SCAMP) was demonstrated in 1973.<sup class=""reference"" id=""cite_ref-7""><a href=""#cite_note-7"">[7]</a></sup> This prototype was based on the <a href=""/wiki/IBM_PALM_processor"" title=""IBM PALM processor"">IBM PALM processor</a>.<sup class=""reference"" id=""cite_ref-8""><a href=""#cite_note-8"">[8]</a></sup> The <a href=""/wiki/IBM_5100"" title=""IBM 5100"">IBM 5100</a>, the first commercially available portable computer, appeared in September 1975, and was based on the SCAMP prototype.<sup class=""reference"" id=""cite_ref-9""><a href=""#cite_note-9"">[9]</a></sup>
</p>, <p>As 8-bit CPU machines became widely accepted, the number of portables increased rapidly. The first ""laptop-sized notebook computer"" was the <a href=""/wiki/Epson_HX-20"" title=""Epson HX-20"">Epson HX-20</a>,<sup class=""reference"" id=""cite_ref-10""><a href=""#cite_note-10"">[10]</a></sup><sup class=""reference"" id=""cite_ref-ipsj_11-0""><a href=""#cite_note-ipsj-11"">[11]</a></sup> invented (patented) by <a class=""mw-redirect"" href=""/wiki/Suwa_Seikosha"" title=""Suwa Seikosha"">Suwa Seikosha</a>'s Yukio Yokozawa in July 1980,<sup class=""reference"" id=""cite_ref-patent_12-0""><a href=""#cite_note-patent-12"">[12]</a></sup> introduced at the <a href=""/wiki/COMDEX"" title=""COMDEX"">COMDEX</a> computer show in <a href=""/wiki/Las_Vegas"" title=""Las Vegas"">Las Vegas</a> by Japanese company <a class=""mw-redirect"" href=""/wiki/Seiko_Epson"" title=""Seiko Epson"">Seiko Epson</a> in 1981,<sup class=""reference"" id=""cite_ref-hx20_13-0""><a href=""#cite_note-hx20-13"">[13]</a></sup><sup class=""reference"" id=""cite_ref-ipsj_11-1""><a href=""#cite_note-ipsj-11"">[11]</a></sup> and released in July 1982.<sup class=""reference"" id=""cite_ref-ipsj_11-2""><a href=""#cite_note-ipsj-11"">[11]</a></sup><sup class=""reference"" id=""cite_ref-peres_14-0""><a href=""#cite_note-peres-14"">[14]</a></sup> It had an <a class=""mw-redirect"" href=""/wiki/Liquid_crystal_display"" title=""Liquid crystal display"">LCD</a> screen, a rechargeable battery, and a calculator-size printer, in a 1.6 kg (3.5 lb) chassis, the size of an <a class=""mw-redirect"" href=""/wiki/A4_paper"" title=""A4 paper"">A4</a> <a href=""/wiki/Notebook"" title=""Notebook"">notebook</a>.<sup class=""reference"" id=""cite_ref-ipsj_11-3""><a href=""#cite_note-ipsj-11"">[11]</a></sup> It was described as a ""laptop"" and ""notebook"" computer in its patent.<sup class=""reference"" id=""cite_ref-patent_12-1""><a href=""#cite_note-patent-12"">[12]</a></sup>
</p>, <p>The portable micro computer <a href=""/wiki/Portal_(computer)"" title=""Portal (computer)"">Portal</a> of the French company R2E <a href=""/wiki/Micral"" title=""Micral"">Micral</a> CCMC officially appeared in September 1980 at the Sicob show in Paris. It was a portable microcomputer designed and marketed by the studies and developments department of R2E <a href=""/wiki/Micral"" title=""Micral"">Micral</a> at the request of the company CCMC specializing in payroll and accounting. It was based on an Intel 8085 processor, 8-bit, clocked at <span class=""nowrap""><span data-sort-value=""7006200000000000000♠""></span>2 MHz</span>. It was equipped with a central 64 KB RAM, a keyboard with 58 alphanumeric keys and 11 numeric keys (separate blocks), a 32-character screen, a floppy disk: capacity = 140,000 characters, of a thermal printer: speed = 28 characters / second, an asynchronous channel, asynchronous channel, a 220 V power supply. It weighed <span class=""nowrap""><span data-sort-value=""7001120000000000000♠""></span>12 kg</span> and its dimensions were 45<span class=""nowrap""> × </span>45<span class=""nowrap""> × </span>15<span class=""nowrap""> </span>cm. It provided total mobility. Its operating system was aptly named Prologue.
</p>, <p>The <a href=""/wiki/Osborne_1"" title=""Osborne 1"">Osborne 1</a>, released in 1981, was a luggable computer that used the <a href=""/wiki/Zilog"" title=""Zilog"">Zilog</a> <a class=""mw-redirect"" href=""/wiki/Z80"" title=""Z80"">Z80</a> and weighed 24.5 pounds (11.1 kg).<sup class=""reference"" id=""cite_ref-15""><a href=""#cite_note-15"">[15]</a></sup> It had no battery, a 5 in (13 cm) <a href=""/wiki/Cathode-ray_tube"" title=""Cathode-ray tube"">cathode-ray tube</a> (CRT) screen, and dual 5.25 in (13.3 cm) single-density floppy drives. Both <a href=""/wiki/RadioShack"" title=""RadioShack"">Tandy/RadioShack</a> and <a class=""mw-redirect"" href=""/wiki/Hewlett_Packard"" title=""Hewlett Packard"">Hewlett Packard</a> (HP) also produced portable computers of varying designs during this period.<sup class=""reference"" id=""cite_ref-16""><a href=""#cite_note-16"">[16]</a></sup><sup class=""reference"" id=""cite_ref-17""><a href=""#cite_note-17"">[17]</a></sup> The first laptops using the <a class=""mw-redirect"" href=""/wiki/Flip_(form)"" title=""Flip (form)"">flip form factor</a> appeared in the early 1980s. The <a href=""/wiki/Dulmont_Magnum"" title=""Dulmont Magnum"">Dulmont Magnum</a> was released in Australia in 1981–82, but was not marketed internationally until 1984–85. The <span style=""white-space: nowrap"">US$8,150</span> (equivalent to $22,880 in 2021) <a class=""mw-redirect"" href=""/wiki/GRiD_Compass"" title=""GRiD Compass"">GRiD Compass 1101</a>, released in 1982, was used at <a href=""/wiki/NASA"" title=""NASA"">NASA</a> and by the <a class=""mw-redirect"" href=""/wiki/United_States_military"" title=""United States military"">military</a>, among others. The <a href=""/wiki/Sharp_PC-5000"" title=""Sharp PC-5000"">Sharp PC-5000</a>,<sup class=""reference"" id=""cite_ref-18""><a href=""#cite_note-18"">[18]</a></sup> Ampere<sup class=""reference"" id=""cite_ref-ampere_19-0""><a href=""#cite_note-ampere-19"">[19]</a></sup> and <a href=""/wiki/Gavilan_SC"" title=""Gavilan SC"">Gavilan SC</a> released in 1983. The Gavilan SC was described as a ""laptop"" by its manufacturer,<sup class=""reference"" id=""cite_ref-20""><a href=""#cite_note-20"">[20]</a></sup> while the Ampere had a modern clamshell design.<sup class=""reference"" id=""cite_ref-ampere_19-1""><a href=""#cite_note-ampere-19"">[19]</a></sup><sup class=""reference"" id=""cite_ref-21""><a href=""#cite_note-21"">[21]</a></sup> The <a href=""/wiki/Toshiba_T1100"" title=""Toshiba T1100"">Toshiba T1100</a> won acceptance not only among PC experts but the mass market as a way to have PC portability.<sup class=""reference"" id=""cite_ref-22""><a href=""#cite_note-22"">[22]</a></sup>
</p>, <p>From 1983 onward, several new input techniques were developed and included in laptops, including the <a href=""/wiki/Touchpad"" title=""Touchpad"">touch pad</a> (<a href=""/wiki/Gavilan_SC"" title=""Gavilan SC"">Gavilan SC</a>, 1983), the <a class=""mw-redirect"" href=""/wiki/Trackpoint"" title=""Trackpoint"">pointing stick</a> (IBM <a class=""mw-redirect"" href=""/wiki/ThinkPad_700"" title=""ThinkPad 700"">ThinkPad 700</a>, 1992), and <a href=""/wiki/Handwriting_recognition"" title=""Handwriting recognition"">handwriting recognition</a> (Linus Write-Top,<sup class=""reference"" id=""cite_ref-23""><a href=""#cite_note-23"">[23]</a></sup> 1987). Some CPUs, such as the 1990 Intel <a class=""mw-redirect"" href=""/wiki/Intel_80386"" title=""Intel 80386"">i386SL</a>, were designed to use minimum power to increase battery life of portable computers and were supported by dynamic <a href=""/wiki/Power_management"" title=""Power management"">power management</a> features such as Intel <a href=""/wiki/SpeedStep"" title=""SpeedStep"">SpeedStep</a> and AMD <a href=""/wiki/PowerNow!"" title=""PowerNow!"">PowerNow!</a> in some designs.
</p>, <p>Displays reached 640x480 (<a class=""mw-redirect"" href=""/wiki/VGA"" title=""VGA"">VGA</a>) resolution by 1988 (Compaq SLT/286), and color screens started becoming a common upgrade in 1991,<sup class=""reference"" id=""cite_ref-24""><a href=""#cite_note-24"">[24]</a></sup> with increases in resolution and screen size occurring frequently until the introduction of 17"" screen laptops in 2003. Hard drives started to be used in portables, encouraged by the introduction of 3.5"" drives in the late 1980s, and became common in laptops starting with the introduction of 2.5"" and smaller drives around 1990; capacities have typically lagged behind physically larger desktop drives.
</p>, <p>Common resolutions of laptop <a href=""/wiki/Webcam"" title=""Webcam"">webcams</a> are 720p (HD), and in lower-end laptops 480p.<sup class=""reference"" id=""cite_ref-25""><a href=""#cite_note-25"">[25]</a></sup> The earliest known laptops with <a href=""/wiki/1080p"" title=""1080p"">1080p</a> (Full HD) webcams like the Samsung 700G7C were released in the early 2010s.<sup class=""reference"" id=""cite_ref-26""><a href=""#cite_note-26"">[26]</a></sup>
</p>, <p><a href=""/wiki/Optical_disc_drive"" title=""Optical disc drive"">Optical disc drives</a> became common in full-size laptops around 1997; this initially consisted of CD-ROM drives, which were supplanted by CD-R, DVD, and <a class=""mw-redirect"" href=""/wiki/Blu-ray_Disc"" title=""Blu-ray Disc"">Blu-ray</a> drives with writing capability over time. Starting around 2011, the trend shifted against internal optical drives, and as of 2021, they have largely disappeared; they are still readily available as external <a href=""/wiki/Peripheral"" title=""Peripheral"">peripherals</a>.
</p>, <p>While the terms <i>laptop</i> and <i>notebook</i> are used interchangeably today, there is some question as to the original etymology and specificity of either term. The term <i>laptop</i> appears to have been coined in the early 1980s to describe a mobile computer which could be used on one's lap and to distinguish these devices from earlier and much heavier <a class=""mw-redirect"" href=""/wiki/Portable_computers"" title=""Portable computers"">portable computers</a> (informally called ""luggables""). The term <i>notebook</i> appears to have gained currency somewhat later as manufacturers started producing even smaller portable devices, further reducing their weight and size and incorporating a display roughly the size of <a href=""/wiki/ISO_216#A_series"" title=""ISO 216"">A4</a> paper;<sup class=""reference"" id=""cite_ref-Buzzle_3-1""><a href=""#cite_note-Buzzle-3"">[3]</a></sup> these were marketed as <i>notebooks</i> to distinguish them from bulkier mainstream or <a href=""/wiki/Desktop_replacement_computer"" title=""Desktop replacement computer"">desktop replacement</a> laptops.
</p>, <p>Since the introduction of portable computers during the late 1970s, their form has changed significantly, <a class=""mw-redirect"" href=""/wiki/Spawning"" title=""Spawning"">spawning</a> a variety of visually and technologically differing subclasses. Except where there is a distinct legal trademark around a term (notably, <a href=""/wiki/Ultrabook"" title=""Ultrabook"">Ultrabook</a>), there are rarely hard distinctions between these classes and their usage has varied over time and between different sources. Since the late 2010s, the use of more specific terms has become less common, with sizes distinguished largely by the size of the screen.
</p>, <p>
There were in the past a number of marketing categories for smaller and larger laptop computers; these included ""<a href=""/wiki/Subnotebook"" title=""Subnotebook"">subnotebook</a>"" models, low cost ""<a href=""/wiki/Netbook"" title=""Netbook"">netbooks</a>"", and ""<a href=""/wiki/Ultra-mobile_PC"" title=""Ultra-mobile PC"">ultra-mobile PCs</a>"" where the size class overlapped with devices like <a href=""/wiki/Smartphone"" title=""Smartphone"">smartphone</a> and handheld <a href=""/wiki/Tablet_computer"" title=""Tablet computer"">tablets</a>, and ""<a href=""/wiki/Desktop_replacement_computer"" title=""Desktop replacement computer"">Desktop replacement</a>"" laptops for machines notably larger and heavier than typical to operate more powerful <a href=""/wiki/Central_processing_unit"" title=""Central processing unit"">processors</a> or <a href=""/wiki/Graphics_processing_unit"" title=""Graphics processing unit"">graphics hardware</a>.<sup class=""reference"" id=""cite_ref-types_cnet_27-0""><a href=""#cite_note-types_cnet-27"">[27]</a></sup> All of these terms have fallen out of favor as the size of mainstream laptops has gone down and their capabilities have gone up; except for niche models, laptop sizes tend to be distinguished by the size of the screen, and for more powerful models, by any specialized purpose the machine is intended for, such as a ""<a href=""/wiki/Gaming_computer"" title=""Gaming computer"">gaming laptop</a>"" or a ""<a href=""/wiki/Workstation"" title=""Workstation"">mobile workstation</a>"" for professional use.<link href=""mw-data:TemplateStyles:r1033289096"" rel=""mw-deduplicated-inline-style""/></p>, <p><span class=""anchor"" id=""Convertible""></span>
<span class=""anchor"" id=""Hybrid""></span>
</p>, <p>The latest trend of <a href=""/wiki/Technological_convergence"" title=""Technological convergence"">technological convergence</a> in the portable computer industry spawned a broad range of devices, which combined features of several previously separate device types. The <i>hybrids</i>, <i>convertibles</i>, and <i>2-in-1s</i> emerged as crossover devices, which share traits of both tablets and laptops. All such devices have a <a href=""/wiki/Touchscreen"" title=""Touchscreen"">touchscreen</a> display designed to allow users to work in a <i>tablet</i> mode, using either <a href=""/wiki/Multi-touch"" title=""Multi-touch"">multi-touch</a> gestures or a <a href=""/wiki/Stylus_(computing)"" title=""Stylus (computing)"">stylus</a>/<a href=""/wiki/Digital_pen"" title=""Digital pen"">digital pen</a>.
</p>, <p><a href=""/wiki/2-in-1_PC"" title=""2-in-1 PC"">Convertibles</a> are devices with the ability to conceal a hardware keyboard. Keyboards on such devices can be flipped, rotated, or slid behind the back of the chassis, thus transforming from a laptop into a tablet. <i>Hybrids</i> have a keyboard detachment mechanism, and due to this feature, all critical components are situated in the part with the display. <i>2-in-1s</i> can have a hybrid or a convertible form, often dubbed <i>2-in-1 detachable</i> and <i>2-in-1 convertibles</i> respectively, but are distinguished by the ability to run a desktop <a href=""/wiki/Operating_system"" title=""Operating system"">OS</a>, such as <a href=""/wiki/Windows_10"" title=""Windows 10"">Windows 10</a>. 2-in-1s are often marketed as <i>laptop replacement tablets</i>.<sup class=""reference"" id=""cite_ref-28""><a href=""#cite_note-28"">[28]</a></sup>
</p>, <p>2-in-1s are often very thin, around 10 millimetres (0.39 in), and light devices with a long battery life. 2-in-1s are distinguished from mainstream tablets as they feature an <a href=""/wiki/X86"" title=""X86"">x86</a>-architecture <a class=""mw-redirect"" href=""/wiki/CPU"" title=""CPU"">CPU</a> (typically a low- or <a href=""/wiki/Ultra-low-voltage_processor"" title=""Ultra-low-voltage processor"">ultra-low-voltage</a> model), such as the Intel <a href=""/wiki/Intel_Core"" title=""Intel Core"">Core i5</a>, run a full-featured desktop <a href=""/wiki/Operating_system"" title=""Operating system"">OS</a> like <a href=""/wiki/Windows_10"" title=""Windows 10"">Windows 10</a>, and have a number of typical laptop <a class=""mw-redirect"" href=""/wiki/I/O"" title=""I/O"">I/O</a> ports, such as <a class=""mw-redirect"" href=""/wiki/USB_3"" title=""USB 3"">USB 3</a> and <a href=""/wiki/Mini_DisplayPort"" title=""Mini DisplayPort"">Mini DisplayPort</a>.
</p>, <p>2-in-1s are designed to be used not only as a <a href=""/wiki/Media_consumption"" title=""Media consumption"">media consumption</a> device but also as valid desktop or laptop replacements, due to their ability to run <i>desktop</i> applications, such as <a href=""/wiki/Adobe_Photoshop"" title=""Adobe Photoshop"">Adobe Photoshop</a>. It is possible to connect multiple <a href=""/wiki/Peripheral"" title=""Peripheral"">peripheral</a> devices, such as a mouse, keyboard, and several external displays to a modern 2-in-1.
</p>, <p><a href=""/wiki/Microsoft_Surface"" title=""Microsoft Surface"">Microsoft Surface</a> Pro-series devices and <a href=""/wiki/Surface_Book"" title=""Surface Book"">Surface Book</a> are examples of modern 2-in-1 detachable, whereas <a href=""/wiki/Lenovo_Yoga"" title=""Lenovo Yoga"">Lenovo Yoga</a>-series computers are a variant of 2-in-1 convertibles. While the older <a class=""mw-redirect"" href=""/wiki/Surface_RT"" title=""Surface RT"">Surface RT</a> and <a href=""/wiki/Surface_2"" title=""Surface 2"">Surface 2</a> have the same chassis design as the Surface Pro, their use of <a class=""mw-redirect"" href=""/wiki/ARM_architecture"" title=""ARM architecture"">ARM processors</a> and <a href=""/wiki/Windows_RT"" title=""Windows RT"">Windows RT</a> do not classify them as 2-in-1s, but as hybrid tablets.<sup class=""reference"" id=""cite_ref-29""><a href=""#cite_note-29"">[29]</a></sup> Similarly, a number of hybrid laptops run a <a href=""/wiki/Mobile_operating_system"" title=""Mobile operating system"">mobile operating system</a>, such as <a href=""/wiki/Android_(operating_system)"" title=""Android (operating system)"">Android</a>. These include <a href=""/wiki/Asus_Transformer"" title=""Asus Transformer"">Asus's Transformer Pad</a> devices, examples of <a class=""mw-redirect"" href=""/wiki/Hybrid_tablet"" title=""Hybrid tablet"">hybrids</a> with a detachable keyboard design, which do not fall in the category of 2-in-1s.
</p>, <p>A rugged laptop is designed to reliably operate in harsh usage conditions such as strong vibrations, extreme temperatures, and wet or dusty environments. Rugged laptops are bulkier, heavier, and much more expensive than regular laptops,<sup class=""reference"" id=""cite_ref-30""><a href=""#cite_note-30"">[30]</a></sup> and thus are seldom seen in regular consumer use.
</p>, <p>The basic components of laptops function identically to their desktop counterparts. Traditionally they were <a href=""/wiki/Miniaturization"" title=""Miniaturization"">miniaturized</a> and adapted to mobile use, although desktop systems increasingly use the same smaller, lower-power parts which were originally developed for mobile use. The design restrictions on power, size, and <a href=""/wiki/Laptop_cooler"" title=""Laptop cooler"">cooling</a> of laptops limit the maximum performance of laptop parts compared to that of desktop components, although that difference has increasingly narrowed.<sup class=""reference"" id=""cite_ref-31""><a href=""#cite_note-31"">[31]</a></sup>
</p>, <p>In general, laptop components are not intended to be replaceable or upgradable by the end-user, except for components that can be detached; in the past, batteries and optical drives were commonly exchangeable. This restriction is one of the major differences between laptops and desktop computers, because the large ""tower"" cases used in desktop computers are designed so that new <a href=""/wiki/Motherboard"" title=""Motherboard"">motherboards</a>, <a class=""mw-redirect"" href=""/wiki/Hard_disk"" title=""Hard disk"">hard disks</a>, <a href=""/wiki/Sound_card"" title=""Sound card"">sound cards</a>, <a href=""/wiki/Random-access_memory"" title=""Random-access memory"">RAM</a>, and other components can be added. Memory and storage can often be upgraded with some disassembly, but with the most compact laptops, there may be no upgradeable components at all.<sup class=""reference"" id=""cite_ref-32""><a href=""#cite_note-32"">[32]</a></sup>
</p>, <p><a href=""/wiki/Intel"" title=""Intel"">Intel</a>, <a href=""/wiki/Asus"" title=""Asus"">Asus</a>, <a href=""/wiki/Compal_Electronics"" title=""Compal Electronics"">Compal</a>, <a href=""/wiki/Quanta_Computer"" title=""Quanta Computer"">Quanta</a>, and some other laptop manufacturers have created the <a href=""/wiki/Common_Building_Block"" title=""Common Building Block"">Common Building Block</a> standard for laptop parts to address some of the inefficiencies caused by the lack of standards and inability to upgrade components.<sup class=""reference"" id=""cite_ref-33""><a href=""#cite_note-33"">[33]</a></sup>
</p>, <p>The following sections summarizes the differences and distinguishing features of laptop components in comparison to desktop personal computer parts.<sup class=""reference"" id=""cite_ref-parts-kate_34-0""><a href=""#cite_note-parts-kate-34"">[34]</a></sup>
</p>, <p>Internally, a display is usually an LCD panel, although occasionally OLEDs are used. These interface to the laptop using the <a class=""mw-redirect"" href=""/wiki/LVDS"" title=""LVDS"">LVDS</a> or <a class=""mw-redirect"" href=""/wiki/Embedded_DisplayPort"" title=""Embedded DisplayPort"">embedded DisplayPort</a> protocol, while externally, it can be a glossy screen or a matte (anti-glare) screen. As of 2021, mainstream consumer laptops tend to come with either 13"" or 15""-16"" screens; 14"" models are more popular among business machines. Larger and smaller models are available, but less common – there is no clear dividing line in minimum or maximum size. Machines small enough to be handheld (screens in the 6–8"" range) can be marketed either as very small laptops or ""handheld PCs,"" while the distinction between the largest laptops and ""All-in-One"" desktops is whether they fold for travel.
</p>, <p>In the past, there was a broader range of marketing terms (both formal and informal) to distinguish between different sizes of laptops. These included <a href=""/wiki/Netbook"" title=""Netbook"">Netbooks</a>, <a href=""/wiki/Subnotebook"" title=""Subnotebook"">subnotebooks</a>, <a href=""/wiki/Ultra-mobile_PC"" title=""Ultra-mobile PC"">Ultra-mobile PC</a>, and <a href=""/wiki/Desktop_replacement_computer"" title=""Desktop replacement computer"">Desktop replacement computers</a>; these are sometimes still used informally, although they are essentially dead in terms of manufacturer marketing.
</p>, <p>Having a higher resolution display allows more items to fit onscreen at a time, improving the user's ability to multitask, although at the higher resolutions on smaller screens, the resolution may only serve to display sharper graphics and text rather than increasing the usable area. Since the introduction of the <a class=""mw-redirect"" href=""/wiki/MacBook_Pro_with_Retina_display"" title=""MacBook Pro with Retina display"">MacBook Pro with Retina display</a> in 2012, there have been an increase in the availability of ""HiDPI"" (or high <a href=""/wiki/Pixel_density"" title=""Pixel density"">Pixel density</a>) displays; as of 2021, this is generally considered to be anything higher than 1920 pixels wide. This has increasingly converged around 4K (3840-pixel-wide) resolutions.
</p>, <p>External displays can be connected to most laptops, and models with a <a href=""/wiki/Mini_DisplayPort"" title=""Mini DisplayPort"">Mini DisplayPort</a> can handle up to three.<sup class=""reference"" id=""cite_ref-3-Displays_FAQ_35-0""><a href=""#cite_note-3-Displays_FAQ-35"">[35]</a></sup>
</p>, <p>The earliest laptops known to feature a display with doubled 120 Hz of <a href=""/wiki/Refresh_rate"" title=""Refresh rate"">refresh rate</a> and <a href=""/wiki/Active_shutter_3D_system"" title=""Active shutter 3D system"">active shutter 3D system</a> were released in 2011 by <a href=""/wiki/Dell"" title=""Dell"">Dell</a> (M17x) and <a href=""/wiki/Samsung"" title=""Samsung"">Samsung</a> (700G7A).<sup class=""reference"" id=""cite_ref-36""><a href=""#cite_note-36"">[36]</a></sup><sup class=""reference"" id=""cite_ref-37""><a href=""#cite_note-37"">[37]</a></sup>
</p>, <p>A laptop's <a href=""/wiki/Central_processing_unit"" title=""Central processing unit"">central processing unit</a> (CPU) has advanced power-saving features and produces less heat than one intended purely for desktop use. Mainstream laptop CPUs made after 2018 have four processor cores, although some inexpensive models still have 2-core CPUs, and 6-core and 8-core models are also available.
</p>, <p>For the low price and mainstream performance, there is no longer a significant performance difference between laptop and desktop CPUs, but at the high end, the fastest desktop CPUs still substantially outperform the fastest laptop processors, at the expense of massively higher power consumption and heat generation; the fastest laptop processors top out at 56 watts of heat, while the fastest desktop processors top out at 150 watts.
</p>, <p>There has been a wide range of <a class=""mw-redirect"" href=""/wiki/Notebook_processors"" title=""Notebook processors"">CPUs designed for laptops</a> available from both <a href=""/wiki/Intel"" title=""Intel"">Intel</a>, <a class=""mw-redirect"" href=""/wiki/AMD"" title=""AMD"">AMD</a>, and other manufacturers. On non-<a href=""/wiki/X86"" title=""X86"">x86</a> architectures, Motorola and IBM produced the chips for the former <a href=""/wiki/PowerPC"" title=""PowerPC"">PowerPC</a>-based Apple laptops (<a href=""/wiki/IBook"" title=""IBook"">iBook</a> and <a href=""/wiki/PowerBook"" title=""PowerBook"">PowerBook</a>). Between around 2000 to 2014, most full-size laptops had socketed, replaceable CPUs; on thinner models, the CPU was soldered on the motherboard and was not replaceable or upgradable without replacing the motherboard. Since 2015, Intel has not offered new laptop CPU models with pins to be interchangeable, preferring <a href=""/wiki/Ball_grid_array"" title=""Ball grid array"">ball grid array</a> chip packages which have to be soldered;<sup class=""reference"" id=""cite_ref-38""><a href=""#cite_note-38"">[38]</a></sup>and as of 2021, only a few rare models using desktop parts.
</p>, <p>In the past, some laptops have used a desktop processor instead of the laptop version and have had high-performance gains at the cost of greater weight, heat, and limited battery life; this is not unknown as of 2021, but since around 2010, the practice has been restricted to small-volume gaming models. Laptop CPUs are rarely able to be <a href=""/wiki/Overclocking"" title=""Overclocking"">overclocked</a>; most use locked processors. Even on gaming models where unlocked processors are available, the cooling system in most laptops is often very close to its limits and there is rarely headroom for an overclocking–related operating temperature increase.
</p>, <p>On most laptops, a <a class=""mw-redirect"" href=""/wiki/Graphical_processing_unit"" title=""Graphical processing unit"">graphical processing unit</a> (GPU) is integrated into the CPU to conserve power and space. This was introduced by Intel with the <a href=""/wiki/Intel_Core"" title=""Intel Core"">Core i-series</a> of mobile processors in 2010, and similar <a class=""mw-redirect"" href=""/wiki/Accelerated_processing_unit"" title=""Accelerated processing unit"">accelerated processing unit</a> (APU) processors by AMD later that year.
</p>, <p>Before that, lower-end machines tended to use graphics processors integrated into the <a href=""/wiki/Chipset#Computers"" title=""Chipset"">system chipset</a>, while higher-end machines had a separate graphics processor. In the past, laptops lacking a separate graphics processor were limited in their utility for gaming and professional applications involving <a href=""/wiki/3D_computer_graphics"" title=""3D computer graphics"">3D</a> graphics, but the capabilities of CPU-integrated graphics have converged with the low-end of dedicated graphics processors since the mid-2010s.
</p>, <p>Higher-end laptops intended for gaming or professional 3D work still come with dedicated and in some cases even dual, graphics processors on the motherboard or as an internal expansion card. Since 2011, these almost always involve switchable graphics so that when there is no demand for the higher performance dedicated graphics processor, the more power-efficient integrated graphics processor will be used. <a href=""/wiki/Nvidia_Optimus"" title=""Nvidia Optimus"">Nvidia Optimus</a> and <a href=""/wiki/AMD_Hybrid_Graphics"" title=""AMD Hybrid Graphics"">AMD Hybrid Graphics</a> are examples of this sort of system of switchable graphics.
</p>, <p>Since around the year 2000, most laptops have used <a class=""mw-redirect"" href=""/wiki/SO-DIMM"" title=""SO-DIMM"">SO-DIMM</a> <a href=""/wiki/Random-access_memory"" title=""Random-access memory"">RAM</a>,<sup class=""reference"" id=""cite_ref-parts-kate_34-1""><a href=""#cite_note-parts-kate-34"">[34]</a></sup> although, as of 2021, an increasing number of models use memory soldered to the motherboard. Before 2000, most laptops used proprietary memory modules if their memory was upgradable.
</p>, <p>In the early 2010s, high end laptops such as the 2011 Samsung <i>700G7A</i> have passed the 10 GB RAM barrier, featuring 16 GB of RAM.<sup class=""reference"" id=""cite_ref-39""><a href=""#cite_note-39"">[39]</a></sup>
</p>, <p>When upgradeable, memory slots are sometimes accessible from the bottom of the laptop for ease of upgrading; in other cases, accessing them requires significant disassembly. Most laptops have two memory slots, although some will have only one, either for cost savings or because some amount of memory is soldered. Some high-end models have four slots; these are usually mobile engineering workstations, although a few high-end models intended for gaming do as well.
</p>, <p>As of 2021, 8 GB RAM is most common, with lower-end models occasionally having 4GB. Higher-end laptops may come with 16 GB of RAM or more.
</p>, <p>The earliest laptops most often used <a href=""/wiki/Floppy_disk"" title=""Floppy disk"">floppy disk</a> for storage, although a few used either RAM disk or tape, by the late 1980s <a href=""/wiki/Hard_disk_drive"" title=""Hard disk drive"">hard disk drives</a> had become the standard form of storage.
</p>, <p>Between 1990 and 2009, almost all laptops typically had a <a href=""/wiki/Hard_disk_drive"" title=""Hard disk drive"">hard disk drive</a> (HDD) for storage; since then, <a href=""/wiki/Solid-state_drive"" title=""Solid-state drive"">solid-state drives</a> (SSD) have gradually come to supplant hard drives in all but some inexpensive consumer models. Solid-state drives are faster and more power-efficient, as well as eliminating the hazard of drive and data corruption caused by a laptop's physical impacts, as they use no mechanical parts such as a rotational platter.<sup class=""reference"" id=""cite_ref-PCW-SSD_40-0""><a href=""#cite_note-PCW-SSD-40"">[40]</a></sup> In many cases, they are more compact as well. Initially, in the late 2000s, SSDs were substantially more expensive than HDDs, but as of 2021 prices on smaller capacity (under 1 <a class=""mw-redirect"" href=""/wiki/Terabyte"" title=""Terabyte"">terabyte</a>) drives have converged; larger capacity drives remain more expensive than comparable-sized HDDs.
</p>, <p>Since around 1990, where a hard drive is present it will typically be a 2.5-inch drive; some very compact laptops support even smaller 1.8-inch HDDs, and a very small number used 1"" <a href=""/wiki/Microdrive"" title=""Microdrive"">Microdrives</a>. Some SSDs are built to match the size/shape of a laptop hard drive, but increasingly they have been replaced with smaller <a class=""mw-redirect"" href=""/wiki/MSATA"" title=""MSATA"">mSATA</a> or <a href=""/wiki/M.2"" title=""M.2"">M.2</a> cards. SSDs using the newer and much faster <a href=""/wiki/NVM_Express"" title=""NVM Express"">NVM Express</a> standard for connecting are only available as cards.
</p>, <p>As of 2021, many laptops no longer contain space for a 2.5"" drive, accepting only M.2 cards; a few of the smallest have storage soldered to the motherboard. For those that can, they can typically contain a single 2.5-inch drive, but a small number of laptops with a screen wider than 15 inches can house two drives.
</p>, <p>A variety of <a href=""/wiki/Hard_disk_drive#External_hard_disk_drives"" title=""Hard disk drive"">external HDDs</a> or <a href=""/wiki/Network-attached_storage"" title=""Network-attached storage"">NAS</a> data storage servers with support of <a href=""/wiki/RAID"" title=""RAID"">RAID</a> technology can be attached to virtually any laptop over such interfaces as <a href=""/wiki/USB"" title=""USB"">USB</a>, <a class=""mw-redirect"" href=""/wiki/FireWire"" title=""FireWire"">FireWire</a>, <a class=""mw-redirect"" href=""/wiki/ESATA"" title=""ESATA"">eSATA</a>, or <a href=""/wiki/Thunderbolt_(interface)"" title=""Thunderbolt (interface)"">Thunderbolt</a>, or over a wired or wireless network to further increase space for the storage of data. Many laptops also incorporate a <a href=""/wiki/Card_reader"" title=""Card reader"">card reader</a> which allows for use of <a href=""/wiki/Memory_card"" title=""Memory card"">memory cards</a>, such as those used for <a href=""/wiki/Digital_camera"" title=""Digital camera"">digital cameras</a>, which are typically <a class=""mw-redirect"" href=""/wiki/Secure_Digital"" title=""Secure Digital"">SD</a> or <a class=""mw-redirect"" href=""/wiki/MicroSD"" title=""MicroSD"">microSD</a> cards. This enables users to download digital pictures from an SD card onto a laptop, thus enabling them to delete the SD card's contents to free up space for taking new pictures.
</p>, <p><a href=""/wiki/Optical_disc_drive"" title=""Optical disc drive"">Optical disc drives</a> capable of playing <a href=""/wiki/CD-ROM"" title=""CD-ROM"">CD-ROMs</a>, compact discs (CD), <a href=""/wiki/DVD#DVD_drives_and_players"" title=""DVD"">DVDs</a>, and in some cases, <a href=""/wiki/Blu-ray"" title=""Blu-ray"">Blu-ray discs</a> (BD), were nearly universal on full-sized models between the mid-1990s and the early 2010s. As of 2021, drives are uncommon in compact or premium laptops; they remain available in some bulkier models, but the trend towards thinner and lighter machines is gradually eliminating these drives and players – when needed they can be connected via USB instead.
</p>, <p>An alphanumeric keyboard is used to enter text, data, and other commands (e.g., <a href=""/wiki/Function_key"" title=""Function key"">function keys</a>). A <a href=""/wiki/Touchpad"" title=""Touchpad"">touchpad</a> (also called a trackpad), a <a href=""/wiki/Pointing_stick"" title=""Pointing stick"">pointing stick</a>, or both, are used to control the position of the cursor on the screen, and an integrated keyboard<sup class=""reference"" id=""cite_ref-41""><a href=""#cite_note-41"">[41]</a></sup> is used for typing. Some touchpads have buttons separate from the touch surface, while others share the surface. A quick double-tap is typically registered as a click, and operating systems may recognize multi-finger touch gestures.
</p>, <p>An external keyboard and mouse may be connected using a <a href=""/wiki/USB"" title=""USB"">USB</a> port or wirelessly, via <a href=""/wiki/Bluetooth"" title=""Bluetooth"">Bluetooth</a> or similar technology. Some laptops have <a class=""mw-redirect"" href=""/wiki/Multitouch"" title=""Multitouch"">multitouch</a> <a href=""/wiki/Touchscreen"" title=""Touchscreen"">touchscreen</a> displays, either available as an option or standard. Most laptops have <a href=""/wiki/Webcam"" title=""Webcam"">webcams</a> and <a href=""/wiki/Microphone"" title=""Microphone"">microphones</a>, which can be used to communicate with other people with both moving images and sound, via <a href=""/wiki/Web_conferencing"" title=""Web conferencing"">web conferencing</a> or <a href=""/wiki/Videotelephony"" title=""Videotelephony"">video-calling</a> software.
</p>, <p>Laptops typically have USB ports and a combined headphone/microphone jack, for use with headphones, a combined headset, or an external mic. Many laptops have a <a href=""/wiki/Card_reader"" title=""Card reader"">card reader</a> for reading digital camera SD cards.
</p>, <p>On a typical laptop there are several <a href=""/wiki/USB"" title=""USB"">USB</a> <a href=""/wiki/Computer_port_(hardware)"" title=""Computer port (hardware)"">ports</a>; if they use only the older USB connectors instead of USB-C, they will typically have an external monitor port (<a class=""mw-redirect"" href=""/wiki/VGA"" title=""VGA"">VGA</a>, <a href=""/wiki/Digital_Visual_Interface"" title=""Digital Visual Interface"">DVI</a>, <a href=""/wiki/HDMI"" title=""HDMI"">HDMI</a> or <a href=""/wiki/Mini_DisplayPort"" title=""Mini DisplayPort"">Mini DisplayPort</a> or occasionally more than one), an audio in/out port (often in form of a single socket) is common. It is possible to connect up to three external displays to a 2014-era laptop via a single Mini DisplayPort, using <a href=""/wiki/DisplayPort#Multi-Stream_Transport"" title=""DisplayPort"">multi-stream transport</a> technology.<sup class=""reference"" id=""cite_ref-3-Displays_FAQ_35-1""><a href=""#cite_note-3-Displays_FAQ-35"">[35]</a></sup>
</p>, <p><a href=""/wiki/Apple_Inc."" title=""Apple Inc."">Apple</a>, in a 2015 version of its <a href=""/wiki/MacBook"" title=""MacBook"">MacBook</a>, transitioned from a number of different <a class=""mw-redirect"" href=""/wiki/I/O"" title=""I/O"">I/O</a> ports to a single <a href=""/wiki/USB-C"" title=""USB-C"">USB-C</a> port.<sup class=""reference"" id=""cite_ref-MacBook_2015_42-0""><a href=""#cite_note-MacBook_2015-42"">[42]</a></sup> This port can be used both for charging and connecting a variety of devices through the use of <a href=""/wiki/Aftermarket_(merchandise)"" title=""Aftermarket (merchandise)"">aftermarket</a> adapters. Google, with its updated version of <a href=""/wiki/Chromebook_Pixel"" title=""Chromebook Pixel"">Chromebook Pixel</a>, shows a similar transition trend towards USB-C, although keeping older USB Type-A ports for a better compatibility with older devices.<sup class=""reference"" id=""cite_ref-ChromebookPixel_2015_43-0""><a href=""#cite_note-ChromebookPixel_2015-43"">[43]</a></sup> Although being common until the end of the 2000s decade, <a href=""/wiki/Ethernet"" title=""Ethernet"">Ethernet</a> network port are rarely found on modern laptops, due to widespread use of <a href=""/wiki/Wireless_network"" title=""Wireless network"">wireless networking</a>, such as <a href=""/wiki/Wi-Fi"" title=""Wi-Fi"">Wi-Fi</a>. <a href=""/wiki/Legacy_port"" title=""Legacy port"">Legacy ports</a> such as a <a class=""mw-redirect"" href=""/wiki/PS/2_connector"" title=""PS/2 connector"">PS/2</a> keyboard/mouse port, <a href=""/wiki/Serial_port"" title=""Serial port"">serial port</a>, <a href=""/wiki/Parallel_port"" title=""Parallel port"">parallel port</a>, or <a class=""mw-redirect"" href=""/wiki/FireWire"" title=""FireWire"">FireWire</a> are provided on some models, but they are increasingly rare. On <a href=""/wiki/Apple_Inc."" title=""Apple Inc."">Apple</a>'s systems, and on a handful of other laptops, there are also <a href=""/wiki/Thunderbolt_(interface)"" title=""Thunderbolt (interface)"">Thunderbolt</a> ports, but <a class=""mw-redirect"" href=""/wiki/Thunderbolt_3"" title=""Thunderbolt 3"">Thunderbolt 3</a> uses USB-C. Laptops typically have a headphone jack, so that the user can connect external headphones or amplified speaker systems for listening to music or other audio.
</p>, <p>In the past, a <a href=""/wiki/PC_Card"" title=""PC Card"">PC Card</a> (formerly <a class=""mw-redirect"" href=""/wiki/PCMCIA"" title=""PCMCIA"">PCMCIA</a>) or <a href=""/wiki/ExpressCard"" title=""ExpressCard"">ExpressCard</a> slot for expansion was often present on laptops to allow adding and removing functionality, even when the laptop is powered on; these are becoming increasingly rare since the introduction of <a href=""/wiki/USB_3.0"" title=""USB 3.0"">USB 3.0</a>. Some internal subsystems such as Ethernet, Wi-Fi, or a wireless cellular modem can be implemented as replaceable internal expansion cards, usually accessible under an access cover on the bottom of the laptop. The standard for such cards is <a href=""/wiki/PCI_Express"" title=""PCI Express"">PCI Express</a>, which comes in both <a class=""mw-redirect"" href=""/wiki/PCI_Express_Mini_Card"" title=""PCI Express Mini Card"">mini</a> and even smaller <a href=""/wiki/M.2"" title=""M.2"">M.2</a> sizes. In newer laptops, it is not uncommon to also see <a class=""mw-redirect"" href=""/wiki/Micro_SATA#mSATA"" title=""Micro SATA"">Micro SATA</a> (mSATA) functionality on PCI Express Mini or M.2 card slots allowing the use of those slots for SATA-based solid-state drives.<sup class=""reference"" id=""cite_ref-parts-cards_44-0""><a href=""#cite_note-parts-cards-44"">[44]</a></sup>
</p>, <p>Since the late 1990s, laptops have typically used <a class=""mw-redirect"" href=""/wiki/Lithium_ion"" title=""Lithium ion"">lithium ion</a> or <a class=""mw-redirect"" href=""/wiki/Lithium_polymer"" title=""Lithium polymer"">lithium polymer</a> <a class=""mw-redirect"" href=""/wiki/Battery_(electricity)"" title=""Battery (electricity)"">batteries</a>, These replaced the older <a class=""mw-redirect"" href=""/wiki/Nickel_metal-hydride"" title=""Nickel metal-hydride"">nickel metal-hydride</a> typically used in the 1990s, and <a href=""/wiki/Nickel%E2%80%93cadmium_battery"" title=""Nickel–cadmium battery"">nickel–cadmium batteries</a> used in most of the earliest laptops. A few of the oldest laptops used <a href=""/wiki/Primary_cell"" title=""Primary cell"">non-rechargeable batteries</a>, or <a href=""/wiki/Lead%E2%80%93acid_battery"" title=""Lead–acid battery"">lead–acid batteries.</a>
</p>, <p>Battery life is highly variable by model and workload and can range from one hour to nearly a day. A battery's performance gradually decreases over time; a substantial reduction in capacity is typically evident after one to three years of regular use, depending on the charging and discharging pattern and the design of the battery. Innovations in laptops and batteries have seen situations in which the battery can provide up to 24 hours of continued operation, assuming average power consumption levels. An example is the HP EliteBook 6930p when used with its ultra-capacity battery.<sup class=""reference"" id=""cite_ref-45""><a href=""#cite_note-45"">[45]</a></sup>
</p>, <p>Laptops with removable batteries may support larger replacement batteries with extended capacity.
</p>, <p>A laptop's battery is charged using an external <a href=""/wiki/Power_supply"" title=""Power supply"">power supply</a>, which is plugged into a wall outlet. The power supply outputs a DC voltage typically in the range of 7.2—24 volts. The power supply is usually external and connected to the laptop through a DC connector cable. In most cases, it can charge the battery and power the laptop simultaneously. When the battery is fully charged, the laptop continues to run on power supplied by the external power supply, avoiding battery use. If the used power supply is not strong enough to power computing components and charge the battery simultaneously, the battery may charge in a shorter period of time if the laptop is turned off or sleeping. The charger typically adds about 400 grams (0.88 lb) to the overall transporting weight of a laptop, although some models are substantially heavier or lighter. Most 2016-era laptops use a <a href=""/wiki/Smart_battery"" title=""Smart battery"">smart battery</a>, a rechargeable <a href=""/wiki/Battery_pack"" title=""Battery pack"">battery pack</a> with a built-in <a href=""/wiki/Battery_management_system"" title=""Battery management system"">battery management system</a> (BMS). The smart battery can internally measure voltage and current, and deduce charge level and State of Health (SoH) parameters, indicating the state of the cells.<sup class=""noprint Inline-Template Template-Fact"" style=""white-space:nowrap;"">[<i><a href=""/wiki/Wikipedia:Citation_needed"" title=""Wikipedia:Citation needed""><span title=""This claim needs references to reliable sources. (July 2016)"">citation needed</span></a></i>]</sup>
</p>, <p>Historically, <a class=""mw-redirect"" href=""/wiki/DC_connectors"" title=""DC connectors"">DC connectors</a>, typically cylindrical/barrel-shaped <a href=""/wiki/Coaxial_power_connector"" title=""Coaxial power connector"">coaxial power connectors</a> have been used in laptops. Some vendors such as <a href=""/wiki/Lenovo"" title=""Lenovo"">Lenovo</a> made intermittent use of a rectangular connector.
</p>, <p>Some connector heads feature a center pin to allow the end device to determine the power supply type by measuring the resistance between it and the connector's negative pole (outer surface). Vendors may block charging if a power supply is not recognized as original part, which could deny the legitimate use of universal third-party chargers.<sup class=""reference"" id=""cite_ref-46""><a href=""#cite_note-46"">[46]</a></sup>
</p>, <p>With the advent of <a href=""/wiki/USB-C"" title=""USB-C"">USB-C</a>, <a class=""mw-redirect"" href=""/wiki/Portable_electronics"" title=""Portable electronics"">portable electronics</a> made increasing use of it for both <a class=""mw-redirect"" href=""/wiki/USB_Power_Delivery"" title=""USB Power Delivery"">power delivery</a> and data transfer. Its support for 20 V (common laptop power supply voltage) and 5 A typically suffices for low to mid-end laptops, but some with higher power demands such as <a class=""mw-redirect"" href=""/wiki/Gaming_laptop"" title=""Gaming laptop"">gaming laptops</a> depend on dedicated DC connectors to handle currents beyond 5 A without risking overheating, some even above 10 A. Additionally, dedicated DC connectors are more durable and less prone to wear and tear from frequent reconnection, as their design is less delicate.<sup class=""reference"" id=""cite_ref-47""><a href=""#cite_note-47"">[47]</a></sup>
</p>, <p><a href=""/wiki/Waste_heat"" title=""Waste heat"">Waste heat</a> from the operation is difficult to remove in the compact internal space of a laptop. The earliest laptops used passive cooling; this gave way to <a href=""/wiki/Heat_sink"" title=""Heat sink"">heat sinks</a> placed directly on the components to be cooled, but when these hot components are deep inside the device, a large space-wasting air duct is needed to exhaust the heat. Modern laptops instead rely on <a href=""/wiki/Heat_pipe"" title=""Heat pipe"">heat pipes</a> to rapidly move waste heat towards the edges of the device, to allow for a much smaller and compact fan and heat sink cooling system. Waste heat is usually exhausted away from the device operator towards the rear or sides of the device. Multiple air intake paths are used since some intakes can be blocked, such as when the device is placed on a soft conforming surface like a chair cushion. Secondary device temperature monitoring may reduce performance or trigger an emergency shutdown if it is unable to dissipate heat, such as if the laptop were to be left running and placed inside a carrying case. Aftermarket cooling pads with external fans can be used with laptops to reduce operating temperatures.
</p>, <p>A <a href=""/wiki/Docking_station"" title=""Docking station"">docking station</a> (sometimes referred to simply as a <i>dock</i>) is a laptop accessory that contains multiple ports and in some cases expansion slots or bays for fixed or removable drives. A laptop connects and disconnects to a docking station, typically through a single large proprietary connector. A docking station is an especially popular laptop accessory in a corporate computing environment, due to a possibility of a docking station transforming a laptop into a full-featured desktop replacement, yet allowing for its easy release. This ability can be advantageous to ""<a href=""/wiki/Road_warrior_(computing)"" title=""Road warrior (computing)"">road warrior</a>"" employees who have to travel frequently for work, and yet who also come into the office. If more ports are needed, or their position on a laptop is inconvenient, one can use a cheaper passive device known as a <a href=""/wiki/Docking_station#Port_replicator"" title=""Docking station"">port replicator</a>. These devices mate to the connectors on the laptop, such as through <a href=""/wiki/USB"" title=""USB"">USB</a> or <a href=""/wiki/IEEE_1394"" title=""IEEE 1394"">FireWire</a>.
</p>, <p><a href=""/wiki/Laptop_charging_trolley"" title=""Laptop charging trolley"">Laptop charging trolleys</a>, also known as laptop trolleys or laptop carts, are mobile storage containers to charge multiple <a class=""mw-redirect"" href=""/wiki/Laptops"" title=""Laptops"">laptops</a>, <a class=""mw-redirect"" href=""/wiki/Netbooks"" title=""Netbooks"">netbooks</a>, and <a href=""/wiki/Tablet_computer"" title=""Tablet computer"">tablet computers</a> at the same time. The trolleys are used in schools that have replaced their traditional static computer labs<sup class=""reference"" id=""cite_ref-doug_48-0""><a href=""#cite_note-doug-48"">[48]</a></sup> suites of desktop equipped with ""tower"" computers, but do not have enough plug sockets in an individual classroom to charge all of the devices. The trolleys can be wheeled between rooms and <a class=""mw-redirect"" href=""/wiki/Classrooms"" title=""Classrooms"">classrooms</a> so that all students and teachers in a particular building can access fully <a href=""/wiki/Electric_charge"" title=""Electric charge"">charged</a> <a href=""/wiki/Information_technology"" title=""Information technology"">IT</a> equipment.<sup class=""reference"" id=""cite_ref-49""><a href=""#cite_note-49"">[49]</a></sup>
</p>, <p>Laptop charging trolleys are also used to deter and protect against opportunistic and organized theft. Schools, especially those with <a href=""/wiki/Open_plan"" title=""Open plan"">open plan</a> designs, are often prime targets for <a class=""mw-redirect"" href=""/wiki/Thieves"" title=""Thieves"">thieves</a> who steal high-value items. Laptops, netbooks, and tablets are among the highest–value portable items in a school. Moreover, laptops can easily be concealed under clothing and stolen from buildings. Many types of laptop–charging trolleys are designed and constructed to protect against theft. They are generally made out of steel, and the laptops remain locked up while not in use. Although the trolleys can be moved between areas from one classroom to another, they can often be mounted or locked to the floor, support pillars, or walls to prevent thieves from stealing the laptops, especially overnight.<sup class=""reference"" id=""cite_ref-doug_48-1""><a href=""#cite_note-doug-48"">[48]</a></sup>
</p>, <p>In some laptops, solar panels are able to generate enough solar power for the laptop to operate.<sup class=""reference"" id=""cite_ref-50""><a href=""#cite_note-50"">[50]</a></sup> The <a class=""mw-redirect"" href=""/wiki/One_Laptop_Per_Child"" title=""One Laptop Per Child"">One Laptop Per Child</a> Initiative released the <a class=""mw-redirect"" href=""/wiki/OLPC_XO-1"" title=""OLPC XO-1"">OLPC XO-1</a> laptop which was tested and successfully operated by use of solar panels.<sup class=""reference"" id=""cite_ref-51""><a href=""#cite_note-51"">[51]</a></sup> Presently, they are designing an <a href=""/wiki/OLPC_XO-3"" title=""OLPC XO-3"">OLPC XO-3</a> laptop with these features. The OLPC XO-3 can operate with 2 watts of electricity because its renewable energy resources generate a total of 4 watts.<sup class=""reference"" id=""cite_ref-52""><a href=""#cite_note-52"">[52]</a></sup><sup class=""reference"" id=""cite_ref-53""><a href=""#cite_note-53"">[53]</a></sup> <a href=""/wiki/Samsung"" title=""Samsung"">Samsung</a> has also designed the NC215S solar–powered notebook that will be sold commercially in the U.S. market.<sup class=""reference"" id=""cite_ref-54""><a href=""#cite_note-54"">[54]</a></sup>
</p>, <p>A common accessory for laptops is a laptop sleeve, laptop skin, or laptop case, which provides a degree of protection from scratches. Sleeves, which are distinguished by being relatively thin and flexible, are most commonly made of <a href=""/wiki/Neoprene"" title=""Neoprene"">neoprene</a>, with sturdier ones made of <a href=""/wiki/LRPu"" title=""LRPu"">low-resilience polyurethane</a>. Some laptop sleeves are wrapped in <a href=""/wiki/Ballistic_nylon"" title=""Ballistic nylon"">ballistic nylon</a> to provide some measure of <a href=""/wiki/Waterproofing"" title=""Waterproofing"">waterproofing</a>. Bulkier and sturdier cases can be made of metal with polyurethane padding inside and may have locks for added security. Metal, padded cases also offer protection against impacts and drops. Another common accessory is a <a href=""/wiki/Laptop_cooler"" title=""Laptop cooler"">laptop cooler</a>, a device that helps lower the internal temperature of the laptop either actively or passively. A common active method involves using electric fans to draw heat away from the laptop, while a passive method might involve propping the laptop up on some type of pad so it can receive more airflow. Some stores sell laptop pads that enable a reclining person on a bed to use a laptop.
</p>, <p>Some of the components of earlier models of laptops can easily be replaced without opening completely its bottom part, such as keyboard, battery, hard disk, memory modules, CPU cooling fan, etc.
</p>, <p>Some of the components of recent models of laptops reside inside. Replacing most of its components, such as keyboard, battery, hard disk, memory modules, CPU cooling fan, etc., requires removal of its either top or bottom part, removal of the motherboard, and returning them.
</p>, <p>In some types, solder and glue are used to mount components such as RAM, storage, and batteries, making repairs additionally difficult.<sup class=""reference"" id=""cite_ref-55""><a href=""#cite_note-55"">[55]</a></sup><sup class=""reference"" id=""cite_ref-56""><a href=""#cite_note-56"">[56]</a></sup>
</p>, <p>Features that certain early models of laptops used to have that are not available in most current laptops include:
</p>, <p><b>Portability</b> is usually the first feature mentioned in any comparison of laptops versus desktop PCs.<sup class=""reference"" id=""cite_ref-58""><a href=""#cite_note-58"">[57]</a></sup> Physical portability allows a laptop to be used in many places—not only at home and the office but also during commuting and flights, in coffee shops, in lecture halls and libraries, at clients' locations or a meeting room, etc. Within a home, portability enables laptop users to move their devices from the living room to the dining room to the family room. Portability offers several distinct advantages:
</p>, <p>Other advantages of laptops:
</p>, <p>Compared to desktop PCs, laptops have disadvantages in the following areas:
</p>, <p>While the performance of mainstream desktops and laptops are comparable, and the cost of laptops has fallen less rapidly than desktops, laptops remain more expensive than desktop PCs at the same performance level.<sup class=""reference"" id=""cite_ref-62""><a href=""#cite_note-62"">[60]</a></sup><sup class=""noprint Inline-Template"" style=""white-space:nowrap;"">[<i><a href=""/wiki/Wikipedia:Manual_of_Style/Dates_and_numbers#Chronological_items"" title=""Wikipedia:Manual of Style/Dates and numbers""><span title=""The date of the event predicted near this tag has passed. (April 2021)"">needs update</span></a></i>]</sup> The upper limits of performance of laptops remain much lower than the highest-end desktops (especially ""workstation class"" machines with two processor sockets), and ""leading-edge"" features usually appear first in desktops and only then, as the underlying technology matures, are adapted to laptops.
</p>, <p>For Internet browsing and typical office applications, where the computer spends the majority of its time waiting for the next user input, even relatively low-end laptops (such as Netbooks) can be fast enough for some users.<sup class=""reference"" id=""cite_ref-63""><a href=""#cite_note-63"">[61]</a></sup> Most higher-end laptops are sufficiently powerful for high-resolution movie playback, some 3D gaming and video editing and encoding. However, laptop processors can be disadvantaged when dealing with a higher-end database, maths, engineering, financial software, virtualization, etc. This is because laptops use the mobile versions of processors to conserve power, and these lag behind desktop chips when it comes to performance. Some manufacturers work around this performance problem by using desktop CPUs for laptops.<sup class=""reference"" id=""cite_ref-64""><a href=""#cite_note-64"">[62]</a></sup>
</p>, <p>The upgradeability of laptops is very limited compared to thoroughly standardized desktops. In general, hard drives and memory can be upgraded easily. Optical drives and <a class=""mw-redirect"" href=""/wiki/Internal_expansion_card"" title=""Internal expansion card"">internal expansion cards</a> may be upgraded if they follow an <a href=""/wiki/Technical_standard"" title=""Technical standard"">industry standard</a>, but all other internal components, including the motherboard, CPU, and graphics, are not always intended to be upgradeable. <a href=""/wiki/Intel"" title=""Intel"">Intel</a>, <a href=""/wiki/Asus"" title=""Asus"">Asus</a>, <a href=""/wiki/Compal_Electronics"" title=""Compal Electronics"">Compal</a>, <a href=""/wiki/Quanta_Computer"" title=""Quanta Computer"">Quanta</a> and some other laptop manufacturers have created the <a href=""/wiki/Common_Building_Block"" title=""Common Building Block"">Common Building Block</a> standard for laptop parts to address some of the inefficiencies caused by the lack of standards. The reasons for limited upgradeability are both technical and economic. There is no industry-wide standard <a class=""mw-redirect"" href=""/wiki/Computer_form_factor"" title=""Computer form factor"">form factor</a> for laptops; each major laptop manufacturer pursues its own <a href=""/wiki/Proprietary_hardware"" title=""Proprietary hardware"">proprietary</a> design and construction, with the result that laptops are difficult to upgrade and have high repair costs. Moreover, starting with 2013 models, laptops have become increasingly integrated (soldered) with the motherboard for most of its components (CPU, SSD, RAM, keyboard, etc.) to reduce size and upgradeability prospects. Devices such as sound cards, network adapters, hard and optical drives, and numerous other peripherals are available, but these upgrades usually impair the laptop's portability, because they add cables and boxes to the setup and often have to be disconnected and reconnected when the laptop is on the move.<sup class=""noprint Inline-Template Template-Fact"" style=""white-space:nowrap;"">[<i><a href=""/wiki/Wikipedia:Citation_needed"" title=""Wikipedia:Citation needed""><span title=""This claim needs references to reliable sources. (February 2020)"">citation needed</span></a></i>]</sup>
</p>, <p>Prolonged use of laptops can cause <a href=""/wiki/Repetitive_strain_injury"" title=""Repetitive strain injury"">repetitive strain injury</a> because of their small, flat keyboard and trackpad pointing devices.<sup class=""reference"" id=""cite_ref-65""><a href=""#cite_note-65"">[63]</a></sup> Usage of separate, external <a href=""/wiki/Ergonomic_keyboard"" title=""Ergonomic keyboard"">ergonomic keyboards</a> and pointing devices is recommended to prevent injury when working for long periods of time; they can be connected to a laptop easily by USB, <a href=""/wiki/Bluetooth"" title=""Bluetooth"">Bluetooth</a> or via a docking station. Some health standards require ergonomic keyboards at workplaces.
</p>, <p>A laptop's integrated screen often requires users to lean over for a better view, which can cause neck or spinal injuries. A larger and higher-quality external screen can be connected to almost any laptop to alleviate this and to provide additional screen space for more productive work. Another solution is to use a <a href=""/wiki/Docking_station#Computer_stands"" title=""Docking station"">computer stand</a>.
</p>, <p>A study by <a href=""/wiki/State_University_of_New_York"" title=""State University of New York"">State University of New York</a> researchers found that heat generated from laptops can increase the temperature of the lap of male users when balancing the computer on their lap, potentially putting <a class=""mw-redirect"" href=""/wiki/Sperm_count"" title=""Sperm count"">sperm count</a> at risk. The study, which included roughly two dozen men between the ages of 21 and 35, found that the sitting position required to balance a laptop can increase scrotum temperature by as much as 2.1 °C (4 °F). However, further research is needed to determine whether this directly affects male <a href=""/wiki/Infertility"" title=""Infertility"">sterility</a>.<sup class=""reference"" id=""cite_ref-66""><a href=""#cite_note-66"">[64]</a></sup> A later 2010 study of 29 males published in <i>Fertility and Sterility</i> found that men who kept their laptops on their laps experienced scrotal hyperthermia (overheating) in which their scrotal temperatures increased by up to 2.0 °C (4 °F). The resulting heat increase, which could not be offset by a laptop cushion, may increase male infertility.<sup class=""reference"" id=""cite_ref-study1_67-0""><a href=""#cite_note-study1-67"">[65]</a></sup><sup class=""reference"" id=""cite_ref-Yin_68-0""><a href=""#cite_note-Yin-68"">[66]</a></sup><sup class=""reference"" id=""cite_ref-69""><a href=""#cite_note-69"">[67]</a></sup><sup class=""reference"" id=""cite_ref-Caulfield_70-0""><a href=""#cite_note-Caulfield-70"">[68]</a></sup><sup class=""reference"" id=""cite_ref-Joelving_71-0""><a href=""#cite_note-Joelving-71"">[69]</a></sup>
</p>, <p>A common practical solution to this problem is to place the laptop on a table or desk or to use a book or pillow between the body and the laptop.<sup class=""noprint Inline-Template Template-Fact"" style=""white-space:nowrap;"">[<i><a href=""/wiki/Wikipedia:Citation_needed"" title=""Wikipedia:Citation needed""><span title=""This claim needs references to reliable sources. (February 2012)"">citation needed</span></a></i>]</sup> Another solution is to obtain a cooling unit for the laptop. These are usually USB powered and consist of a hard thin plastic case housing one, two, or three cooling fans – with the entire assembly designed to sit under the laptop in question – which results in the laptop remaining cool to the touch, and greatly reduces laptop heat buildup.
</p>, <p>Heat generated from using a laptop on the lap can also cause skin discoloration on the thighs known as ""<a href=""/wiki/Erythema_ab_igne"" title=""Erythema ab igne"">toasted skin syndrome</a>"".<sup class=""reference"" id=""cite_ref-72""><a href=""#cite_note-72"">[70]</a></sup><sup class=""reference"" id=""cite_ref-Diaz_73-0""><a href=""#cite_note-Diaz-73"">[71]</a></sup><sup class=""reference"" id=""cite_ref-Hendrick_74-0""><a href=""#cite_note-Hendrick-74"">[72]</a></sup><sup class=""reference"" id=""cite_ref-TANNER_75-0""><a href=""#cite_note-TANNER-75"">[73]</a></sup>
</p>, <p>
Laptops are less durable than desktops/PCs. However, the durability of the laptop depends on the user if proper maintenance is done then the laptop can work longer.</p>, <p>Because of their portability, laptops are subject to more wear and physical damage than desktops. Components such as screen hinges, latches, <a href=""/wiki/DC_connector"" title=""DC connector"">power jacks</a>, and <a href=""/wiki/Power_cord"" title=""Power cord"">power cords</a> deteriorate gradually from ordinary use and may have to be replaced. A liquid spill onto the keyboard, a rather minor mishap with a desktop system (given that a basic keyboard costs about US$20), can damage the internals of a laptop and destroy the computer, result in a costly repair or entire replacement of laptops. One study found that a laptop is three times more likely to break during the first year of use than a desktop.<sup class=""reference"" id=""cite_ref-76""><a href=""#cite_note-76"">[74]</a></sup> To maintain a laptop, it is recommended to clean it every three months for dirt, debris, dust, and food particles. Most cleaning kits consist of a lint-free or <a href=""/wiki/Microfiber"" title=""Microfiber"">microfiber</a> cloth for the LCD screen and keyboard, compressed air for getting dust out of the cooling fan, and a cleaning solution. Harsh chemicals such as bleach should not be used to clean a laptop, as they can damage it.<sup class=""reference"" id=""cite_ref-77""><a href=""#cite_note-77"">[75]</a></sup>
</p>, <p>Laptops rely on extremely compact cooling systems involving a <a href=""/wiki/Computer_fan"" title=""Computer fan"">fan</a> and <a href=""/wiki/Heat_sink"" title=""Heat sink"">heat sink</a> that can fail from blockage caused by accumulated airborne dust and debris. Most laptops do not have any type of removable dust collection filter over the air intake for these cooling systems, resulting in a system that gradually conducts more heat and noise as the years pass. In some cases, the laptop starts to overheat even at idle load levels. This dust is usually stuck inside where the fan and heat sink meet, where it can not be removed by a casual cleaning and vacuuming. Most of the time, compressed air can dislodge the dust and debris but may not entirely remove it. After the device is turned on, the loose debris is reaccumulated into the cooling system by the fans. Complete disassembly is usually required to clean the laptop entirely. However, preventative maintenance such as regular cleaning of the heat sink via compressed air can prevent dust build-up on the heat sink. Many laptops are difficult to disassemble by the average user and contain components that are sensitive to <a href=""/wiki/Electrostatic_discharge"" title=""Electrostatic discharge"">electrostatic discharge</a> (ESD).
</p>, <p>Battery life is limited because the capacity drops with time, eventually requiring replacement after as little as a year. A new battery typically stores enough energy to run the laptop for three to five hours, depending on usage, configuration, and power management settings. Yet, as it ages, the battery's energy storage will dissipate progressively until it lasts only a few minutes. The battery is often easily replaceable and a higher capacity model may be obtained for longer charging and discharging time. Some laptops (specifically ultrabooks) do not have the usual removable battery and have to be brought to the service center of their manufacturer or a third-party laptop service center to have their battery replaced. Replacement batteries can also be expensive.
</p>, <p>Because they are valuable, commonly used, portable, and easy to hide in a backpack or other type of travel bag, laptops are often <a href=""/wiki/Laptop_theft"" title=""Laptop theft"">stolen</a>. Every day, over 1,600 laptops go missing from U.S. airports.<sup class=""reference"" id=""cite_ref-78""><a href=""#cite_note-78"">[76]</a></sup> The cost of stolen business or personal data, and of the resulting problems (<a href=""/wiki/Identity_theft"" title=""Identity theft"">identity theft</a>, <a href=""/wiki/Credit_card_fraud"" title=""Credit card fraud"">credit card fraud</a>, breach of privacy), can be many times the value of the stolen laptop itself. Consequently, the physical protection of laptops and the safeguarding of data contained on them are both of great importance. Most laptops have a <a class=""mw-redirect"" href=""/wiki/Kensington_security_slot"" title=""Kensington security slot"">Kensington security slot</a>, which can be used to tether them to a desk or other immovable object with a security cable and lock. In addition, modern operating systems and <a href=""/wiki/Disk_encryption_software"" title=""Disk encryption software"">third-party software</a> offer <a href=""/wiki/Disk_encryption"" title=""Disk encryption"">disk encryption</a> functionality, which renders the data on the laptop's <a class=""mw-redirect"" href=""/wiki/Hard_drive"" title=""Hard drive"">hard drive</a> unreadable without a <a href=""/wiki/Key_(cryptography)"" title=""Key (cryptography)"">key</a> or a passphrase. As of 2015, some laptops also have additional security elements added, including eye recognition software and fingerprint scanning components.<sup class=""reference"" id=""cite_ref-79""><a href=""#cite_note-79"">[77]</a></sup>
</p>, <p>Software such as LoJack for Laptops, Laptop Cop, and GadgetTrack have been engineered to help people locate and recover their stolen laptops in the event of theft. Setting one's laptop with a password on its firmware (protection against going to firmware setup or booting), internal HDD/SSD (protection against accessing it and loading an operating system on it afterward), and every user account of the operating system are additional security measures that a user should do.<sup class=""reference"" id=""cite_ref-80""><a href=""#cite_note-80"">[78]</a></sup><sup class=""reference"" id=""cite_ref-81""><a href=""#cite_note-81"">[79]</a></sup> Fewer than 5% of lost or stolen laptops are recovered by the companies that own them,<sup class=""reference"" id=""cite_ref-82""><a href=""#cite_note-82"">[80]</a></sup> however, that number may decrease due to a variety of companies and software solutions specializing in laptop recovery. In the 2010s, the common availability of webcams on laptops raised privacy concerns. In <i><a href=""/wiki/Robbins_v._Lower_Merion_School_District"" title=""Robbins v. Lower Merion School District"">Robbins v. Lower Merion School District</a></i> (Eastern District of Pennsylvania 2010), school-issued laptops loaded with special software enabled staff from two high schools to take secret webcam shots of students at home, via their students' laptops.<sup class=""reference"" id=""cite_ref-83""><a href=""#cite_note-83"">[81]</a></sup><sup class=""reference"" id=""cite_ref-84""><a href=""#cite_note-84"">[82]</a></sup><sup class=""reference"" id=""cite_ref-85""><a href=""#cite_note-85"">[83]</a></sup>
</p>, <p>There are many laptop brands and manufacturers. Several major brands that offer notebooks in various classes are listed in the adjacent box.
The major brands usually offer good service and support, including well-executed documentation and driver downloads that remain available for many years after a particular laptop model is no longer produced. Capitalizing on service, support, and brand image, laptops from major brands are more expensive than laptops by smaller brands and <a href=""/wiki/Original_design_manufacturer"" title=""Original design manufacturer"">ODMs</a>. Some brands specialize in a particular class of laptops, such as gaming laptops (<a href=""/wiki/Alienware"" title=""Alienware"">Alienware</a>), high-performance laptops (<a href=""/wiki/HP_Envy"" title=""HP Envy"">HP Envy</a>), netbooks (<a class=""mw-redirect"" href=""/wiki/EeePC"" title=""EeePC"">EeePC</a>) and laptops for children (<a href=""/wiki/One_Laptop_per_Child"" title=""One Laptop per Child"">OLPC</a>).
</p>, <p>Many brands, including the major ones, do not design and do not manufacture their laptops. Instead, a small number of Original Design Manufacturers (ODMs) design new models of laptops, and the brands choose the models to be included in their lineup. In 2006, 7 major ODMs manufactured 7 of every 10 laptops in the world, with the largest one (<a href=""/wiki/Quanta_Computer"" title=""Quanta Computer"">Quanta Computer</a>) having 30% of the world market share.<sup class=""reference"" id=""cite_ref-86""><a href=""#cite_note-86"">[84]</a></sup> Therefore, identical models are available both from a major label and from a low-profile ODM in-house brand.
</p>, <p>Battery-powered portable computers had just 2% worldwide market share in 1986.<sup class=""reference"" id=""cite_ref-87""><a href=""#cite_note-87"">[85]</a></sup> However, laptops have become increasingly popular, both for business and personal use.<sup class=""reference"" id=""cite_ref-Computer_Economics,_Inc_88-0""><a href=""#cite_note-Computer_Economics,_Inc-88"">[86]</a></sup> Around 109 million notebook PCs shipped worldwide in 2007, a growth of 33% compared to 2006.<sup class=""reference"" id=""cite_ref-ssev2011-09-12_89-0""><a href=""#cite_note-ssev2011-09-12-89"">[87]</a></sup> In 2008 it was estimated that 145.9 million notebooks were sold, and that the number would grow in 2009 to 177.7 million.<sup class=""reference"" id=""cite_ref-ee2009-01-10_90-0""><a href=""#cite_note-ee2009-01-10-90"">[88]</a></sup> The third quarter of 2008 was the first time when worldwide notebook PC shipments exceeded <a href=""/wiki/Desktop_computer"" title=""Desktop computer"">desktops</a>, with 38.6 million units versus 38.5 million units.<sup class=""reference"" id=""cite_ref-Computer_Economics,_Inc_88-1""><a href=""#cite_note-Computer_Economics,_Inc-88"">[86]</a></sup><sup class=""reference"" id=""cite_ref-is2009-01-13_91-0""><a href=""#cite_note-is2009-01-13-91"">[89]</a></sup><sup class=""reference"" id=""cite_ref-NYT09_92-0""><a href=""#cite_note-NYT09-92"">[90]</a></sup><sup class=""reference"" id=""cite_ref-93""><a href=""#cite_note-93"">[91]</a></sup>
</p>, <p>May 2005 was the first time notebooks outsold desktops in the US over the course of a full month; at the time notebooks sold for an average of <span style=""white-space: nowrap"">$1,131</span> while desktops sold for an average of <span style=""white-space: nowrap"">$696</span>.<sup class=""reference"" id=""cite_ref-94""><a href=""#cite_note-94"">[92]</a></sup> When looking at <a href=""/wiki/Operating_system"" title=""Operating system"">operating systems</a>, for <a href=""/wiki/Microsoft_Windows"" title=""Microsoft Windows"">Microsoft Windows</a> laptops the <a href=""/wiki/Average_selling_price"" title=""Average selling price"">average selling price</a> (ASP) showed a decline in 2008/2009, possibly due to low-cost <a href=""/wiki/Netbook"" title=""Netbook"">netbooks</a>, drawing an average <span style=""white-space: nowrap"">US$689</span> at U.S. retail stores in August 2008. In 2009, ASP had further fallen to <span style=""white-space: nowrap"">$602</span> by January and to <span style=""white-space: nowrap"">$560</span> in February. While Windows machines ASP fell <span style=""white-space: nowrap"">$129</span> in these seven months, <a href=""/wiki/Apple_Inc."" title=""Apple Inc."">Apple</a> <a href=""/wiki/MacOS"" title=""MacOS"">macOS</a> laptop ASP declined just <span style=""white-space: nowrap"">$12</span> from <span style=""white-space: nowrap"">$1,524</span> to <span style=""white-space: nowrap"">$1,512</span>.<sup class=""reference"" id=""cite_ref-95""><a href=""#cite_note-95"">[93]</a></sup>
</p>, <p>The list of materials that go into a laptop computer is long, and many of the substances used, such as <a href=""/wiki/Beryllium"" title=""Beryllium"">beryllium</a> (used in beryllium-copper alloy contacts in some connectors and sockets), lead (used in lead-tin solder), <a href=""/wiki/Chromium"" title=""Chromium"">chromium</a>, and <a href=""/wiki/Mercury_(element)"" title=""Mercury (element)"">mercury</a> (used in CCFL LCD backlights) compounds, are toxic or carcinogenic to humans. Although these toxins are relatively harmless when the laptop is in use, concerns that discarded laptops cause a serious health risk and toxic environmental damage, were so strong, that the Waste Electrical and Electronic Equipment Directive (WEEE Directive) in Europe specified that all laptop computers must be recycled by law. Similarly, the U.S. Environmental Protection Agency (<a href=""/wiki/Epargyreus_clarus"" title=""Epargyreus clarus"">EPA</a>) has outlawed <a href=""/wiki/Landfill"" title=""Landfill"">landfill</a> dumping or the incinerating of discarded laptop computers.
</p>, <p>Most laptop computers begin the recycling process with a method known as <a href=""/wiki/Demanufacturing"" title=""Demanufacturing"">Demanufacturing</a>, this involves the physical separation of the components of the laptop.<sup class=""reference"" id=""cite_ref-96""><a href=""#cite_note-96"">[94]</a></sup> These components are then either grouped into materials (e.g. plastic, metal and glass) for recycling or more complex items that require more advanced materials separation (e.g.) circuit boards, hard drives and batteries.
</p>, <p>Corporate laptop recycling can require an additional process known as data destruction. The data destruction process ensures that all information or data that has been stored on a laptop hard drive can never be retrieved again. Below is an overview of some of the data protection and environmental laws and regulations applicable for laptop recycling data destruction:
</p>, <p>The ruggedized <a href=""/wiki/Grid_Compass"" title=""Grid Compass"">Grid Compass</a> computer was used since the early days of the Space Shuttle program. The first <a href=""/wiki/Commercial_off-the-shelf"" title=""Commercial off-the-shelf"">commercial</a> laptop used in space was a <a class=""mw-redirect"" href=""/wiki/Macintosh_portable"" title=""Macintosh portable"">Macintosh portable</a> in 1991 aboard Space Shuttle mission <a href=""/wiki/STS-43"" title=""STS-43"">STS-43</a>.<sup class=""reference"" id=""cite_ref-97""><a href=""#cite_note-97"">[95]</a></sup><sup class=""reference"" id=""cite_ref-98""><a href=""#cite_note-98"">[96]</a></sup><sup class=""reference"" id=""cite_ref-99""><a href=""#cite_note-99"">[97]</a></sup> Apple and other laptop computers continue to be flown aboard crewed spaceflights, though the only long-duration flight certified computer for the International Space Station is the <a href=""/wiki/ThinkPad#Use_in_space"" title=""ThinkPad"">ThinkPad</a>.<sup class=""reference"" id=""cite_ref-100""><a href=""#cite_note-100"">[98]</a></sup> As of 2011, over 100 ThinkPads were aboard the ISS. Laptops used aboard the International Space Station and other spaceflights are generally the same ones that <a href=""/wiki/Commercial_off-the-shelf"" title=""Commercial off-the-shelf"">can be purchased</a> by the general public but needed modifications are made to allow them to be used safely and effectively in a weightless environment such as updating the cooling systems to function without relying on hot air rising and accommodation for the lower cabin air pressure.<sup class=""reference"" id=""cite_ref-101""><a href=""#cite_note-101"">[99]</a></sup> Laptops operating in harsh usage environments and conditions, such as strong vibrations, extreme temperatures, and wet or dusty conditions differ from those used in space in that they are <a href=""/wiki/Rugged_computer"" title=""Rugged computer"">custom designed</a> for the task and do not use <a href=""/wiki/Commercial_off-the-shelf"" title=""Commercial off-the-shelf"">commercial off-the-shelf</a> hardware.
</p>]"
Mobile,"[<p><b>Mobile</b> may refer to:
</p>]"
Computer,"[<p class=""mw-empty-elt"">
</p>, <p>A <b>computer</b> is a <a href=""/wiki/Digital_electronics"" title=""Digital electronics"">digital electronic</a> <a href=""/wiki/Machine"" title=""Machine"">machine</a> that can be programmed to <a href=""/wiki/Execution_(computing)"" title=""Execution (computing)"">carry out</a> <a href=""/wiki/Sequence"" title=""Sequence"">sequences</a> of <a href=""/wiki/Arithmetic"" title=""Arithmetic"">arithmetic</a> or <a class=""mw-redirect"" href=""/wiki/Logical_operations"" title=""Logical operations"">logical operations</a> (<a href=""/wiki/Computation"" title=""Computation"">computation</a>) automatically. Modern computers can perform generic sets of operations known as <a href=""/wiki/Computer_program"" title=""Computer program"">programs</a>. These programs enable computers to perform a wide range of tasks. A <b>computer system</b> is a ""complete"" computer that includes the <a href=""/wiki/Computer_hardware"" title=""Computer hardware"">hardware</a>, <a href=""/wiki/Operating_system"" title=""Operating system"">operating system</a> (main <a href=""/wiki/Software"" title=""Software"">software</a>), and <a href=""/wiki/Peripheral"" title=""Peripheral"">peripheral</a> equipment needed and used for ""full"" operation. This term may also refer to a group of computers that are linked and function together, such as a <a href=""/wiki/Computer_network"" title=""Computer network"">computer network</a> or <a href=""/wiki/Computer_cluster"" title=""Computer cluster"">computer cluster</a>.
</p>, <p>A broad range of <a href=""/wiki/Programmable_logic_controller"" title=""Programmable logic controller"">industrial</a> and <a href=""/wiki/Consumer_electronics"" title=""Consumer electronics"">consumer products</a> use computers as <a href=""/wiki/Control_system"" title=""Control system"">control systems</a>. Simple special-purpose devices like <a href=""/wiki/Microwave_oven"" title=""Microwave oven"">microwave ovens</a> and <a href=""/wiki/Remote_control"" title=""Remote control"">remote controls</a> are included, as are factory devices like <a href=""/wiki/Industrial_robot"" title=""Industrial robot"">industrial robots</a> and <a href=""/wiki/Computer-aided_design"" title=""Computer-aided design"">computer-aided design</a>, as well as general-purpose devices like <a href=""/wiki/Personal_computer"" title=""Personal computer"">personal computers</a> and <a href=""/wiki/Mobile_device"" title=""Mobile device"">mobile devices</a> like <a href=""/wiki/Smartphone"" title=""Smartphone"">smartphones</a>. Computers power the <a href=""/wiki/Internet"" title=""Internet"">Internet</a>, which links billions of other computers and users.
</p>, <p>Early computers were meant to be used only for calculations. Simple manual instruments like the <a href=""/wiki/Abacus"" title=""Abacus"">abacus</a> have aided people in doing calculations since ancient times. Early in the <a href=""/wiki/Industrial_Revolution"" title=""Industrial Revolution"">Industrial Revolution</a>, some mechanical devices were built to automate long tedious tasks, such as guiding patterns for <a href=""/wiki/Loom"" title=""Loom"">looms</a>. More sophisticated electrical <a href=""/wiki/Machine"" title=""Machine"">machines</a> did specialized <a href=""/wiki/Analogue_electronics"" title=""Analogue electronics"">analog</a> calculations in the early 20th century. The first <a href=""/wiki/Digital_data"" title=""Digital data"">digital</a> electronic calculating machines were developed during <a href=""/wiki/World_War_II"" title=""World War II"">World War II</a>. The first <a href=""/wiki/Semiconductor"" title=""Semiconductor"">semiconductor</a> <a href=""/wiki/Transistor"" title=""Transistor"">transistors</a> in the late 1940s were followed by the <a href=""/wiki/Silicon"" title=""Silicon"">silicon</a>-based <a href=""/wiki/MOSFET"" title=""MOSFET"">MOSFET</a> (MOS transistor) and <a class=""mw-redirect"" href=""/wiki/Monolithic_integrated_circuit"" title=""Monolithic integrated circuit"">monolithic integrated circuit</a> (IC) chip technologies in the late 1950s, leading to the <a href=""/wiki/Microprocessor"" title=""Microprocessor"">microprocessor</a> and the <a class=""mw-redirect"" href=""/wiki/Microcomputer_revolution"" title=""Microcomputer revolution"">microcomputer revolution</a> in the 1970s. The speed, power and versatility of computers have been increasing dramatically ever since then, with <a href=""/wiki/Transistor_count"" title=""Transistor count"">transistor counts</a> increasing at a rapid pace (as predicted by <a href=""/wiki/Moore%27s_law"" title=""Moore's law"">Moore's law</a>), leading to the <a href=""/wiki/Digital_Revolution"" title=""Digital Revolution"">Digital Revolution</a> during the late 20th to early 21st centuries.
</p>, <p>Conventionally, a modern computer consists of at least one <a class=""mw-redirect"" href=""/wiki/Processing_element"" title=""Processing element"">processing element</a>, typically a <a href=""/wiki/Central_processing_unit"" title=""Central processing unit"">central processing unit</a> (CPU) in the form of a <a href=""/wiki/Microprocessor"" title=""Microprocessor"">microprocessor</a>, along with some type of <a href=""/wiki/Computer_memory"" title=""Computer memory"">computer memory</a>, typically <a href=""/wiki/Semiconductor_memory"" title=""Semiconductor memory"">semiconductor memory</a> chips. The processing element carries out arithmetic and logical operations, and a sequencing and control unit can change the order of operations in response to stored <a href=""/wiki/Data"" title=""Data"">information</a>. <a href=""/wiki/Peripheral"" title=""Peripheral"">Peripheral</a> devices include input devices (keyboards, mice, <a href=""/wiki/Joystick"" title=""Joystick"">joystick</a>, etc.), output devices (monitor screens, <a href=""/wiki/Printer_(computing)"" title=""Printer (computing)"">printers</a>, etc.), and input/output devices that perform both functions (e.g., the 2000s-era <a href=""/wiki/Touchscreen"" title=""Touchscreen"">touchscreen</a>). Peripheral devices allow information to be retrieved from an external source and they enable the result of operations to be saved and retrieved.
</p>, <p>According to the <i><a href=""/wiki/Oxford_English_Dictionary"" title=""Oxford English Dictionary"">Oxford English Dictionary</a></i>, the first known use of <i>computer</i> was in a 1613 book called <i>The Yong Mans Gleanings</i> by the English writer <a href=""/wiki/Richard_Brathwait"" title=""Richard Brathwait"">Richard Brathwait</a>: ""I haue  [<i><a href=""/wiki/Sic"" title=""Sic"">sic</a></i>] read the truest computer of Times, and the best Arithmetician that euer [sic] breathed, and he reduceth thy dayes into a short number."" This usage of the term referred to a <a class=""mw-redirect"" href=""/wiki/Human_computer"" title=""Human computer"">human computer</a>, a person who carried out calculations or computations. The word continued with the same meaning until the middle of the 20th century. During the latter part of this period women were often hired as computers because they could be paid less than their male counterparts.<sup class=""reference"" id=""cite_ref-FOOTNOTEEvans201823_1-0""><a href=""#cite_note-FOOTNOTEEvans201823-1"">[1]</a></sup> By 1943, most human computers were women.<sup class=""reference"" id=""cite_ref-FOOTNOTESmith20136_2-0""><a href=""#cite_note-FOOTNOTESmith20136-2"">[2]</a></sup>
</p>, <p>The <i><a href=""/wiki/Online_Etymology_Dictionary"" title=""Online Etymology Dictionary"">Online Etymology Dictionary</a></i> gives the first attested use of <i>computer</i> in the 1640s, meaning 'one who calculates'; this is an ""agent noun from compute (v.)"". The <i>Online Etymology Dictionary</i> states that the use of the term to mean <span style=""padding-right:.15em;"">""</span>'calculating machine' (of any type) is from 1897.""  The <i>Online Etymology Dictionary</i> indicates that the ""modern use"" of the term, to mean 'programmable digital electronic computer' dates from ""1945 under this name; [in a] theoretical [sense] from 1937, as <i><a href=""/wiki/Turing_machine"" title=""Turing machine"">Turing machine</a></i>"".<sup class=""reference"" id=""cite_ref-3""><a href=""#cite_note-3"">[3]</a></sup>
</p>, <p>Devices have been used to aid computation for thousands of years, mostly using <a class=""mw-redirect"" href=""/wiki/One-to-one_correspondence"" title=""One-to-one correspondence"">one-to-one correspondence</a> with <a class=""mw-redirect"" href=""/wiki/Finger_counting"" title=""Finger counting"">fingers</a>. The earliest counting device was probably a form of <a href=""/wiki/Tally_stick"" title=""Tally stick"">tally stick</a>. Later record keeping aids throughout the <a href=""/wiki/Fertile_Crescent"" title=""Fertile Crescent"">Fertile Crescent</a> included calculi (clay spheres, cones, etc.) which represented counts of items, probably livestock or grains, sealed in hollow unbaked clay containers.<sup class=""reference"" id=""cite_ref-4""><a href=""#cite_note-4"">[a]</a></sup><sup class=""reference"" id=""cite_ref-5""><a href=""#cite_note-5"">[4]</a></sup> The use of <a href=""/wiki/Counting_rods"" title=""Counting rods"">counting rods</a> is one example.
</p>, <p>The <a href=""/wiki/Abacus"" title=""Abacus"">abacus</a> was initially used for arithmetic tasks. The <a href=""/wiki/Roman_abacus"" title=""Roman abacus"">Roman abacus</a> was developed from devices used in <a href=""/wiki/Babylonia"" title=""Babylonia"">Babylonia</a> as early as 2400 BC. Since then, many other forms of reckoning boards or tables have been invented. In a medieval European <a href=""/wiki/Counting_house"" title=""Counting house"">counting house</a>, a checkered cloth would be placed on a table, and markers moved around on it according to certain rules, as an aid to calculating sums of money.<sup class=""reference"" id=""cite_ref-6""><a href=""#cite_note-6"">[5]</a></sup>
</p>, <p>The <a href=""/wiki/Antikythera_mechanism"" title=""Antikythera mechanism"">Antikythera mechanism</a> is believed to be the earliest known mechanical <a href=""/wiki/Analog_computer"" title=""Analog computer"">analog computer</a>, according to <a href=""/wiki/Derek_J._de_Solla_Price"" title=""Derek J. de Solla Price"">Derek J. de Solla Price</a>.<sup class=""reference"" id=""cite_ref-7""><a href=""#cite_note-7"">[6]</a></sup> It was designed to calculate astronomical positions. It was discovered in 1901 in the <a href=""/wiki/Antikythera_wreck"" title=""Antikythera wreck"">Antikythera wreck</a> off the Greek island of <a href=""/wiki/Antikythera"" title=""Antikythera"">Antikythera</a>, between <a class=""mw-redirect"" href=""/wiki/Kythera"" title=""Kythera"">Kythera</a> and <a href=""/wiki/Crete"" title=""Crete"">Crete</a>, and has been dated to approximately <abbr title=""circa"">c.</abbr><span style=""white-space:nowrap;""> 100 BC</span>. Devices of comparable complexity to the Antikythera mechanism would not reappear until the fourteenth century.<sup class=""reference"" id=""cite_ref-8""><a href=""#cite_note-8"">[7]</a></sup>
</p>, <p>Many mechanical aids to calculation and measurement were constructed for astronomical and navigation use. The <a href=""/wiki/Planisphere"" title=""Planisphere"">planisphere</a> was a <a href=""/wiki/Star_chart"" title=""Star chart"">star chart</a> invented by <a href=""/wiki/Al-Biruni"" title=""Al-Biruni"">Abū Rayhān al-Bīrūnī</a> in the early 11th century.<sup class=""reference"" id=""cite_ref-Wiet_9-0""><a href=""#cite_note-Wiet-9"">[8]</a></sup> The <a href=""/wiki/Astrolabe"" title=""Astrolabe"">astrolabe</a> was invented in the <a class=""mw-redirect"" href=""/wiki/Hellenistic_civilization"" title=""Hellenistic civilization"">Hellenistic world</a> in either the 1st or 2nd centuries BC and is often attributed to <a href=""/wiki/Hipparchus"" title=""Hipparchus"">Hipparchus</a>. A combination of the planisphere and <a href=""/wiki/Dioptra"" title=""Dioptra"">dioptra</a>, the astrolabe was effectively an analog computer capable of working out several different kinds of problems in <a href=""/wiki/Spherical_astronomy"" title=""Spherical astronomy"">spherical astronomy</a>. An astrolabe incorporating a mechanical <a href=""/wiki/Calendar"" title=""Calendar"">calendar</a> computer<sup class=""reference"" id=""cite_ref-10""><a href=""#cite_note-10"">[9]</a></sup><sup class=""reference"" id=""cite_ref-11""><a href=""#cite_note-11"">[10]</a></sup> and <a href=""/wiki/Gear"" title=""Gear"">gear</a>-wheels was invented by Abi Bakr of <a href=""/wiki/Isfahan"" title=""Isfahan"">Isfahan</a>, <a class=""mw-redirect"" href=""/wiki/Persia"" title=""Persia"">Persia</a> in 1235.<sup class=""reference"" id=""cite_ref-12""><a href=""#cite_note-12"">[11]</a></sup> Abū Rayhān al-Bīrūnī invented the first mechanical geared <a href=""/wiki/Lunisolar_calendar"" title=""Lunisolar calendar"">lunisolar calendar</a> astrolabe,<sup class=""reference"" id=""cite_ref-13""><a href=""#cite_note-13"">[12]</a></sup> an early fixed-<a href=""/wiki/Wire"" title=""Wire"">wired</a> knowledge processing <a href=""/wiki/Machine"" title=""Machine"">machine</a><sup class=""reference"" id=""cite_ref-Oren_14-0""><a href=""#cite_note-Oren-14"">[13]</a></sup> with a <a href=""/wiki/Gear_train"" title=""Gear train"">gear train</a> and gear-wheels,<sup class=""reference"" id=""cite_ref-15""><a href=""#cite_note-15"">[14]</a></sup> <abbr title=""circa"">c.</abbr><span style=""white-space:nowrap;""> 1000 AD</span>.
</p>, <p>The <a href=""/wiki/Sector_(instrument)"" title=""Sector (instrument)"">sector</a>, a calculating instrument used for solving problems in proportion, trigonometry, multiplication and division, and for various functions, such as squares and cube roots, was developed in the late 16th century and found application in gunnery, surveying and navigation.
</p>, <p>The <a href=""/wiki/Planimeter"" title=""Planimeter"">planimeter</a> was a manual instrument to calculate the area of a closed figure by tracing over it with a mechanical linkage.
</p>, <p>The <a href=""/wiki/Slide_rule"" title=""Slide rule"">slide rule</a> was invented around 1620–1630 by the English clergyman <a href=""/wiki/William_Oughtred"" title=""William Oughtred"">William Oughtred</a>, shortly after the publication of the concept of the <a href=""/wiki/Logarithm"" title=""Logarithm"">logarithm</a>. It is a hand-operated analog computer for doing multiplication and division. As slide rule development progressed, added scales provided reciprocals, squares and square roots, cubes and cube roots, as well as <a href=""/wiki/Transcendental_function"" title=""Transcendental function"">transcendental functions</a> such as logarithms and exponentials, circular and <a href=""/wiki/Hyperbolic_functions"" title=""Hyperbolic functions"">hyperbolic</a> <a href=""/wiki/Trigonometry"" title=""Trigonometry"">trigonometry</a> and other <a href=""/wiki/Function_(mathematics)"" title=""Function (mathematics)"">functions</a>. Slide rules with special scales are still used for quick performance of routine calculations, such as the <a href=""/wiki/E6B"" title=""E6B"">E6B</a> circular slide rule used for time and distance calculations on light aircraft.
</p>, <p>In the 1770s, <a href=""/wiki/Pierre_Jaquet-Droz"" title=""Pierre Jaquet-Droz"">Pierre Jaquet-Droz</a>, a Swiss <a href=""/wiki/Watchmaker"" title=""Watchmaker"">watchmaker</a>, built a mechanical doll (<a class=""mw-redirect"" href=""/wiki/Automata"" title=""Automata"">automaton</a>) that could write holding a quill pen. By switching the number and order of its internal wheels different letters, and hence different messages, could be produced. In effect, it could be mechanically ""programmed"" to read instructions. Along with two other complex machines, the doll is at the Musée d'Art et d'Histoire of <a href=""/wiki/Neuch%C3%A2tel"" title=""Neuchâtel"">Neuchâtel</a>, <a href=""/wiki/Switzerland"" title=""Switzerland"">Switzerland</a>, and still operates.<sup class=""reference"" id=""cite_ref-16""><a href=""#cite_note-16"">[15]</a></sup>
</p>, <p>In 1831–1835, mathematician and engineer <a href=""/wiki/Giovanni_Plana"" title=""Giovanni Plana"">Giovanni Plana</a> devised a <a class=""mw-redirect"" href=""/wiki/Cappella_dei_Mercanti_(Turin)#Perpetual_calendar"" title=""Cappella dei Mercanti (Turin)"">Perpetual Calendar machine</a>, which, through a system of pulleys and cylinders and over, could predict the <a href=""/wiki/Perpetual_calendar"" title=""Perpetual calendar"">perpetual calendar</a> for every year from AD 0 (that is, 1 BC) to AD 4000, keeping track of leap years and varying day length. The <a href=""/wiki/Tide-predicting_machine"" title=""Tide-predicting machine"">tide-predicting machine</a> invented by the Scottish scientist <a href=""/wiki/William_Thomson,_1st_Baron_Kelvin"" title=""William Thomson, 1st Baron Kelvin"">Sir William Thomson</a> in 1872 was of great utility to navigation in shallow waters. It used a system of pulleys and wires to automatically calculate predicted tide levels for a set period at a particular location.
</p>, <p>The <a href=""/wiki/Differential_analyser"" title=""Differential analyser"">differential analyser</a>, a mechanical analog computer designed to solve <a href=""/wiki/Differential_equation"" title=""Differential equation"">differential equations</a> by <a href=""/wiki/Integral"" title=""Integral"">integration</a>, used wheel-and-disc mechanisms to perform the integration. In 1876, Sir William Thomson had already discussed the possible construction of such calculators, but he had been stymied by the limited output torque of the <a href=""/wiki/Ball-and-disk_integrator"" title=""Ball-and-disk integrator"">ball-and-disk integrators</a>.<sup class=""reference"" id=""cite_ref-scientific-computing.com_17-0""><a href=""#cite_note-scientific-computing.com-17"">[16]</a></sup> In a differential analyzer, the output of one integrator drove the input of the next integrator, or a graphing output. The <a href=""/wiki/Torque_amplifier"" title=""Torque amplifier"">torque amplifier</a> was the advance that allowed these machines to work. Starting in the 1920s, <a href=""/wiki/Vannevar_Bush"" title=""Vannevar Bush"">Vannevar Bush</a> and others developed mechanical differential analyzers.
</p>, <p><a href=""/wiki/Charles_Babbage"" title=""Charles Babbage"">Charles Babbage</a>, an English mechanical engineer and <a href=""/wiki/Polymath"" title=""Polymath"">polymath</a>, originated the concept of a programmable computer. Considered the ""<a class=""mw-redirect"" href=""/wiki/Computer_pioneer"" title=""Computer pioneer"">father of the computer</a>"",<sup class=""reference"" id=""cite_ref-18""><a href=""#cite_note-18"">[17]</a></sup> he conceptualized and invented the first <a href=""/wiki/Mechanical_computer"" title=""Mechanical computer"">mechanical computer</a> in the early 19th century. After working on his revolutionary <a href=""/wiki/Difference_engine"" title=""Difference engine"">difference engine</a>, designed to aid in navigational calculations, in 1833 he realized that a much more general design, an <a href=""/wiki/Analytical_Engine"" title=""Analytical Engine"">Analytical Engine</a>, was possible. The input of programs and data was to be provided to the machine via <a href=""/wiki/Punched_card"" title=""Punched card"">punched cards</a>, a method being used at the time to direct mechanical <a href=""/wiki/Loom"" title=""Loom"">looms</a> such as the <a class=""mw-redirect"" href=""/wiki/Jacquard_loom"" title=""Jacquard loom"">Jacquard loom</a>. For output, the machine would have a printer, a curve plotter and a bell. The machine would also be able to punch numbers onto cards to be read in later. The Engine incorporated an <a href=""/wiki/Arithmetic_logic_unit"" title=""Arithmetic logic unit"">arithmetic logic unit</a>, <a href=""/wiki/Control_flow"" title=""Control flow"">control flow</a> in the form of <a class=""mw-redirect"" href=""/wiki/Conditional_branching"" title=""Conditional branching"">conditional branching</a> and <a class=""mw-redirect"" href=""/wiki/Program_loop#Loops"" title=""Program loop"">loops</a>, and integrated <a href=""/wiki/Computer_memory"" title=""Computer memory"">memory</a>, making it the first design for a general-purpose computer that could be described in modern terms as <a class=""mw-redirect"" href=""/wiki/Turing-complete"" title=""Turing-complete"">Turing-complete</a>.<sup class=""reference"" id=""cite_ref-babbageonline_19-0""><a href=""#cite_note-babbageonline-19"">[18]</a></sup><sup class=""reference"" id=""cite_ref-20""><a href=""#cite_note-20"">[19]</a></sup>
</p>, <p>The machine was about a century ahead of its time. All the parts for his machine had to be made by hand – this was a major problem for a device with thousands of parts. Eventually, the project was dissolved with the decision of the <a class=""mw-redirect"" href=""/wiki/British_Government"" title=""British Government"">British Government</a> to cease funding. Babbage's failure to complete the analytical engine can be chiefly attributed to political and financial difficulties as well as his desire to develop an increasingly sophisticated computer and to move ahead faster than anyone else could follow. Nevertheless, his son, Henry Babbage, completed a simplified version of the analytical engine's computing unit (the <i>mill</i>) in 1888. He gave a successful demonstration of its use in computing tables in 1906.
</p>, <p>During the first half of the 20th century, many scientific <a href=""/wiki/Computing"" title=""Computing"">computing</a> needs were met by increasingly sophisticated <a href=""/wiki/Analog_computer"" title=""Analog computer"">analog computers</a>, which used a direct mechanical or electrical model of the problem as a basis for <a href=""/wiki/Computation"" title=""Computation"">computation</a>. However, these were not programmable and generally lacked the versatility and accuracy of modern digital computers.<sup class=""reference"" id=""cite_ref-stanf_21-0""><a href=""#cite_note-stanf-21"">[20]</a></sup> The first modern analog computer was a <a href=""/wiki/Tide-predicting_machine"" title=""Tide-predicting machine"">tide-predicting machine</a>, invented by <a href=""/wiki/William_Thomson,_1st_Baron_Kelvin"" title=""William Thomson, 1st Baron Kelvin"">Sir William Thomson</a> (later to become Lord Kelvin) in 1872. The <a href=""/wiki/Differential_analyser"" title=""Differential analyser"">differential analyser</a>, a mechanical analog computer designed to solve differential equations by integration using wheel-and-disc mechanisms, was conceptualized in 1876 by <a href=""/wiki/James_Thomson_(engineer)"" title=""James Thomson (engineer)"">James Thomson</a>, the elder brother of the more famous Sir William Thomson.<sup class=""reference"" id=""cite_ref-scientific-computing.com_17-1""><a href=""#cite_note-scientific-computing.com-17"">[16]</a></sup>
</p>, <p>The art of mechanical analog computing reached its zenith with the <a class=""mw-redirect"" href=""/wiki/Differential_analyzer"" title=""Differential analyzer"">differential analyzer</a>, built by H. L. Hazen and <a href=""/wiki/Vannevar_Bush"" title=""Vannevar Bush"">Vannevar Bush</a> at <a href=""/wiki/Massachusetts_Institute_of_Technology"" title=""Massachusetts Institute of Technology"">MIT</a> starting in 1927. This built on the mechanical integrators of <a href=""/wiki/James_Thomson_(engineer)"" title=""James Thomson (engineer)"">James Thomson</a> and the torque amplifiers invented by H. W. Nieman. A dozen of these devices were built before their obsolescence became obvious. By the 1950s, the success of <a href=""/wiki/Digital_electronic_computer"" title=""Digital electronic computer"">digital electronic computers</a> had spelled the end for most analog computing machines, but analog computers remained in use during the 1950s in some specialized applications such as education (<a href=""/wiki/Slide_rule"" title=""Slide rule"">slide rule</a>) and aircraft (<a href=""/wiki/Control_system"" title=""Control system"">control systems</a>).
</p>, <p>By 1938, the <a href=""/wiki/United_States_Navy"" title=""United States Navy"">United States Navy</a> had developed an electromechanical analog computer small enough to use aboard a <a href=""/wiki/Submarine"" title=""Submarine"">submarine</a>. This was the <a href=""/wiki/Torpedo_Data_Computer"" title=""Torpedo Data Computer"">Torpedo Data Computer</a>, which used trigonometry to solve the problem of firing a torpedo at a moving target. During <a href=""/wiki/World_War_II"" title=""World War II"">World War II</a> similar devices were developed in other countries as well.
</p>, <p>Early digital computers were <a href=""/wiki/Electromechanics"" title=""Electromechanics"">electromechanical</a>; electric switches drove mechanical relays to perform the calculation. These devices had a low operating speed and were eventually superseded by much faster all-electric computers, originally using <a href=""/wiki/Vacuum_tube"" title=""Vacuum tube"">vacuum tubes</a>. The <a href=""/wiki/Z2_(computer)"" title=""Z2 (computer)"">Z2</a>, created by German engineer <a href=""/wiki/Konrad_Zuse"" title=""Konrad Zuse"">Konrad Zuse</a> in 1939, was one of the earliest examples of an electromechanical relay computer.<sup class=""reference"" id=""cite_ref-Part_4_Zuse_22-0""><a href=""#cite_note-Part_4_Zuse-22"">[21]</a></sup>
</p>, <p>In 1941, Zuse followed his earlier machine up with the <a href=""/wiki/Z3_(computer)"" title=""Z3 (computer)"">Z3</a>, the world's first working electromechanical <a href=""/wiki/Computer_programming"" title=""Computer programming"">programmable</a>, fully automatic digital computer.<sup class=""reference"" id=""cite_ref-23""><a href=""#cite_note-23"">[22]</a></sup><sup class=""reference"" id=""cite_ref-24""><a href=""#cite_note-24"">[23]</a></sup> The Z3 was built with 2000 <a href=""/wiki/Relay"" title=""Relay"">relays</a>, implementing a 22 <a href=""/wiki/Bit"" title=""Bit"">bit</a> <a href=""/wiki/Word_(computer_architecture)"" title=""Word (computer architecture)"">word length</a> that operated at a <a class=""mw-redirect"" href=""/wiki/Clock_frequency"" title=""Clock frequency"">clock frequency</a> of about 5–10 <a href=""/wiki/Hertz"" title=""Hertz"">Hz</a>.<sup class=""reference"" id=""cite_ref-25""><a href=""#cite_note-25"">[24]</a></sup> Program code was supplied on punched <a href=""/wiki/Celluloid"" title=""Celluloid"">film</a> while data could be stored in 64 words of memory or supplied from the keyboard. It was quite similar to modern machines in some respects, pioneering numerous advances such as <a class=""mw-redirect"" href=""/wiki/Floating-point_number"" title=""Floating-point number"">floating-point numbers</a>. Rather than the harder-to-implement decimal system (used in <a href=""/wiki/Charles_Babbage"" title=""Charles Babbage"">Charles Babbage</a>'s earlier design), using a <a href=""/wiki/Binary_number"" title=""Binary number"">binary</a> system meant that Zuse's machines were easier to build and potentially more reliable, given the technologies available at that time.<sup class=""reference"" id=""cite_ref-26""><a href=""#cite_note-26"">[25]</a></sup> The Z3 was not itself a universal computer but could be extended to be <a class=""mw-redirect"" href=""/wiki/Turing_complete"" title=""Turing complete"">Turing complete</a>.<sup class=""reference"" id=""cite_ref-rojas-ieee_27-0""><a href=""#cite_note-rojas-ieee-27"">[26]</a></sup><sup class=""reference"" id=""cite_ref-rojas-universal_28-0""><a href=""#cite_note-rojas-universal-28"">[27]</a></sup>
</p>, <p>Zuse's next computer, the <a href=""/wiki/Z4_(computer)"" title=""Z4 (computer)"">Z4</a>, became the world's first commercial computer; after initial delay due to the Second World War, it was completed in 1950 and delivered to the <a href=""/wiki/ETH_Zurich"" title=""ETH Zurich"">ETH Zurich</a>.<sup class=""reference"" id=""cite_ref-OReganZuse_29-0""><a href=""#cite_note-OReganZuse-29"">[28]</a></sup> The computer was manufactured by Zuse's own company, <a class=""new"" href=""/w/index.php?title=Zuse_KG&amp;action=edit&amp;redlink=1"" title=""Zuse KG (page does not exist)"">Zuse KG</a><span class=""noprint"" style=""font-size:85%; font-style: normal;""> [<a class=""extiw"" href=""https://de.wikipedia.org/wiki/Zuse_KG"" title=""de:Zuse KG"">de</a>]</span>, which was founded in 1941 as the first company with the sole purpose of developing computers.<sup class=""reference"" id=""cite_ref-OReganZuse_29-1""><a href=""#cite_note-OReganZuse-29"">[28]</a></sup>
</p>, <p><span class=""anchor"" id=""Digital_computer""></span><span class=""anchor"" id=""Digital""></span>
Purely <a href=""/wiki/Electronic_circuit"" title=""Electronic circuit"">electronic circuit</a> elements soon replaced their mechanical and electromechanical equivalents, at the same time that digital calculation replaced analog. The engineer <a href=""/wiki/Tommy_Flowers"" title=""Tommy Flowers"">Tommy Flowers</a>, working at the <a href=""/wiki/Post_Office_Research_Station"" title=""Post Office Research Station"">Post Office Research Station</a> in <a href=""/wiki/London"" title=""London"">London</a> in the 1930s, began to explore the possible use of electronics for the <a href=""/wiki/Telephone_exchange"" title=""Telephone exchange"">telephone exchange</a>. Experimental equipment that he built in 1934 went into operation five years later, converting a portion of the <a href=""/wiki/Telephone_exchange"" title=""Telephone exchange"">telephone exchange</a> network into an electronic data processing system, using thousands of <a href=""/wiki/Vacuum_tube"" title=""Vacuum tube"">vacuum tubes</a>.<sup class=""reference"" id=""cite_ref-stanf_21-1""><a href=""#cite_note-stanf-21"">[20]</a></sup> In the US, <a href=""/wiki/John_Vincent_Atanasoff"" title=""John Vincent Atanasoff"">John Vincent Atanasoff</a> and <a href=""/wiki/Clifford_Berry"" title=""Clifford Berry"">Clifford E. Berry</a> of <a href=""/wiki/Iowa_State_University"" title=""Iowa State University"">Iowa State University</a> developed and tested the <a class=""mw-redirect"" href=""/wiki/Atanasoff%E2%80%93Berry_Computer"" title=""Atanasoff–Berry Computer"">Atanasoff–Berry Computer</a> (ABC) in 1942,<sup class=""reference"" id=""cite_ref-30""><a href=""#cite_note-30"">[29]</a></sup> the first ""automatic electronic digital computer"".<sup class=""reference"" id=""cite_ref-31""><a href=""#cite_note-31"">[30]</a></sup> This design was also all-electronic and used about 300 vacuum tubes, with capacitors fixed in a mechanically rotating drum for memory.<sup class=""reference"" id=""cite_ref-Copeland2006_32-0""><a href=""#cite_note-Copeland2006-32"">[31]</a></sup>
</p>, <p>During World War II, the British code-breakers at <a href=""/wiki/Bletchley_Park"" title=""Bletchley Park"">Bletchley Park</a> achieved a number of successes at breaking encrypted German military communications. The German encryption machine, <a href=""/wiki/Enigma_machine"" title=""Enigma machine"">Enigma</a>, was first attacked with the help of the electro-mechanical <a href=""/wiki/Bombe"" title=""Bombe"">bombes</a> which were often run by women.<sup class=""reference"" id=""cite_ref-:0_33-0""><a href=""#cite_note-:0-33"">[32]</a></sup><sup class=""reference"" id=""cite_ref-34""><a href=""#cite_note-34"">[33]</a></sup> To crack the more sophisticated German <a class=""mw-redirect"" href=""/wiki/Lorenz_SZ_40/42"" title=""Lorenz SZ 40/42"">Lorenz SZ 40/42</a> machine, used for high-level Army communications, <a href=""/wiki/Max_Newman"" title=""Max Newman"">Max Newman</a> and his colleagues commissioned Flowers to build the <a href=""/wiki/Colossus_computer"" title=""Colossus computer"">Colossus</a>.<sup class=""reference"" id=""cite_ref-Copeland2006_32-1""><a href=""#cite_note-Copeland2006-32"">[31]</a></sup> He spent eleven months from early February 1943 designing and building the first Colossus.<sup class=""reference"" id=""cite_ref-35""><a href=""#cite_note-35"">[34]</a></sup> After a functional test in December 1943, Colossus was shipped to Bletchley Park, where it was delivered on 18 January 1944<sup class=""reference"" id=""cite_ref-The_Colossus_Computer_36-0""><a href=""#cite_note-The_Colossus_Computer-36"">[35]</a></sup> and attacked its first message on 5 February.<sup class=""reference"" id=""cite_ref-Copeland2006_32-2""><a href=""#cite_note-Copeland2006-32"">[31]</a></sup>
</p>, <p>Colossus was the world's first <a href=""/wiki/Electronics"" title=""Electronics"">electronic</a> <a href=""/wiki/Digital_electronics"" title=""Digital electronics"">digital</a> <a href=""/wiki/Computer_programming"" title=""Computer programming"">programmable</a> computer.<sup class=""reference"" id=""cite_ref-stanf_21-2""><a href=""#cite_note-stanf-21"">[20]</a></sup> It used a large number of valves (vacuum tubes). It had paper-tape input and was capable of being configured to perform a variety of <a class=""mw-redirect"" href=""/wiki/Boolean_logic"" title=""Boolean logic"">boolean logical</a> operations on its data, but it was not <a class=""mw-redirect"" href=""/wiki/Turing-complete"" title=""Turing-complete"">Turing-complete</a>. Nine Mk II Colossi were built (The Mk I was converted to a Mk II making ten machines in total). Colossus Mark I contained 1,500 thermionic valves (tubes), but Mark II with 2,400 valves, was both five times faster and simpler to operate than Mark I, greatly speeding the decoding process.<sup class=""reference"" id=""cite_ref-37""><a href=""#cite_note-37"">[36]</a></sup><sup class=""reference"" id=""cite_ref-38""><a href=""#cite_note-38"">[37]</a></sup>
</p>, <p>The <a href=""/wiki/ENIAC"" title=""ENIAC"">ENIAC</a><sup class=""reference"" id=""cite_ref-39""><a href=""#cite_note-39"">[38]</a></sup> (Electronic Numerical Integrator and Computer) was the first electronic programmable computer built in the U.S. Although the ENIAC was similar to the Colossus, it was much faster, more flexible, and it was <a class=""mw-redirect"" href=""/wiki/Turing-complete"" title=""Turing-complete"">Turing-complete</a>. Like the Colossus, a ""program"" on the ENIAC was defined by the states of its patch cables and switches, a far cry from the <a class=""mw-redirect"" href=""/wiki/Stored_program"" title=""Stored program"">stored program</a> electronic machines that came later. Once a program was written, it had to be mechanically set into the machine with manual resetting of plugs and switches. The programmers of the ENIAC were six women, often known collectively as the ""ENIAC girls"".<sup class=""reference"" id=""cite_ref-FOOTNOTEEvans201839_40-0""><a href=""#cite_note-FOOTNOTEEvans201839-40"">[39]</a></sup><sup class=""reference"" id=""cite_ref-FOOTNOTELight1999459_41-0""><a href=""#cite_note-FOOTNOTELight1999459-41"">[40]</a></sup>
</p>, <p>It combined the high speed of electronics with the ability to be programmed for many complex problems. It could add or subtract 5000 times a second, a thousand times faster than any other machine. It also had modules to multiply, divide, and square root. High speed memory was limited to 20 words (about 80 bytes). Built under the direction of <a href=""/wiki/John_Mauchly"" title=""John Mauchly"">John Mauchly</a> and <a href=""/wiki/J._Presper_Eckert"" title=""J. Presper Eckert"">J. Presper Eckert</a> at the University of Pennsylvania, ENIAC's development and construction lasted from 1943 to full operation at the end of 1945. The machine was huge, weighing 30 tons, using 200 kilowatts of electric power and contained over 18,000 vacuum tubes, 1,500 relays, and hundreds of thousands of resistors, capacitors, and inductors.<sup class=""reference"" id=""cite_ref-Eniac_42-0""><a href=""#cite_note-Eniac-42"">[41]</a></sup>
</p>, <p>The principle of the modern computer was proposed by <a href=""/wiki/Alan_Turing"" title=""Alan Turing"">Alan Turing</a> in his seminal 1936 paper,<sup class=""reference"" id=""cite_ref-43""><a href=""#cite_note-43"">[42]</a></sup> <i>On Computable Numbers</i>. Turing proposed a simple device that he called ""Universal Computing machine"" and that is now known as a <a href=""/wiki/Universal_Turing_machine"" title=""Universal Turing machine"">universal Turing machine</a>. He proved that such a machine is capable of computing anything that is computable by executing instructions (program) stored on tape, allowing the machine to be programmable. The fundamental concept of Turing's design is the <a class=""mw-redirect"" href=""/wiki/Stored_program"" title=""Stored program"">stored program</a>, where all the instructions for computing are stored in memory. <a href=""/wiki/John_von_Neumann"" title=""John von Neumann"">Von Neumann</a> acknowledged that the central concept of the modern computer was due to this paper.<sup class=""reference"" id=""cite_ref-44""><a href=""#cite_note-44"">[43]</a></sup> Turing machines are to this day a central object of study in <a href=""/wiki/Theory_of_computation"" title=""Theory of computation"">theory of computation</a>. Except for the limitations imposed by their finite memory stores, modern computers are said to be <a class=""mw-redirect"" href=""/wiki/Turing-complete"" title=""Turing-complete"">Turing-complete</a>, which is to say, they have <a href=""/wiki/Algorithm"" title=""Algorithm"">algorithm</a> execution capability equivalent to a universal Turing machine.
</p>, <p>Early computing machines had fixed programs. Changing its function required the re-wiring and re-structuring of the machine.<sup class=""reference"" id=""cite_ref-Copeland2006_32-3""><a href=""#cite_note-Copeland2006-32"">[31]</a></sup> With the proposal of the stored-program computer this changed. A stored-program computer includes by design an <a class=""mw-redirect"" href=""/wiki/Instruction_set"" title=""Instruction set"">instruction set</a> and can store in memory a set of instructions (a <a href=""/wiki/Computer_program"" title=""Computer program"">program</a>) that details the <a href=""/wiki/Computation"" title=""Computation"">computation</a>. The theoretical basis for the stored-program computer was laid by <a href=""/wiki/Alan_Turing"" title=""Alan Turing"">Alan Turing</a> in his 1936 paper. In 1945, Turing joined the <a href=""/wiki/National_Physical_Laboratory_(United_Kingdom)"" title=""National Physical Laboratory (United Kingdom)"">National Physical Laboratory</a> and began work on developing an electronic stored-program digital computer. His 1945 report ""Proposed Electronic Calculator"" was the first specification for such a device. John von Neumann at the <a href=""/wiki/University_of_Pennsylvania"" title=""University of Pennsylvania"">University of Pennsylvania</a> also circulated his <i><a href=""/wiki/First_Draft_of_a_Report_on_the_EDVAC"" title=""First Draft of a Report on the EDVAC"">First Draft of a Report on the EDVAC</a></i> in 1945.<sup class=""reference"" id=""cite_ref-stanf_21-3""><a href=""#cite_note-stanf-21"">[20]</a></sup>
</p>, <p>The <a href=""/wiki/Manchester_Baby"" title=""Manchester Baby"">Manchester Baby</a> was the world's first <a href=""/wiki/Stored-program_computer"" title=""Stored-program computer"">stored-program computer</a>. It was built at the <a href=""/wiki/University_of_Manchester"" title=""University of Manchester"">University of Manchester</a> in England by <a href=""/wiki/Frederic_Calland_Williams"" title=""Frederic Calland Williams"">Frederic C. Williams</a>, <a href=""/wiki/Tom_Kilburn"" title=""Tom Kilburn"">Tom Kilburn</a> and <a href=""/wiki/Geoff_Tootill"" title=""Geoff Tootill"">Geoff Tootill</a>, and ran its first program on 21 June 1948.<sup class=""reference"" id=""cite_ref-45""><a href=""#cite_note-45"">[44]</a></sup> It was designed as a <a href=""/wiki/Testbed"" title=""Testbed"">testbed</a> for the <a href=""/wiki/Williams_tube"" title=""Williams tube"">Williams tube</a>, the first <a href=""/wiki/Random-access_memory"" title=""Random-access memory"">random-access</a> digital storage device.<sup class=""reference"" id=""cite_ref-46""><a href=""#cite_note-46"">[45]</a></sup> Although the computer was considered ""small and primitive"" by the standards of its time, it was the first working machine to contain all of the elements essential to a modern electronic computer.<sup class=""reference"" id=""cite_ref-47""><a href=""#cite_note-47"">[46]</a></sup> As soon as the Baby had demonstrated the feasibility of its design, a project was initiated at the university to develop it into a more usable computer, the <a href=""/wiki/Manchester_Mark_1"" title=""Manchester Mark 1"">Manchester Mark 1</a>. <a href=""/wiki/Grace_Hopper"" title=""Grace Hopper"">Grace Hopper</a> was the first person to develop a <a href=""/wiki/Compiler"" title=""Compiler"">compiler</a> for programming language.<sup class=""reference"" id=""cite_ref-FOOTNOTESmith20136_2-1""><a href=""#cite_note-FOOTNOTESmith20136-2"">[2]</a></sup>
</p>, <p>The Mark 1 in turn quickly became the prototype for the <a href=""/wiki/Ferranti_Mark_1"" title=""Ferranti Mark 1"">Ferranti Mark 1</a>, the world's first commercially available general-purpose computer.<sup class=""reference"" id=""cite_ref-NapperMK1_48-0""><a href=""#cite_note-NapperMK1-48"">[47]</a></sup> Built by <a href=""/wiki/Ferranti"" title=""Ferranti"">Ferranti</a>, it was delivered to the University of Manchester in February 1951. At least seven of these later machines were delivered between 1953 and 1957, one of them to <a class=""mw-redirect"" href=""/wiki/Royal_Dutch_Shell"" title=""Royal Dutch Shell"">Shell</a> labs in <a href=""/wiki/Amsterdam"" title=""Amsterdam"">Amsterdam</a>.<sup class=""reference"" id=""cite_ref-49""><a href=""#cite_note-49"">[48]</a></sup> In October 1947, the directors of British catering company <a href=""/wiki/J._Lyons_and_Co."" title=""J. Lyons and Co."">J. Lyons &amp; Company</a> decided to take an active role in promoting the commercial development of computers. The <a class=""mw-redirect"" href=""/wiki/LEO_computer"" title=""LEO computer"">LEO I</a> computer became operational in April 1951<sup class=""reference"" id=""cite_ref-50""><a href=""#cite_note-50"">[49]</a></sup> and ran the world's first regular routine office computer <a class=""mw-redirect"" href=""/wiki/Job_(software)"" title=""Job (software)"">job</a>.
</p>, <p>The concept of a <a href=""/wiki/Field-effect_transistor"" title=""Field-effect transistor"">field-effect transistor</a> was proposed by <a href=""/wiki/Julius_Edgar_Lilienfeld"" title=""Julius Edgar Lilienfeld"">Julius Edgar Lilienfeld</a> in 1925. <a href=""/wiki/John_Bardeen"" title=""John Bardeen"">John Bardeen</a> and <a class=""mw-redirect"" href=""/wiki/Walter_Brattain"" title=""Walter Brattain"">Walter Brattain</a>, while working under <a href=""/wiki/William_Shockley"" title=""William Shockley"">William Shockley</a> at <a href=""/wiki/Bell_Labs"" title=""Bell Labs"">Bell Labs</a>, built the first working <a href=""/wiki/Transistor"" title=""Transistor"">transistor</a>, the <a href=""/wiki/Point-contact_transistor"" title=""Point-contact transistor"">point-contact transistor</a>, in 1947, which was followed by Shockley's <a href=""/wiki/Bipolar_junction_transistor"" title=""Bipolar junction transistor"">bipolar junction transistor</a> in 1948.<sup class=""reference"" id=""cite_ref-Lee_51-0""><a href=""#cite_note-Lee-51"">[50]</a></sup><sup class=""reference"" id=""cite_ref-Puers_52-0""><a href=""#cite_note-Puers-52"">[51]</a></sup> From 1955 onwards, transistors replaced <a href=""/wiki/Vacuum_tube"" title=""Vacuum tube"">vacuum tubes</a> in computer designs, giving rise to the ""second generation"" of computers. Compared to vacuum tubes, transistors have many advantages: they are smaller, and require less power than vacuum tubes, so give off less heat. <a class=""mw-redirect"" href=""/wiki/Junction_transistor"" title=""Junction transistor"">Junction transistors</a> were much more reliable than vacuum tubes and had longer, indefinite, service life. Transistorized computers could contain tens of thousands of binary logic circuits in a relatively compact space. However, early junction transistors were relatively bulky devices that were difficult to manufacture on a <a class=""mw-redirect"" href=""/wiki/Mass-production"" title=""Mass-production"">mass-production</a> basis, which limited them to a number of specialised applications.<sup class=""reference"" id=""cite_ref-Moskowitz_53-0""><a href=""#cite_note-Moskowitz-53"">[52]</a></sup>
</p>, <p>At the <a href=""/wiki/University_of_Manchester"" title=""University of Manchester"">University of Manchester</a>, a team under the leadership of <a href=""/wiki/Tom_Kilburn"" title=""Tom Kilburn"">Tom Kilburn</a> designed and built a machine using the newly developed transistors instead of valves.<sup class=""reference"" id=""cite_ref-FOOTNOTELavington199834–35_54-0""><a href=""#cite_note-FOOTNOTELavington199834–35-54"">[53]</a></sup> Their first <a href=""/wiki/Transistor_computer"" title=""Transistor computer"">transistorised computer</a> and the first in the world, was <a href=""/wiki/Manchester_computers#Transistor_Computer"" title=""Manchester computers"">operational by 1953</a>, and a second version was completed there in April 1955. However, the machine did make use of valves to generate its 125 kHz clock waveforms and in the circuitry to read and write on its magnetic <a href=""/wiki/Drum_memory"" title=""Drum memory"">drum memory</a>, so it was not the first completely transistorized computer. That distinction goes to the <a href=""/wiki/Harwell_CADET"" title=""Harwell CADET"">Harwell CADET</a> of 1955,<sup class=""reference"" id=""cite_ref-ieeexplore.ieee_55-0""><a href=""#cite_note-ieeexplore.ieee-55"">[54]</a></sup> built by the electronics division of the <a href=""/wiki/Atomic_Energy_Research_Establishment"" title=""Atomic Energy Research Establishment"">Atomic Energy Research Establishment</a> at <a href=""/wiki/Harwell,_Oxfordshire"" title=""Harwell, Oxfordshire"">Harwell</a>.<sup class=""reference"" id=""cite_ref-ieeexplore.ieee_55-1""><a href=""#cite_note-ieeexplore.ieee-55"">[54]</a></sup><sup class=""reference"" id=""cite_ref-56""><a href=""#cite_note-56"">[55]</a></sup>
</p>, <p>The <a href=""/wiki/MOSFET"" title=""MOSFET"">metal–oxide–silicon field-effect transistor</a> (MOSFET), also known as the MOS transistor, was invented by <a href=""/wiki/Mohamed_M._Atalla"" title=""Mohamed M. Atalla"">Mohamed M. Atalla</a> and <a href=""/wiki/Dawon_Kahng"" title=""Dawon Kahng"">Dawon Kahng</a> at Bell Labs in 1959.<sup class=""reference"" id=""cite_ref-computerhistory_57-0""><a href=""#cite_note-computerhistory-57"">[56]</a></sup> It was the first truly compact transistor that could be miniaturised and mass-produced for a wide range of uses.<sup class=""reference"" id=""cite_ref-Moskowitz_53-1""><a href=""#cite_note-Moskowitz-53"">[52]</a></sup> With its <a class=""mw-redirect"" href=""/wiki/MOSFET_scaling"" title=""MOSFET scaling"">high scalability</a>,<sup class=""reference"" id=""cite_ref-58""><a href=""#cite_note-58"">[57]</a></sup> and much lower power consumption and higher density than bipolar junction transistors,<sup class=""reference"" id=""cite_ref-59""><a href=""#cite_note-59"">[58]</a></sup> the MOSFET made it possible to build <a class=""mw-redirect"" href=""/wiki/Very_large-scale_integration"" title=""Very large-scale integration"">high-density integrated circuits</a>.<sup class=""reference"" id=""cite_ref-computerhistory-transistor_60-0""><a href=""#cite_note-computerhistory-transistor-60"">[59]</a></sup><sup class=""reference"" id=""cite_ref-Hittinger_61-0""><a href=""#cite_note-Hittinger-61"">[60]</a></sup> In addition to data processing, it also enabled the practical use of MOS transistors as <a href=""/wiki/Memory_cell_(computing)"" title=""Memory cell (computing)"">memory cell</a> storage elements, leading to the development of MOS <a href=""/wiki/Semiconductor_memory"" title=""Semiconductor memory"">semiconductor memory</a>, which replaced earlier <a href=""/wiki/Magnetic-core_memory"" title=""Magnetic-core memory"">magnetic-core memory</a> in computers. The MOSFET led to the <a class=""mw-redirect"" href=""/wiki/Microcomputer_revolution"" title=""Microcomputer revolution"">microcomputer revolution</a>,<sup class=""reference"" id=""cite_ref-62""><a href=""#cite_note-62"">[61]</a></sup> and became the driving force behind the <a class=""mw-redirect"" href=""/wiki/Computer_revolution"" title=""Computer revolution"">computer revolution</a>.<sup class=""reference"" id=""cite_ref-63""><a href=""#cite_note-63"">[62]</a></sup><sup class=""reference"" id=""cite_ref-uspto_64-0""><a href=""#cite_note-uspto-64"">[63]</a></sup> The MOSFET is the most widely used transistor in computers,<sup class=""reference"" id=""cite_ref-kahng_65-0""><a href=""#cite_note-kahng-65"">[64]</a></sup><sup class=""reference"" id=""cite_ref-atalla_66-0""><a href=""#cite_note-atalla-66"">[65]</a></sup> and is the fundamental building block of <a href=""/wiki/Digital_electronics"" title=""Digital electronics"">digital electronics</a>.<sup class=""reference"" id=""cite_ref-triumph_67-0""><a href=""#cite_note-triumph-67"">[66]</a></sup>
</p>, <p>The next great advance in computing power came with the advent of the <a href=""/wiki/Integrated_circuit"" title=""Integrated circuit"">integrated circuit</a> (IC).
The idea of the integrated circuit was first conceived by a radar scientist working for the <a href=""/wiki/Royal_Radar_Establishment"" title=""Royal Radar Establishment"">Royal Radar Establishment</a> of the <a href=""/wiki/Ministry_of_Defence_(United_Kingdom)"" title=""Ministry of Defence (United Kingdom)"">Ministry of Defence</a>, <a href=""/wiki/Geoffrey_Dummer"" title=""Geoffrey Dummer"">Geoffrey W.A. Dummer</a>. Dummer presented the first public description of an integrated circuit at the Symposium on Progress in Quality Electronic Components in <a href=""/wiki/Washington,_D.C."" title=""Washington, D.C."">Washington, D.C.</a> on 7 May 1952.<sup class=""reference"" id=""cite_ref-68""><a href=""#cite_note-68"">[67]</a></sup>
</p>, <p>The first working ICs were invented by <a href=""/wiki/Jack_Kilby"" title=""Jack Kilby"">Jack Kilby</a> at <a href=""/wiki/Texas_Instruments"" title=""Texas Instruments"">Texas Instruments</a> and <a href=""/wiki/Robert_Noyce"" title=""Robert Noyce"">Robert Noyce</a> at <a href=""/wiki/Fairchild_Semiconductor"" title=""Fairchild Semiconductor"">Fairchild Semiconductor</a>.<sup class=""reference"" id=""cite_ref-69""><a href=""#cite_note-69"">[68]</a></sup> Kilby recorded his initial ideas concerning the integrated circuit in July 1958, successfully demonstrating the first working integrated example on 12 September 1958.<sup class=""reference"" id=""cite_ref-TIJackBuilt_70-0""><a href=""#cite_note-TIJackBuilt-70"">[69]</a></sup> In his patent application of 6 February 1959, Kilby described his new device as ""a body of semiconductor material ... wherein all the components of the electronic circuit are completely integrated"".<sup class=""reference"" id=""cite_ref-71""><a href=""#cite_note-71"">[70]</a></sup><sup class=""reference"" id=""cite_ref-72""><a href=""#cite_note-72"">[71]</a></sup> However, Kilby's invention was a <a href=""/wiki/Hybrid_integrated_circuit"" title=""Hybrid integrated circuit"">hybrid integrated circuit</a> (hybrid IC), rather than a <a class=""mw-redirect"" href=""/wiki/Monolithic_integrated_circuit"" title=""Monolithic integrated circuit"">monolithic integrated circuit</a> (IC) chip.<sup class=""reference"" id=""cite_ref-Saxena140_73-0""><a href=""#cite_note-Saxena140-73"">[72]</a></sup> Kilby's IC had external wire connections, which made it difficult to mass-produce.<sup class=""reference"" id=""cite_ref-nasa_74-0""><a href=""#cite_note-nasa-74"">[73]</a></sup>
</p>, <p>Noyce also came up with his own idea of an integrated circuit half a year later than Kilby.<sup class=""reference"" id=""cite_ref-75""><a href=""#cite_note-75"">[74]</a></sup> Noyce's invention was the first true monolithic IC chip.<sup class=""reference"" id=""cite_ref-computerhistory1959_76-0""><a href=""#cite_note-computerhistory1959-76"">[75]</a></sup><sup class=""reference"" id=""cite_ref-nasa_74-1""><a href=""#cite_note-nasa-74"">[73]</a></sup> His chip solved many practical problems that Kilby's had not. Produced at Fairchild Semiconductor, it was made of <a href=""/wiki/Silicon"" title=""Silicon"">silicon</a>, whereas Kilby's chip was made of <a href=""/wiki/Germanium"" title=""Germanium"">germanium</a>. Noyce's monolithic IC was <a href=""/wiki/Semiconductor_device_fabrication"" title=""Semiconductor device fabrication"">fabricated</a> using the <a href=""/wiki/Planar_process"" title=""Planar process"">planar process</a>, developed by his colleague <a href=""/wiki/Jean_Hoerni"" title=""Jean Hoerni"">Jean Hoerni</a> in early 1959. In turn, the planar process was based on Mohamed M. Atalla's work on semiconductor surface passivation by silicon dioxide in the late 1950s.<sup class=""reference"" id=""cite_ref-Lojek120_77-0""><a href=""#cite_note-Lojek120-77"">[76]</a></sup><sup class=""reference"" id=""cite_ref-78""><a href=""#cite_note-78"">[77]</a></sup><sup class=""reference"" id=""cite_ref-79""><a href=""#cite_note-79"">[78]</a></sup>
</p>, <p>Modern monolithic ICs are predominantly MOS (<a class=""mw-redirect"" href=""/wiki/Metal-oxide-semiconductor"" title=""Metal-oxide-semiconductor"">metal-oxide-semiconductor</a>) integrated circuits, built from <a href=""/wiki/MOSFET"" title=""MOSFET"">MOSFETs</a> (MOS transistors).<sup class=""reference"" id=""cite_ref-Kuo_80-0""><a href=""#cite_note-Kuo-80"">[79]</a></sup> The earliest experimental MOS IC to be fabricated was a 16-transistor chip built by Fred Heiman and Steven Hofstein at <a href=""/wiki/RCA"" title=""RCA"">RCA</a> in 1962.<sup class=""reference"" id=""cite_ref-computerhistory-digital_81-0""><a href=""#cite_note-computerhistory-digital-81"">[80]</a></sup> <a class=""mw-redirect"" href=""/wiki/General_Microelectronics"" title=""General Microelectronics"">General Microelectronics</a> later introduced the first commercial MOS IC in 1964,<sup class=""reference"" id=""cite_ref-82""><a href=""#cite_note-82"">[81]</a></sup> developed by Robert Norman.<sup class=""reference"" id=""cite_ref-computerhistory-digital_81-1""><a href=""#cite_note-computerhistory-digital-81"">[80]</a></sup> Following the development of the <a href=""/wiki/Self-aligned_gate"" title=""Self-aligned gate"">self-aligned gate</a> (silicon-gate) MOS transistor by Robert Kerwin, <a href=""/wiki/Donald_L._Klein"" title=""Donald L. Klein"">Donald Klein</a> and John Sarace at Bell Labs in 1967, the first <a class=""mw-redirect"" href=""/wiki/Silicon-gate"" title=""Silicon-gate"">silicon-gate</a> MOS IC with <a href=""/wiki/Self-aligned_gate"" title=""Self-aligned gate"">self-aligned gates</a> was developed by <a href=""/wiki/Federico_Faggin"" title=""Federico Faggin"">Federico Faggin</a> at Fairchild Semiconductor in 1968.<sup class=""reference"" id=""cite_ref-83""><a href=""#cite_note-83"">[82]</a></sup> The MOSFET has since become the most critical device component in modern ICs.<sup class=""reference"" id=""cite_ref-84""><a href=""#cite_note-84"">[83]</a></sup>
</p>, <p>The development of the MOS integrated circuit led to the invention of the <a href=""/wiki/Microprocessor"" title=""Microprocessor"">microprocessor</a>,<sup class=""reference"" id=""cite_ref-computerhistory1971_85-0""><a href=""#cite_note-computerhistory1971-85"">[84]</a></sup><sup class=""reference"" id=""cite_ref-Colinge2016_86-0""><a href=""#cite_note-Colinge2016-86"">[85]</a></sup> and heralded an explosion in the commercial and personal use of computers. While the subject of exactly which device was the first microprocessor is contentious, partly due to lack of agreement on the exact definition of the term ""microprocessor"", it is largely undisputed that the first single-chip microprocessor was the <a href=""/wiki/Intel_4004"" title=""Intel 4004"">Intel 4004</a>,<sup class=""reference"" id=""cite_ref-87""><a href=""#cite_note-87"">[86]</a></sup> designed and realized by Federico Faggin with his silicon-gate MOS IC technology,<sup class=""reference"" id=""cite_ref-computerhistory1971_85-1""><a href=""#cite_note-computerhistory1971-85"">[84]</a></sup> along with <a href=""/wiki/Marcian_Hoff"" title=""Marcian Hoff"">Ted Hoff</a>, <a href=""/wiki/Masatoshi_Shima"" title=""Masatoshi Shima"">Masatoshi Shima</a> and <a href=""/wiki/Stanley_Mazor"" title=""Stanley Mazor"">Stanley Mazor</a> at <a href=""/wiki/Intel"" title=""Intel"">Intel</a>.<sup class=""reference"" id=""cite_ref-89""><a href=""#cite_note-89"">[b]</a></sup><sup class=""reference"" id=""cite_ref-ieee_90-0""><a href=""#cite_note-ieee-90"">[88]</a></sup> In the early 1970s, MOS IC technology enabled the <a class=""mw-redirect"" href=""/wiki/Very_large-scale_integration"" title=""Very large-scale integration"">integration</a> of more than 10,000 transistors on a single chip.<sup class=""reference"" id=""cite_ref-Hittinger_61-1""><a href=""#cite_note-Hittinger-61"">[60]</a></sup>
</p>, <p><a class=""mw-redirect"" href=""/wiki/System_on_a_Chip"" title=""System on a Chip"">System on a Chip</a> (SoCs) are complete computers on a <a class=""mw-redirect"" href=""/wiki/Microchip"" title=""Microchip"">microchip</a> (or chip) the size of a coin.<sup class=""reference"" id=""cite_ref-networkworld.com_91-0""><a href=""#cite_note-networkworld.com-91"">[89]</a></sup> They may or may not have integrated <a href=""/wiki/Random-access_memory"" title=""Random-access memory"">RAM</a> and <a href=""/wiki/Flash_memory"" title=""Flash memory"">flash memory</a>. If not integrated, the RAM is usually placed directly above (known as <a class=""mw-redirect"" href=""/wiki/Package_on_package"" title=""Package on package"">Package on package</a>) or below (on the opposite side of the <a class=""mw-redirect"" href=""/wiki/Circuit_board"" title=""Circuit board"">circuit board</a>) the SoC, and the flash memory is usually placed right next to the SoC, this all done to improve data transfer speeds, as the data signals don't have to travel long distances. Since ENIAC in 1945, computers have advanced enormously, with modern SoCs (Such as the Snapdragon 865) being the size of a coin while also being hundreds of thousands of times more powerful than ENIAC, integrating billions of transistors, and consuming only a few watts of power.
</p>, <p>The first <a href=""/wiki/Portable_computer"" title=""Portable computer"">mobile computers</a> were heavy and ran from mains power. The 50 lb (23 kg) <a href=""/wiki/IBM_5100"" title=""IBM 5100"">IBM 5100</a> was an early example. Later portables such as the <a href=""/wiki/Osborne_1"" title=""Osborne 1"">Osborne 1</a> and <a href=""/wiki/Compaq_Portable"" title=""Compaq Portable"">Compaq Portable</a> were considerably lighter but still needed to be plugged in. The first <a href=""/wiki/Laptop"" title=""Laptop"">laptops</a>, such as the <a href=""/wiki/Grid_Compass"" title=""Grid Compass"">Grid Compass</a>, removed this requirement by incorporating batteries – and with the continued miniaturization of computing resources and advancements in portable battery life, portable computers grew in popularity in the 2000s.<sup class=""reference"" id=""cite_ref-92""><a href=""#cite_note-92"">[90]</a></sup> The same developments allowed manufacturers to integrate computing resources into cellular mobile phones by the early 2000s.
</p>, <p>These <a href=""/wiki/Smartphone"" title=""Smartphone"">smartphones</a> and <a href=""/wiki/Tablet_computer"" title=""Tablet computer"">tablets</a> run on a variety of operating systems and recently became the dominant computing device on the market.<sup class=""reference"" id=""cite_ref-93""><a href=""#cite_note-93"">[91]</a></sup> These are powered by <a class=""mw-redirect"" href=""/wiki/System_on_a_Chip"" title=""System on a Chip"">System on a Chip</a> (SoCs), which are complete computers on a microchip the size of a coin.<sup class=""reference"" id=""cite_ref-networkworld.com_91-1""><a href=""#cite_note-networkworld.com-91"">[89]</a></sup>
</p>, <p>Computers can be classified in a number of different ways, including:
</p>, <p>The term <i>hardware</i> covers all of those parts of a computer that are tangible physical objects. <a href=""/wiki/Electrical_network"" title=""Electrical network"">Circuits</a>, computer chips, graphic cards, sound cards, memory (RAM), motherboard, displays, power supplies, cables, keyboards, printers and ""mice"" input devices are all hardware.
</p>, <p>A general-purpose computer has four main components: the <a href=""/wiki/Arithmetic_logic_unit"" title=""Arithmetic logic unit"">arithmetic logic unit</a> (ALU), the <a href=""/wiki/Control_unit"" title=""Control unit"">control unit</a>, the <a href=""/wiki/Computer_data_storage"" title=""Computer data storage"">memory</a>, and the <a class=""mw-redirect"" href=""/wiki/Input_and_output_devices"" title=""Input and output devices"">input and output devices</a> (collectively termed I/O). These parts are interconnected by <a href=""/wiki/Bus_(computing)"" title=""Bus (computing)"">buses</a>, often made of groups of <a href=""/wiki/Wire"" title=""Wire"">wires</a>. Inside each of these parts are thousands to trillions of small <a href=""/wiki/Electrical_network"" title=""Electrical network"">electrical circuits</a> which can be turned off or on by means of an <a href=""/wiki/Transistor"" title=""Transistor"">electronic switch</a>. Each circuit represents a <a href=""/wiki/Bit"" title=""Bit"">bit</a> (binary digit) of information so that when the circuit is on it represents a ""1"", and when off it represents a ""0"" (in positive logic representation). The circuits are arranged in <a href=""/wiki/Logic_gate"" title=""Logic gate"">logic gates</a> so that one or more of the circuits may control the state of one or more of the other circuits.
</p>, <p>When unprocessed data is sent to the computer with the help of input devices, the data is processed and sent to output devices. The input devices may be hand-operated or automated. The act of processing is mainly regulated by the CPU. Some examples of input devices are:
</p>, <p>The means through which computer gives output are known as output devices. Some examples of output devices are:
</p>, <p>The <a href=""/wiki/Control_unit"" title=""Control unit"">control unit</a> (often called a control system or central controller) manages the computer's various components; it reads and interprets (decodes) the program instructions, transforming them into control signals that activate other parts of the computer.<sup class=""reference"" id=""cite_ref-95""><a href=""#cite_note-95"">[d]</a></sup> Control systems in advanced computers may change the order of execution of some instructions to improve performance.
</p>, <p>A key component common to all CPUs is the <a href=""/wiki/Program_counter"" title=""Program counter"">program counter</a>, a special memory cell (a <a href=""/wiki/Processor_register"" title=""Processor register"">register</a>) that keeps track of which location in memory the next instruction is to be read from.<sup class=""reference"" id=""cite_ref-96""><a href=""#cite_note-96"">[e]</a></sup>
</p>, <p>The control system's function is as follows— this is a simplified description, and some of these steps may be performed concurrently or in a different order depending on the type of CPU:
</p>, <p>Since the program counter is (conceptually) just another set of memory cells, it can be changed by calculations done in the ALU. Adding 100 to the program counter would cause the next instruction to be read from a place 100 locations further down the program. Instructions that modify the program counter are often known as ""jumps"" and allow for loops (instructions that are repeated by the computer) and often conditional instruction execution (both examples of <a href=""/wiki/Control_flow"" title=""Control flow"">control flow</a>).
</p>, <p>The sequence of operations that the control unit goes through to process an instruction is in itself like a short <a href=""/wiki/Computer_program"" title=""Computer program"">computer program</a>, and indeed, in some more complex CPU designs, there is another yet smaller computer called a <a href=""/wiki/Microsequencer"" title=""Microsequencer"">microsequencer</a>, which runs a <a href=""/wiki/Microcode"" title=""Microcode"">microcode</a> program that causes all of these events to happen.
</p>, <p>The control unit, ALU, and registers are collectively known as a <a href=""/wiki/Central_processing_unit"" title=""Central processing unit"">central processing unit</a> (CPU). Early CPUs were composed of many separate components. Since the 1970s, CPUs have typically been constructed on a single <a class=""mw-redirect"" href=""/wiki/MOS_integrated_circuit"" title=""MOS integrated circuit"">MOS integrated circuit</a> chip called a <i><a href=""/wiki/Microprocessor"" title=""Microprocessor"">microprocessor</a></i>.
</p>, <p>The ALU is capable of performing two classes of operations: arithmetic and logic.<sup class=""reference"" id=""cite_ref-97""><a href=""#cite_note-97"">[92]</a></sup> The set of arithmetic operations that a particular ALU supports may be limited to addition and subtraction, or might include multiplication, division, <a href=""/wiki/Trigonometry"" title=""Trigonometry"">trigonometry</a> functions such as sine, cosine, etc., and <a href=""/wiki/Square_root"" title=""Square root"">square roots</a>. Some can operate only on whole numbers (<a href=""/wiki/Integer"" title=""Integer"">integers</a>) while others use <a class=""mw-redirect"" href=""/wiki/Floating_point"" title=""Floating point"">floating point</a> to represent <a href=""/wiki/Real_number"" title=""Real number"">real numbers</a>, albeit with limited precision. However, any computer that is capable of performing just the simplest operations can be programmed to break down the more complex operations into simple steps that it can perform. Therefore, any computer can be programmed to perform any arithmetic operation—although it will take more time to do so if its ALU does not directly support the operation. An ALU may also compare numbers and return <a href=""/wiki/Truth_value"" title=""Truth value"">Boolean truth values</a> (true or false) depending on whether one is equal to, greater than or less than the other (""is 64 greater than 65?""). Logic operations involve <a class=""mw-redirect"" href=""/wiki/Boolean_logic"" title=""Boolean logic"">Boolean logic</a>: <a href=""/wiki/Logical_conjunction"" title=""Logical conjunction"">AND</a>, <a href=""/wiki/Logical_disjunction"" title=""Logical disjunction"">OR</a>, <a href=""/wiki/Exclusive_or"" title=""Exclusive or"">XOR</a>, and <a href=""/wiki/Negation"" title=""Negation"">NOT</a>. These can be useful for creating complicated <a class=""mw-redirect"" href=""/wiki/Conditional_(programming)"" title=""Conditional (programming)"">conditional statements</a> and processing <a class=""mw-redirect"" href=""/wiki/Boolean_logic"" title=""Boolean logic"">Boolean logic</a>.
</p>, <p><a class=""mw-redirect"" href=""/wiki/Superscalar"" title=""Superscalar"">Superscalar</a> computers may contain multiple ALUs, allowing them to process several instructions simultaneously.<sup class=""reference"" id=""cite_ref-98""><a href=""#cite_note-98"">[93]</a></sup> <a href=""/wiki/Graphics_processing_unit"" title=""Graphics processing unit"">Graphics processors</a> and computers with <a href=""/wiki/Single_instruction,_multiple_data"" title=""Single instruction, multiple data"">SIMD</a> and <a href=""/wiki/Multiple_instruction,_multiple_data"" title=""Multiple instruction, multiple data"">MIMD</a> features often contain ALUs that can perform arithmetic on <a href=""/wiki/Euclidean_vector"" title=""Euclidean vector"">vectors</a> and <a href=""/wiki/Matrix_(mathematics)"" title=""Matrix (mathematics)"">matrices</a>.
</p>, <p>A computer's memory can be viewed as a list of cells into which numbers can be placed or read. Each cell has a numbered ""address"" and can store a single number. The computer can be instructed to ""put the number 123 into the cell numbered 1357"" or to ""add the number that is in cell 1357 to the number that is in cell 2468 and put the answer into cell 1595."" The information stored in memory may represent practically anything. Letters, numbers, even computer instructions can be placed into memory with equal ease. Since the CPU does not differentiate between different types of information, it is the software's responsibility to give significance to what the memory sees as nothing but a series of numbers.
</p>, <p>In almost all modern computers, each memory cell is set up to store <a href=""/wiki/Binary_number"" title=""Binary number"">binary numbers</a> in groups of eight bits (called a <a href=""/wiki/Byte"" title=""Byte"">byte</a>). Each byte is able to represent 256 different numbers (2<sup>8</sup> = 256); either from 0 to 255 or −128 to +127. To store larger numbers, several consecutive bytes may be used (typically, two, four or eight). When negative numbers are required, they are usually stored in <a href=""/wiki/Two%27s_complement"" title=""Two's complement"">two's complement</a> notation. Other arrangements are possible, but are usually not seen outside of specialized applications or historical contexts. A computer can store any kind of information in memory if it can be represented numerically. Modern computers have billions or even trillions of bytes of memory.
</p>, <p>The CPU contains a special set of memory cells called <a href=""/wiki/Processor_register"" title=""Processor register"">registers</a> that can be read and written to much more rapidly than the main memory area. There are typically between two and one hundred registers depending on the type of CPU. Registers are used for the most frequently needed data items to avoid having to access main memory every time data is needed. As data is constantly being worked on, reducing the need to access main memory (which is often slow compared to the ALU and control units) greatly increases the computer's speed.
</p>, <p>Computer main memory comes in two principal varieties:
</p>, <p>RAM can be read and written to anytime the CPU commands it, but ROM is preloaded with data and software that never changes, therefore the CPU can only read from it. ROM is typically used to store the computer's initial start-up instructions. In general, the contents of RAM are erased when the power to the computer is turned off, but ROM retains its data indefinitely. In a PC, the ROM contains a specialized program called the <a href=""/wiki/BIOS"" title=""BIOS"">BIOS</a> that orchestrates loading the computer's <a href=""/wiki/Operating_system"" title=""Operating system"">operating system</a> from the hard disk drive into RAM whenever the computer is turned on or reset. In <a href=""/wiki/Embedded_system"" title=""Embedded system"">embedded computers</a>, which frequently do not have disk drives, all of the required software may be stored in ROM. Software stored in ROM is often called <a href=""/wiki/Firmware"" title=""Firmware"">firmware</a>, because it is notionally more like hardware than software. <a href=""/wiki/Flash_memory"" title=""Flash memory"">Flash memory</a> blurs the distinction between ROM and RAM, as it retains its data when turned off but is also rewritable. It is typically much slower than conventional ROM and RAM however, so its use is restricted to applications where high speed is unnecessary.<sup class=""reference"" id=""cite_ref-100""><a href=""#cite_note-100"">[f]</a></sup>
</p>, <p>In more sophisticated computers there may be one or more RAM <a href=""/wiki/CPU_cache"" title=""CPU cache"">cache memories</a>, which are slower than registers but faster than main memory. Generally computers with this sort of cache are designed to move frequently needed data into the cache automatically, often without the need for any intervention on the programmer's part.
</p>, <p>I/O is the means by which a computer exchanges information with the outside world.<sup class=""reference"" id=""cite_ref-101""><a href=""#cite_note-101"">[95]</a></sup> Devices that provide input or output to the computer are called <a href=""/wiki/Peripheral"" title=""Peripheral"">peripherals</a>.<sup class=""reference"" id=""cite_ref-102""><a href=""#cite_note-102"">[96]</a></sup> On a typical personal computer, peripherals include input devices like the keyboard and <a href=""/wiki/Computer_mouse"" title=""Computer mouse"">mouse</a>, and output devices such as the <a href=""/wiki/Computer_monitor"" title=""Computer monitor"">display</a> and <a href=""/wiki/Printer_(computing)"" title=""Printer (computing)"">printer</a>. <a href=""/wiki/Hard_disk_drive"" title=""Hard disk drive"">Hard disk drives</a>, <a href=""/wiki/Floppy_disk"" title=""Floppy disk"">floppy disk</a> drives and <a href=""/wiki/Optical_disc_drive"" title=""Optical disc drive"">optical disc drives</a> serve as both input and output devices. <a href=""/wiki/Computer_network"" title=""Computer network"">Computer networking</a> is another form of I/O.
I/O devices are often complex computers in their own right, with their own CPU and memory. A <a href=""/wiki/Graphics_processing_unit"" title=""Graphics processing unit"">graphics processing unit</a> might contain fifty or more tiny computers that perform the calculations necessary to display <a href=""/wiki/3D_computer_graphics"" title=""3D computer graphics"">3D graphics</a>.<sup class=""noprint Inline-Template Template-Fact"" style=""white-space:nowrap;"">[<i><a href=""/wiki/Wikipedia:Citation_needed"" title=""Wikipedia:Citation needed""><span title=""This claim needs references to reliable sources. (December 2007)"">citation needed</span></a></i>]</sup> Modern <a href=""/wiki/Desktop_computer"" title=""Desktop computer"">desktop computers</a> contain many smaller computers that assist the main CPU in performing I/O. A 2016-era flat screen display contains its own computer circuitry.
</p>, <p>While a computer may be viewed as running one gigantic program stored in its main memory, in some systems it is necessary to give the appearance of running several programs simultaneously. This is achieved by multitasking i.e. having the computer switch rapidly between running each program in turn.<sup class=""reference"" id=""cite_ref-103""><a href=""#cite_note-103"">[97]</a></sup> One means by which this is done is with a special signal called an <a href=""/wiki/Interrupt"" title=""Interrupt"">interrupt</a>, which can periodically cause the computer to stop executing instructions where it was and do something else instead. By remembering where it was executing prior to the interrupt, the computer can return to that task later. If several programs are running ""at the same time"". then the interrupt generator might be causing several hundred interrupts per second, causing a program switch each time. Since modern computers typically execute instructions several orders of magnitude faster than human perception, it may appear that many programs are running at the same time even though only one is ever executing in any given instant. This method of multitasking is sometimes termed ""time-sharing"" since each program is allocated a ""slice"" of time in turn.<sup class=""reference"" id=""cite_ref-104""><a href=""#cite_note-104"">[98]</a></sup>
</p>, <p>Before the era of inexpensive computers, the principal use for multitasking was to allow many people to share the same computer. Seemingly, multitasking would cause a computer that is switching between several programs to run more slowly, in direct proportion to the number of programs it is running, but most programs spend much of their time waiting for slow input/output devices to complete their tasks. If a program is waiting for the user to click on the mouse or press a key on the keyboard, then it will not take a ""time slice"" until the <a href=""/wiki/Event_(computing)"" title=""Event (computing)"">event</a> it is waiting for has occurred. This frees up time for other programs to execute so that many programs may be run simultaneously without unacceptable speed loss.
</p>, <p>Some computers are designed to distribute their work across several CPUs in a multiprocessing configuration, a technique once employed in only large and powerful machines such as <a href=""/wiki/Supercomputer"" title=""Supercomputer"">supercomputers</a>, <a href=""/wiki/Mainframe_computer"" title=""Mainframe computer"">mainframe computers</a> and <a href=""/wiki/Server_(computing)"" title=""Server (computing)"">servers</a>. Multiprocessor and <a class=""mw-redirect"" href=""/wiki/Multi-core"" title=""Multi-core"">multi-core</a> (multiple CPUs on a single integrated circuit) personal and laptop computers are now widely available, and are being increasingly used in lower-end markets as a result.
</p>, <p>Supercomputers in particular often have highly unique architectures that differ significantly from the basic stored-program architecture and from general-purpose computers.<sup class=""reference"" id=""cite_ref-106""><a href=""#cite_note-106"">[g]</a></sup> They often feature thousands of CPUs, customized high-speed interconnects, and specialized computing hardware. Such designs tend to be useful for only specialized tasks due to the large scale of program organization required to successfully utilize most of the available resources at once. Supercomputers usually see usage in large-scale <a href=""/wiki/Computer_simulation"" title=""Computer simulation"">simulation</a>, <a href=""/wiki/Rendering_(computer_graphics)"" title=""Rendering (computer graphics)"">graphics rendering</a>, and <a href=""/wiki/Cryptography"" title=""Cryptography"">cryptography</a> applications, as well as with other so-called ""<a href=""/wiki/Embarrassingly_parallel"" title=""Embarrassingly parallel"">embarrassingly parallel</a>"" tasks.
</p>, <p><i>Software</i> refers to parts of the computer which do not have a material form, such as programs, data, protocols, etc. Software is that part of a computer system that consists of encoded information or computer instructions, in contrast to the physical <a href=""/wiki/Computer_hardware"" title=""Computer hardware"">hardware</a> from which the system is built. Computer software includes <a href=""/wiki/Computer_program"" title=""Computer program"">computer programs</a>, <a href=""/wiki/Library_(computing)"" title=""Library (computing)"">libraries</a> and related non-executable <a href=""/wiki/Data_(computing)"" title=""Data (computing)"">data</a>, such as <a href=""/wiki/Software_documentation"" title=""Software documentation"">online documentation</a> or <a href=""/wiki/Digital_media"" title=""Digital media"">digital media</a>. It is often divided into <a href=""/wiki/System_software"" title=""System software"">system software</a> and <a href=""/wiki/Application_software"" title=""Application software"">application software</a> Computer hardware and software require each other and neither can be realistically used on its own. When software is stored in hardware that cannot easily be modified, such as with <a href=""/wiki/BIOS"" title=""BIOS"">BIOS</a> <a href=""/wiki/Read-only_memory"" title=""Read-only memory"">ROM</a> in an <a href=""/wiki/IBM_PC_compatible"" title=""IBM PC compatible"">IBM PC compatible</a> computer, it is sometimes called ""firmware"".
</p>, <p>There are thousands of different programming languages—some intended for general purpose, others useful for only highly specialized applications.
</p>, <p>The defining feature of modern computers which distinguishes them from all other machines is that they can be <a href=""/wiki/Computer_programming"" title=""Computer programming"">programmed</a>. That is to say that some type of <a class=""mw-redirect"" href=""/wiki/Instruction_(computer_science)"" title=""Instruction (computer science)"">instructions</a> (the <a href=""/wiki/Computer_program"" title=""Computer program"">program</a>) can be given to the computer, and it will process them. Modern computers based on the <a href=""/wiki/Von_Neumann_architecture"" title=""Von Neumann architecture"">von Neumann architecture</a> often have machine code in the form of an <a class=""mw-redirect"" href=""/wiki/Imperative_programming_language"" title=""Imperative programming language"">imperative programming language</a>. In practical terms, a computer program may be just a few instructions or extend to many millions of instructions, as do the programs for <a href=""/wiki/Word_processor"" title=""Word processor"">word processors</a> and <a href=""/wiki/Web_browser"" title=""Web browser"">web browsers</a> for example. A typical modern computer can execute billions of instructions per second (<a href=""/wiki/FLOPS"" title=""FLOPS"">gigaflops</a>) and rarely makes a mistake over many years of operation. Large computer programs consisting of several million instructions may take teams of <a href=""/wiki/Programmer"" title=""Programmer"">programmers</a> years to write, and due to the complexity of the task almost certainly contain errors.
</p>, <p>This section applies to most common <a class=""mw-redirect"" href=""/wiki/RAM_machine"" title=""RAM machine"">RAM machine</a>–based computers.
</p>, <p>In most cases, computer instructions are simple: add one number to another, move some data from one location to another, send a message to some external device, etc. These instructions are read from the computer's <a href=""/wiki/Computer_data_storage"" title=""Computer data storage"">memory</a> and are generally carried out (<a href=""/wiki/Execution_(computing)"" title=""Execution (computing)"">executed</a>) in the order they were given. However, there are usually specialized instructions to tell the computer to jump ahead or backwards to some other place in the program and to carry on executing from there. These are called ""jump"" instructions (or <a href=""/wiki/Branch_(computer_science)"" title=""Branch (computer science)"">branches</a>). Furthermore, jump instructions may be made to happen <a class=""mw-redirect"" href=""/wiki/Conditional_(programming)"" title=""Conditional (programming)"">conditionally</a> so that different sequences of instructions may be used depending on the result of some previous calculation or some external event. Many computers directly support <a href=""/wiki/Subroutine"" title=""Subroutine"">subroutines</a> by providing a type of jump that ""remembers"" the location it jumped from and another instruction to return to the instruction following that jump instruction.
</p>, <p>Program execution might be likened to reading a book. While a person will normally read each word and line in sequence, they may at times jump back to an earlier place in the text or skip sections that are not of interest. Similarly, a computer may sometimes go back and repeat the instructions in some section of the program over and over again until some internal condition is met. This is called the <a href=""/wiki/Control_flow"" title=""Control flow"">flow of control</a> within the program and it is what allows the computer to perform tasks repeatedly without human intervention.
</p>, <p>Comparatively, a person using a pocket <a href=""/wiki/Calculator"" title=""Calculator"">calculator</a> can perform a basic arithmetic operation such as adding two numbers with just a few button presses. But to add together all of the numbers from 1 to 1,000 would take thousands of button presses and a lot of time, with a near certainty of making a mistake. On the other hand, a computer may be programmed to do this with just a few simple instructions. The following example is written in the <a href=""/wiki/MIPS_architecture"" title=""MIPS architecture"">MIPS assembly language</a>:
</p>, <p>Once told to run this program, the computer will perform the repetitive addition task without further human intervention. It will almost never make a mistake and a modern PC can complete the task in a fraction of a second.
</p>, <p>In most computers, individual instructions are stored as <a href=""/wiki/Machine_code"" title=""Machine code"">machine code</a> with each instruction being given a unique number (its operation code or <a href=""/wiki/Opcode"" title=""Opcode"">opcode</a> for short). The command to add two numbers together would have one opcode; the command to multiply them would have a different opcode, and so on. The simplest computers are able to perform any of a handful of different instructions; the more complex computers have several hundred to choose from, each with a unique numerical code. Since the computer's memory is able to store numbers, it can also store the instruction codes. This leads to the important fact that entire programs (which are just lists of these instructions) can be represented as lists of numbers and can themselves be manipulated inside the computer in the same way as numeric data. The fundamental concept of storing programs in the computer's memory alongside the data they operate on is the crux of the von Neumann, or stored program<sup class=""noprint Inline-Template Template-Fact"" style=""white-space:nowrap;"">[<i><a href=""/wiki/Wikipedia:Citation_needed"" title=""Wikipedia:Citation needed""><span title=""This claim needs references to reliable sources. (May 2014)"">citation needed</span></a></i>]</sup>, architecture. In some cases, a computer might store some or all of its program in memory that is kept separate from the data it operates on. This is called the <a href=""/wiki/Harvard_architecture"" title=""Harvard architecture"">Harvard architecture</a> after the <a href=""/wiki/Harvard_Mark_I"" title=""Harvard Mark I"">Harvard Mark I</a> computer. Modern von Neumann computers display some traits of the Harvard architecture in their designs, such as in <a href=""/wiki/CPU_cache"" title=""CPU cache"">CPU caches</a>.
</p>, <p>While it is possible to write computer programs as long lists of numbers (<a href=""/wiki/Machine_code"" title=""Machine code"">machine language</a>) and while this technique was used with many early computers,<sup class=""reference"" id=""cite_ref-107""><a href=""#cite_note-107"">[h]</a></sup> it is extremely tedious and potentially error-prone to do so in practice, especially for complicated programs. Instead, each basic instruction can be given a short name that is indicative of its function and easy to remember – a <a href=""/wiki/Mnemonic"" title=""Mnemonic"">mnemonic</a> such as ADD, SUB, MULT or JUMP. These mnemonics are collectively known as a computer's <a href=""/wiki/Assembly_language"" title=""Assembly language"">assembly language</a>. Converting programs written in assembly language into something the computer can actually understand (machine language) is usually done by a computer program called an assembler.
</p>, <p>Programming languages provide various ways of specifying programs for computers to run. Unlike <a href=""/wiki/Natural_language"" title=""Natural language"">natural languages</a>, programming languages are designed to permit no ambiguity and to be concise. They are purely written languages and are often difficult to read aloud. They are generally either translated into <a href=""/wiki/Machine_code"" title=""Machine code"">machine code</a> by a <a href=""/wiki/Compiler"" title=""Compiler"">compiler</a> or an <a href=""/wiki/Assembly_language#Assembler"" title=""Assembly language"">assembler</a> before being run, or translated directly at run time by an <a href=""/wiki/Interpreter_(computing)"" title=""Interpreter (computing)"">interpreter</a>. Sometimes programs are executed by a hybrid method of the two techniques.
</p>, <p>Machine languages and the assembly languages that represent them (collectively termed <i>low-level programming languages</i>) are generally unique to the particular architecture of a computer's central processing unit (<a class=""mw-redirect"" href=""/wiki/CPU"" title=""CPU"">CPU</a>). For instance, an <a class=""mw-redirect"" href=""/wiki/ARM_architecture"" title=""ARM architecture"">ARM architecture</a> CPU (such as may be found in a <a href=""/wiki/Smartphone"" title=""Smartphone"">smartphone</a> or a <a class=""mw-redirect"" href=""/wiki/Handheld_video_game"" title=""Handheld video game"">hand-held videogame</a>) cannot understand the machine language of an <a href=""/wiki/X86"" title=""X86"">x86</a> CPU that might be in a <a href=""/wiki/Personal_computer"" title=""Personal computer"">PC</a>.<sup class=""reference"" id=""cite_ref-108""><a href=""#cite_note-108"">[i]</a></sup> Historically a significant number of other cpu architectures were created and saw extensive use, notably including the MOS Technology 6502 and 6510 in addition to the Zilog Z80.
</p>, <p>Although considerably easier than in machine language, writing long programs in assembly language is often difficult and is also error prone. Therefore, most practical programs are written in more abstract <a href=""/wiki/High-level_programming_language"" title=""High-level programming language"">high-level programming languages</a> that are able to express the needs of the <a href=""/wiki/Programmer"" title=""Programmer"">programmer</a> more conveniently (and thereby help reduce programmer error). High level languages are usually ""compiled"" into machine language (or sometimes into assembly language and then into machine language) using another computer program called a <a href=""/wiki/Compiler"" title=""Compiler"">compiler</a>.<sup class=""reference"" id=""cite_ref-109""><a href=""#cite_note-109"">[j]</a></sup> High level languages are less related to the workings of the target computer than assembly language, and more related to the language and structure of the problem(s) to be solved by the final program. It is therefore often possible to use different compilers to translate the same high level language program into the machine language of many different types of computer. This is part of the means by which software like video games may be made available for different computer architectures such as personal computers and various <a href=""/wiki/Video_game_console"" title=""Video game console"">video game consoles</a>.
</p>, <p>Program design of small programs is relatively simple and involves the analysis of the problem, collection of inputs, using the programming constructs within languages, devising or using established procedures and algorithms, providing data for output devices and solutions to the problem as applicable. As problems become larger and more complex, features such as subprograms, modules, formal documentation, and new paradigms such as object-oriented programming are encountered. Large programs involving thousands of line of code and more require formal software methodologies.
The task of developing large <a class=""mw-redirect"" href=""/wiki/Computer_software"" title=""Computer software"">software</a> systems presents a significant intellectual challenge. Producing software with an acceptably high reliability within a predictable schedule and budget has historically been difficult; the academic and professional discipline of <a href=""/wiki/Software_engineering"" title=""Software engineering"">software engineering</a> concentrates specifically on this challenge.
</p>, <p>Errors in computer programs are called ""<a href=""/wiki/Software_bug"" title=""Software bug"">bugs</a>"". They may be benign and not affect the usefulness of the program, or have only subtle effects. But in some cases, they may cause the program or the entire system to ""<a href=""/wiki/Hang_(computing)"" title=""Hang (computing)"">hang</a>"", becoming unresponsive to input such as <a href=""/wiki/Computer_mouse"" title=""Computer mouse"">mouse</a> clicks or keystrokes, to completely fail, or to <a href=""/wiki/Crash_(computing)"" title=""Crash (computing)"">crash</a>.<sup class=""reference"" id=""cite_ref-110""><a href=""#cite_note-110"">[100]</a></sup> Otherwise benign bugs may sometimes be harnessed for malicious intent by an unscrupulous user writing an <a href=""/wiki/Exploit_(computer_security)"" title=""Exploit (computer security)"">exploit</a>, code designed to take advantage of a bug and disrupt a computer's proper execution. Bugs are usually not the fault of the computer. Since computers merely execute the instructions they are given, bugs are nearly always the result of programmer error or an oversight made in the program's design.<sup class=""reference"" id=""cite_ref-111""><a href=""#cite_note-111"">[k]</a></sup> Admiral <a href=""/wiki/Grace_Hopper"" title=""Grace Hopper"">Grace Hopper</a>, an American computer scientist and developer of the first <a href=""/wiki/Compiler"" title=""Compiler"">compiler</a>, is credited for having first used the term ""bugs"" in computing after a dead moth was found shorting a relay in the <a href=""/wiki/Harvard_Mark_II"" title=""Harvard Mark II"">Harvard Mark II</a> computer in September 1947.<sup class=""reference"" id=""cite_ref-taylor84_112-0""><a href=""#cite_note-taylor84-112"">[101]</a></sup>
</p>, <p>Computers have been used to coordinate information between multiple locations since the 1950s. The U.S. military's <a href=""/wiki/Semi-Automatic_Ground_Environment"" title=""Semi-Automatic Ground Environment"">SAGE</a> system was the first large-scale example of such a system, which led to a number of special-purpose commercial systems such as <a href=""/wiki/Sabre_(computer_system)"" title=""Sabre (computer system)"">Sabre</a>.<sup class=""reference"" id=""cite_ref-113""><a href=""#cite_note-113"">[102]</a></sup> In the 1970s, computer engineers at research institutions throughout the United States began to link their computers together using telecommunications technology. The effort was funded by ARPA (now <a href=""/wiki/DARPA"" title=""DARPA"">DARPA</a>), and the <a href=""/wiki/Computer_network"" title=""Computer network"">computer network</a> that resulted was called the <a href=""/wiki/ARPANET"" title=""ARPANET"">ARPANET</a>.<sup class=""reference"" id=""cite_ref-114""><a href=""#cite_note-114"">[103]</a></sup> The technologies that made the Arpanet possible spread and evolved.
</p>, <p>In time, the network spread beyond academic and military institutions and became known as the Internet. The emergence of networking involved a redefinition of the nature and boundaries of the computer. Computer operating systems and applications were modified to include the ability to define and access the resources of other computers on the network, such as peripheral devices, stored information, and the like, as extensions of the resources of an individual computer. Initially these facilities were available primarily to people working in high-tech environments, but in the 1990s the spread of applications like e-mail and the <a href=""/wiki/World_Wide_Web"" title=""World Wide Web"">World Wide Web</a>, combined with the development of cheap, fast networking technologies like <a href=""/wiki/Ethernet"" title=""Ethernet"">Ethernet</a> and <a href=""/wiki/Asymmetric_digital_subscriber_line"" title=""Asymmetric digital subscriber line"">ADSL</a> saw computer networking become almost ubiquitous. In fact, the number of computers that are networked is growing phenomenally. A very large proportion of personal computers regularly connect to the Internet to communicate and receive information. ""Wireless"" networking, often utilizing mobile phone networks, has meant networking is becoming increasingly ubiquitous even in mobile computing environments.
</p>, <p>A computer does not need to be <a href=""/wiki/Electronics"" title=""Electronics"">electronic</a>, nor even have a <a href=""/wiki/Central_processing_unit"" title=""Central processing unit"">processor</a>, nor <a href=""/wiki/Random-access_memory"" title=""Random-access memory"">RAM</a>, nor even a <a class=""mw-redirect"" href=""/wiki/Hard_disk"" title=""Hard disk"">hard disk</a>. While popular usage of the word ""computer"" is synonymous with a personal electronic computer,<sup class=""reference"" id=""cite_ref-115""><a href=""#cite_note-115"">[l]</a></sup> the modern definition of a computer is literally: ""<i>A device that computes</i>, especially a programmable [usually] electronic machine that performs high-speed mathematical or logical operations or that assembles, stores, correlates, or otherwise processes information.""<sup class=""reference"" id=""cite_ref-116""><a href=""#cite_note-116"">[104]</a></sup> Any device which <i>processes information</i> qualifies as a computer, especially if the processing is purposeful.<sup class=""noprint Inline-Template Template-Fact"" style=""white-space:nowrap;"">[<i><a href=""/wiki/Wikipedia:Citation_needed"" title=""Wikipedia:Citation needed""><span title=""This claim needs references to reliable sources. (September 2015)"">citation needed</span></a></i>]</sup>
</p>, <p>There is active research to make computers out of many promising new types of technology, such as <a href=""/wiki/Optical_computing"" title=""Optical computing"">optical computers</a>, <a href=""/wiki/DNA_computing"" title=""DNA computing"">DNA computers</a>, <a href=""/wiki/Wetware_computer"" title=""Wetware computer"">neural computers</a>, and <a href=""/wiki/Quantum_computing"" title=""Quantum computing"">quantum computers</a>. Most computers are universal, and are able to calculate any <a href=""/wiki/Computable_function"" title=""Computable function"">computable function</a>, and are limited only by their memory capacity and operating speed. However different designs of computers can give very different performance for particular problems; for example quantum computers can potentially break some modern encryption algorithms (by <a href=""/wiki/Shor%27s_algorithm"" title=""Shor's algorithm"">quantum factoring</a>) very quickly.
</p>, <p>There are many types of <a href=""/wiki/Computer_architecture"" title=""Computer architecture"">computer architectures</a>:
</p>, <p>Of all these <a href=""/wiki/Abstract_machine"" title=""Abstract machine"">abstract machines</a>, a quantum computer holds the most promise for revolutionizing computing.<sup class=""reference"" id=""cite_ref-117""><a href=""#cite_note-117"">[105]</a></sup> <a href=""/wiki/Logic_gate"" title=""Logic gate"">Logic gates</a> are a common abstraction which can apply to most of the above <a href=""/wiki/Digital_data"" title=""Digital data"">digital</a> or <a href=""/wiki/Analog_signal"" title=""Analog signal"">analog</a> paradigms. The ability to store and execute lists of instructions called <a href=""/wiki/Computer_program"" title=""Computer program"">programs</a> makes computers extremely versatile, distinguishing them from <a href=""/wiki/Calculator"" title=""Calculator"">calculators</a>. The <a href=""/wiki/Church%E2%80%93Turing_thesis"" title=""Church–Turing thesis"">Church–Turing thesis</a> is a mathematical statement of this versatility: any computer with a <a class=""mw-redirect"" href=""/wiki/Turing-complete"" title=""Turing-complete"">minimum capability (being Turing-complete)</a> is, in principle, capable of performing the same tasks that any other computer can perform. Therefore, any type of computer (<a href=""/wiki/Netbook"" title=""Netbook"">netbook</a>, <a href=""/wiki/Supercomputer"" title=""Supercomputer"">supercomputer</a>, <a href=""/wiki/Cellular_automaton"" title=""Cellular automaton"">cellular automaton</a>, etc.) is able to perform the same computational tasks, given enough time and storage capacity.
</p>, <p>A computer will solve problems in exactly the way it is programmed to, without regard to efficiency, alternative solutions, possible shortcuts, or possible errors in the code. Computer programs that learn and adapt are part of the emerging field of <a href=""/wiki/Artificial_intelligence"" title=""Artificial intelligence"">artificial intelligence</a> and <a href=""/wiki/Machine_learning"" title=""Machine learning"">machine learning</a>. Artificial intelligence based products generally fall into two major categories: <a href=""/wiki/Rule-based_system"" title=""Rule-based system"">rule-based systems</a> and <a href=""/wiki/Pattern_recognition"" title=""Pattern recognition"">pattern recognition</a> systems. Rule-based systems attempt to represent the rules used by human experts and tend to be expensive to develop. Pattern-based systems use data about a problem to generate conclusions. Examples of pattern-based systems include <a href=""/wiki/Speech_recognition"" title=""Speech recognition"">voice recognition</a>, font recognition, translation and the emerging field of on-line marketing.
</p>, <p>As the use of computers has spread throughout society, there are an increasing number of careers involving computers.
</p>, <p>The need for computers to work well together and to be able to exchange information has spawned the need for many standards organizations, clubs and societies of both a formal and informal nature.
</p>, <p class=""mw-empty-elt"">
</p>]"
Microsoft Windows,"[<p class=""mw-empty-elt"">
</p>, <p><b>Microsoft Windows</b>, commonly referred to as <b>Windows</b>, is a group of several <a href=""/wiki/Proprietary_software"" title=""Proprietary software"">proprietary</a> <a href=""/wiki/Graphical_user_interface"" title=""Graphical user interface"">graphical</a> <a href=""/wiki/Operating_system"" title=""Operating system"">operating system</a> families, all of which are developed and marketed by <a href=""/wiki/Microsoft"" title=""Microsoft"">Microsoft</a>. Each family caters to a certain sector of the computing industry. Active Microsoft Windows families include <a href=""/wiki/Windows_NT"" title=""Windows NT"">Windows NT</a> and <a href=""/wiki/Windows_IoT"" title=""Windows IoT"">Windows IoT</a>; these may encompass subfamilies, (e.g. <a href=""/wiki/Windows_Server"" title=""Windows Server"">Windows Server</a> or <a href=""/wiki/Windows_Embedded_Compact"" title=""Windows Embedded Compact"">Windows Embedded Compact</a>) (Windows CE). Defunct Microsoft Windows families include <a href=""/wiki/Windows_9x"" title=""Windows 9x"">Windows 9x</a>, <a href=""/wiki/Windows_Mobile"" title=""Windows Mobile"">Windows Mobile</a> and <a href=""/wiki/Windows_Phone"" title=""Windows Phone"">Windows Phone</a>.
</p>, <p>Microsoft introduced an <a href=""/wiki/Operating_environment"" title=""Operating environment"">operating environment</a> named <i>Windows</i> on November 20, 1985, as a graphical <a class=""mw-redirect"" href=""/wiki/Operating_system_shell"" title=""Operating system shell"">operating system shell</a> for <a href=""/wiki/MS-DOS"" title=""MS-DOS"">MS-DOS</a> in response to the growing interest in <a href=""/wiki/Graphical_user_interface"" title=""Graphical user interface"">graphical user interfaces</a> (GUIs).<sup class=""reference"" id=""cite_ref-aboutcomnov_4-0""><a href=""#cite_note-aboutcomnov-4"">[4]</a></sup> Microsoft Windows came to <a href=""/wiki/Dominance_(economics)"" title=""Dominance (economics)"">dominate</a> the world's <a href=""/wiki/Personal_computer"" title=""Personal computer"">personal computer</a> (PC) market with <a href=""/wiki/Usage_share_of_operating_systems"" title=""Usage share of operating systems"">over 90% market share</a>, overtaking <a href=""/wiki/Classic_Mac_OS"" title=""Classic Mac OS"">Mac OS</a>, which had been introduced in 1984.
</p>, <p><a href=""/wiki/Apple_Inc."" title=""Apple Inc."">Apple</a> came to see Windows as an unfair encroachment on their innovation in GUI development as implemented on products such as the <a href=""/wiki/Apple_Lisa"" title=""Apple Lisa"">Lisa</a> and <a href=""/wiki/Macintosh"" title=""Macintosh"">Macintosh</a> (eventually settled in court in Microsoft's favor in 1993).  On PCs, Windows is still the most popular operating system in all countries.<sup class=""reference"" id=""cite_ref-5""><a href=""#cite_note-5"">[5]</a></sup><sup class=""reference"" id=""cite_ref-6""><a href=""#cite_note-6"">[6]</a></sup> However, in 2014, Microsoft admitted losing the majority of the overall operating system market to <a href=""/wiki/Android_(operating_system)"" title=""Android (operating system)"">Android</a>,<sup class=""reference"" id=""cite_ref-7""><a href=""#cite_note-7"">[7]</a></sup> because of the massive growth in sales of Android <a href=""/wiki/Smartphone"" title=""Smartphone"">smartphones</a>. In 2014, the number of Windows devices sold was less than 25% that of Android devices sold. This comparison, however, may not be fully relevant, as the two operating systems traditionally target different platforms. Still, numbers for server use of Windows (that are comparable to competitors) show one third market share, similar to that for end user use. 
</p>, <p>As of October 2021<sup class=""plainlinks noexcerpt noprint asof-tag update"" style=""display:none;""><a class=""external text"" href=""https://en.wikipedia.org/w/index.php?title=Microsoft_Windows&amp;action=edit"">[update]</a></sup>, the most recent version of Windows for PCs and <a class=""mw-redirect"" href=""/wiki/Tablet_computers"" title=""Tablet computers"">tablets</a> is <a href=""/wiki/Windows_11"" title=""Windows 11"">Windows 11</a>, version 21H2. The most recent version for <a href=""/wiki/Embedded_system"" title=""Embedded system"">embedded devices</a> is <a href=""/wiki/Windows_10"" title=""Windows 10"">Windows 10</a>, version 21H1. The most recent version for <a href=""/wiki/Server_(computing)"" title=""Server (computing)"">server computers</a> is <a href=""/wiki/Windows_Server_2022"" title=""Windows Server 2022"">Windows Server 2022</a>, version 21H2.<sup class=""reference"" id=""cite_ref-8""><a href=""#cite_note-8"">[8]</a></sup> <a class=""mw-redirect"" href=""/wiki/Xbox_One_system_software"" title=""Xbox One system software"">A specialized version of Windows</a> also runs on the <a href=""/wiki/Xbox_One"" title=""Xbox One"">Xbox One</a> and <a href=""/wiki/Xbox_Series_X_and_Series_S"" title=""Xbox Series X and Series S"">Xbox Series X/S</a> <a href=""/wiki/Video_game_console"" title=""Video game console"">video game consoles</a>.<sup class=""reference"" id=""cite_ref-9""><a href=""#cite_note-9"">[9]</a></sup>
</p>, <p>Microsoft, the developer of Windows, has registered several trademarks, each of which denotes a family of Windows operating systems that target a specific sector of the computing industry. As of 2014, the following Windows families were being actively developed:
</p>, <p>The following Windows families are no longer being developed:
</p>, <p>The term <i>Windows</i> collectively describes any or all of several generations of <a href=""/wiki/Microsoft"" title=""Microsoft"">Microsoft</a> <a href=""/wiki/Operating_system"" title=""Operating system"">operating system</a> products. These products are generally categorized as follows:
</p>, <p>The history of Windows dates back to 1981 when Microsoft started work on a program called ""Interface Manager"". It was announced in November 1983 (after the <a href=""/wiki/Apple_Lisa"" title=""Apple Lisa"">Apple Lisa</a>, but before the <a href=""/wiki/Macintosh"" title=""Macintosh"">Macintosh</a>) under the name ""Windows"", but <a class=""mw-redirect"" href=""/wiki/Windows_1.0"" title=""Windows 1.0"">Windows 1.0</a> was not released until November 1985.<sup class=""reference"" id=""cite_ref-12""><a href=""#cite_note-12"">[12]</a></sup> Windows 1.0 was to compete with <a href=""/wiki/Apple_Inc."" title=""Apple Inc."">Apple</a>'s operating system, but achieved little popularity.  Windows 1.0 is not a complete operating system; rather, it extends <a href=""/wiki/MS-DOS"" title=""MS-DOS"">MS-DOS</a>. The shell of Windows 1.0 is a program known as the <a class=""mw-redirect"" href=""/wiki/MS-DOS_Executive"" title=""MS-DOS Executive"">MS-DOS Executive</a>. Components included <a class=""mw-redirect"" href=""/wiki/Microsoft_Calculator"" title=""Microsoft Calculator"">Calculator</a>, Calendar, <a href=""/wiki/Cardfile"" title=""Cardfile"">Cardfile</a>, <a href=""/wiki/ClipBook_Viewer"" title=""ClipBook Viewer"">Clipboard Viewer</a>, Clock, <a href=""/wiki/Control_Panel_(Windows)"" title=""Control Panel (Windows)"">Control Panel</a>, <a class=""mw-redirect"" href=""/wiki/Notepad_(Windows)"" title=""Notepad (Windows)"">Notepad</a>, <a href=""/wiki/Microsoft_Paint"" title=""Microsoft Paint"">Paint</a>, <a href=""/wiki/Reversi"" title=""Reversi"">Reversi</a>, <a href=""/wiki/Terminal_emulator"" title=""Terminal emulator"">Terminal</a> and <a class=""mw-redirect"" href=""/wiki/Windows_Write"" title=""Windows Write"">Write</a>. Windows 1.0 does not allow overlapping windows. Instead all windows are <a href=""/wiki/Tiling_window_manager"" title=""Tiling window manager"">tiled</a>. Only modal dialog boxes may appear over other windows. Microsoft sold as included Windows Development libraries with the C development environment, which included numerous windows samples.<sup class=""reference"" id=""cite_ref-13""><a href=""#cite_note-13"">[13]</a></sup>
</p>, <p><a class=""mw-redirect"" href=""/wiki/Windows_2.0"" title=""Windows 2.0"">Windows 2.0</a> was released in December 1987, and was more popular than its predecessor. It features several improvements to the user interface and memory management.<sup class=""reference"" id=""cite_ref-14""><a href=""#cite_note-14"">[14]</a></sup> Windows 2.03 changed the OS from tiled windows to overlapping windows. The result of this change led to <a href=""/wiki/Apple_Computer,_Inc._v._Microsoft_Corp."" title=""Apple Computer, Inc. v. Microsoft Corp."">Apple Computer filing a suit against Microsoft</a> alleging infringement on Apple's copyrights.<sup class=""reference"" id=""cite_ref-15""><a href=""#cite_note-15"">[15]</a></sup><sup class=""reference"" id=""cite_ref-16""><a href=""#cite_note-16"">[16]</a></sup> Windows 2.0 also introduced more sophisticated <a href=""/wiki/Keyboard_shortcut"" title=""Keyboard shortcut"">keyboard shortcuts</a> and could make use of <a href=""/wiki/Expanded_memory"" title=""Expanded memory"">expanded memory</a>.
</p>, <p>Windows 2.1 was released in two different versions: <a class=""mw-redirect"" href=""/wiki/Windows/286"" title=""Windows/286"">Windows/286</a> and <a class=""mw-redirect"" href=""/wiki/Windows/386"" title=""Windows/386"">Windows/386</a>. Windows/386 uses the <a href=""/wiki/Virtual_8086_mode"" title=""Virtual 8086 mode"">virtual 8086 mode</a> of the <a class=""mw-redirect"" href=""/wiki/Intel_80386"" title=""Intel 80386"">Intel 80386</a> to multitask several DOS programs and the <a class=""mw-redirect"" href=""/wiki/Paging"" title=""Paging"">paged memory model</a> to emulate expanded memory using available <a href=""/wiki/Extended_memory"" title=""Extended memory"">extended memory</a>. Windows/286, in spite of its name, runs on both <a href=""/wiki/Intel_8086"" title=""Intel 8086"">Intel 8086</a> and <a href=""/wiki/Intel_80286"" title=""Intel 80286"">Intel 80286</a> processors. It runs in <a href=""/wiki/Real_mode"" title=""Real mode"">real mode</a> but can make use of the <a href=""/wiki/High_memory_area"" title=""High memory area"">high memory area</a>.<sup class=""noprint Inline-Template Template-Fact"" style=""white-space:nowrap;"">[<i><a href=""/wiki/Wikipedia:Citation_needed"" title=""Wikipedia:Citation needed""><span title=""This claim needs references to reliable sources. (October 2015)"">citation needed</span></a></i>]</sup>
</p>, <p>In addition to full Windows-packages, there were runtime-only versions that shipped with early Windows software from third parties and made it possible to run their Windows software on MS-DOS and without the full Windows feature set.
</p>, <p>The early versions of Windows are often thought of as graphical shells, mostly because they ran on top of MS-DOS and use it for <a href=""/wiki/File_system"" title=""File system"">file system</a> services.<sup class=""reference"" id=""cite_ref-Evolution_17-0""><a href=""#cite_note-Evolution-17"">[17]</a></sup> However, even the earliest Windows versions already assumed many typical operating system functions; notably, having their own <a class=""mw-redirect"" href=""/wiki/Executable_file_format"" title=""Executable file format"">executable file format</a> and providing their own <a href=""/wiki/Device_driver"" title=""Device driver"">device drivers</a> (timer, graphics, printer, mouse, keyboard and sound). Unlike MS-DOS, Windows allowed users to execute multiple graphical applications at the same time, through <a class=""mw-redirect"" href=""/wiki/Nonpreemptive_multitasking"" title=""Nonpreemptive multitasking"">cooperative multitasking</a>. Windows implemented an elaborate, segment-based, software virtual memory scheme, which allows it to run applications larger than available memory: code segments and <a href=""/wiki/Resource_(Windows)"" title=""Resource (Windows)"">resources</a> are swapped in and thrown away when memory became scarce; data segments moved in memory when a given application had relinquished processor control.
</p>, <p><a href=""/wiki/Windows_3.0"" title=""Windows 3.0"">Windows 3.0</a>, released in 1990, improved the design, mostly because of <a href=""/wiki/Virtual_memory"" title=""Virtual memory"">virtual memory</a> and loadable virtual device drivers (<a href=""/wiki/VxD"" title=""VxD"">VxDs</a>) that allow Windows to share arbitrary devices between multi-tasked DOS applications.<sup class=""noprint Inline-Template Template-Fact"" style=""white-space:nowrap;"">[<i><a href=""/wiki/Wikipedia:Citation_needed"" title=""Wikipedia:Citation needed""><span title=""This claim needs references to reliable sources. (July 2007)"">citation needed</span></a></i>]</sup> Windows 3.0 applications can run in <a href=""/wiki/Protected_mode"" title=""Protected mode"">protected mode</a>, which gives them access to several megabytes of memory without the obligation to participate in the software virtual memory scheme. They run inside the same address space, where the segmented memory provides a degree of protection. Windows 3.0 also featured improvements to the user interface. Microsoft rewrote critical operations from <a href=""/wiki/C_(programming_language)"" title=""C (programming language)"">C</a> into <a href=""/wiki/Assembly_language"" title=""Assembly language"">assembly</a>. Windows 3.0 is the first Microsoft Windows version to achieve broad commercial success, selling 2 million copies in the first six months.<sup class=""reference"" id=""cite_ref-18""><a href=""#cite_note-18"">[18]</a></sup><sup class=""reference"" id=""cite_ref-19""><a href=""#cite_note-19"">[19]</a></sup>
</p>, <p>Windows 3.1, made <a class=""mw-redirect"" href=""/wiki/General_availability_release"" title=""General availability release"">generally available</a> on March 1, 1992, featured a facelift. In August 1993, Windows for Workgroups, a special version with integrated <a class=""mw-redirect"" href=""/wiki/Peer-to-peer_networking"" title=""Peer-to-peer networking"">peer-to-peer networking</a> features and a version number of 3.11, was released. It was sold along with Windows 3.1. Support for Windows 3.1 ended on December 31, 2001.<sup class=""reference"" id=""cite_ref-20""><a href=""#cite_note-20"">[20]</a></sup>
</p>, <p>Windows 3.2, released 1994, is an updated version of the Chinese version of Windows 3.1.<sup class=""reference"" id=""cite_ref-21""><a href=""#cite_note-21"">[21]</a></sup> The update was limited to this language version, as it fixed only issues related to the complex writing system of the Chinese language.<sup class=""reference"" id=""cite_ref-22""><a href=""#cite_note-22"">[22]</a></sup> Windows 3.2 was generally sold by computer manufacturers with a ten-disk version of <a href=""/wiki/MS-DOS"" title=""MS-DOS"">MS-DOS</a> that also had <a href=""/wiki/Simplified_Chinese_characters"" title=""Simplified Chinese characters"">Simplified Chinese characters</a> in basic output and some translated utilities.
</p>, <p>The next major consumer-oriented release of Windows, <a href=""/wiki/Windows_95"" title=""Windows 95"">Windows 95</a>, was released on August 24, 1995. While still remaining MS-DOS-based, Windows 95 introduced support for native <a class=""mw-redirect"" href=""/wiki/32-bit_application"" title=""32-bit application"">32-bit applications</a>, <a href=""/wiki/Plug_and_play"" title=""Plug and play"">plug and play</a> hardware, <a class=""mw-redirect"" href=""/wiki/Preemptive_multitasking"" title=""Preemptive multitasking"">preemptive multitasking</a>, <a class=""mw-redirect"" href=""/wiki/Long_file_name"" title=""Long file name"">long file names</a> of up to 255 characters, and provided increased stability over its predecessors. Windows 95 also introduced a redesigned, <a href=""/wiki/Object-oriented_design"" title=""Object-oriented design"">object oriented</a> user interface, replacing the previous <a href=""/wiki/Program_Manager"" title=""Program Manager"">Program Manager</a> with the <a href=""/wiki/Start_menu"" title=""Start menu"">Start menu</a>, <a href=""/wiki/Taskbar"" title=""Taskbar"">taskbar</a>, and <a class=""mw-redirect"" href=""/wiki/Windows_Explorer"" title=""Windows Explorer"">Windows Explorer</a> <a href=""/wiki/Windows_shell"" title=""Windows shell"">shell</a>. Windows 95 was a major commercial success for Microsoft; Ina Fried of <a href=""/wiki/CNET"" title=""CNET"">CNET</a> remarked that ""by the time Windows 95 was finally ushered off the market in 2001, it had become a fixture on computer desktops around the world.""<sup class=""reference"" id=""cite_ref-23""><a href=""#cite_note-23"">[23]</a></sup> Microsoft published four OEM Service Releases (OSR) of Windows 95, each of which was roughly equivalent to a <a href=""/wiki/Service_pack"" title=""Service pack"">service pack</a>. The first OSR of Windows 95 was also the first version of Windows to be bundled with Microsoft's <a href=""/wiki/Web_browser"" title=""Web browser"">web browser</a>, <a href=""/wiki/Internet_Explorer"" title=""Internet Explorer"">Internet Explorer</a>.<sup class=""reference"" id=""cite_ref-apr96ms_24-0""><a href=""#cite_note-apr96ms-24"">[24]</a></sup> Mainstream support for Windows 95 ended on December 31, 2000, and extended support for Windows 95 ended on December 31, 2001.<sup class=""reference"" id=""cite_ref-Windows_95_Support_Lifecycle_25-0""><a href=""#cite_note-Windows_95_Support_Lifecycle-25"">[25]</a></sup>
</p>, <p>Windows 95 was followed up with the release of <a href=""/wiki/Windows_98"" title=""Windows 98"">Windows 98</a> on June 25, 1998, which introduced the <a href=""/wiki/Windows_Driver_Model"" title=""Windows Driver Model"">Windows Driver Model</a>, support for <a class=""mw-redirect"" href=""/wiki/Universal_Serial_Bus#Overview"" title=""Universal Serial Bus"">USB composite devices</a>, support for <a href=""/wiki/Advanced_Configuration_and_Power_Interface"" title=""Advanced Configuration and Power Interface"">ACPI</a>, <a href=""/wiki/Hibernation_(computing)"" title=""Hibernation (computing)"">hibernation</a>, and support for <a href=""/wiki/Multi-monitor"" title=""Multi-monitor"">multi-monitor</a> configurations. Windows 98 also included integration with <a href=""/wiki/Internet_Explorer_4"" title=""Internet Explorer 4"">Internet Explorer 4</a> through <a href=""/wiki/Active_Desktop"" title=""Active Desktop"">Active Desktop</a> and other aspects of the <a href=""/wiki/Windows_Desktop_Update"" title=""Windows Desktop Update"">Windows Desktop Update</a> (a series of enhancements to the Explorer shell which were also made available for Windows 95). In May 1999, Microsoft released <a class=""mw-redirect"" href=""/wiki/Windows_98_Second_Edition"" title=""Windows 98 Second Edition"">Windows 98 Second Edition</a>, an updated version of Windows 98. Windows 98 SE added <a class=""mw-redirect"" href=""/wiki/Internet_Explorer_5.0"" title=""Internet Explorer 5.0"">Internet Explorer 5.0</a> and <a href=""/wiki/Windows_Media_Player"" title=""Windows Media Player"">Windows Media Player</a> 6.2 amongst other upgrades. Mainstream support for Windows 98 ended on June 30, 2002, and extended support for Windows 98 ended on July 11, 2006.<sup class=""reference"" id=""cite_ref-Windows_98_Standard_Edition_Support_Lifecycle_26-0""><a href=""#cite_note-Windows_98_Standard_Edition_Support_Lifecycle-26"">[26]</a></sup>
</p>, <p>On September 14, 2000, Microsoft released <a href=""/wiki/Windows_Me"" title=""Windows Me"">Windows Me</a> (Millennium Edition), the last DOS-based version of Windows. Windows Me incorporated visual interface enhancements from its Windows NT-based counterpart <a href=""/wiki/Windows_2000"" title=""Windows 2000"">Windows 2000</a>, had faster boot times than previous versions (which however, required the removal of the ability to access a <a href=""/wiki/Real_mode"" title=""Real mode"">real mode</a> DOS environment, removing compatibility with some older programs),<sup class=""reference"" id=""cite_ref-FastBoot1_27-0""><a href=""#cite_note-FastBoot1-27"">[27]</a></sup> expanded <a href=""/wiki/Multimedia"" title=""Multimedia"">multimedia</a> functionality (including Windows Media Player 7, <a href=""/wiki/Windows_Movie_Maker"" title=""Windows Movie Maker"">Windows Movie Maker</a>, and the <a href=""/wiki/Windows_Image_Acquisition"" title=""Windows Image Acquisition"">Windows Image Acquisition</a> framework for retrieving images from scanners and digital cameras), additional system utilities such as <a class=""mw-redirect"" href=""/wiki/System_File_Protection"" title=""System File Protection"">System File Protection</a> and <a href=""/wiki/System_Restore"" title=""System Restore"">System Restore</a>, and updated <a href=""/wiki/Home_network"" title=""Home network"">home networking</a> tools.<sup class=""reference"" id=""cite_ref-pcw-me_28-0""><a href=""#cite_note-pcw-me-28"">[28]</a></sup> However, Windows Me was faced with criticism for its speed and instability, along with hardware compatibility issues and its removal of real mode DOS support. <i><a href=""/wiki/PC_World"" title=""PC World"">PC World</a></i> considered Windows Me to be one of the worst operating systems Microsoft had ever released, and the 4th worst tech product of all time.<sup class=""reference"" id=""cite_ref-WinMEbad_11-1""><a href=""#cite_note-WinMEbad-11"">[11]</a></sup>
</p>, <p>In November 1988, a new development team within Microsoft (which included former <a href=""/wiki/Digital_Equipment_Corporation"" title=""Digital Equipment Corporation"">Digital Equipment Corporation</a> developers <a href=""/wiki/Dave_Cutler"" title=""Dave Cutler"">Dave Cutler</a> and <a href=""/wiki/Mark_Lucovsky"" title=""Mark Lucovsky"">Mark Lucovsky</a>) began work on a revamped version of <a href=""/wiki/IBM"" title=""IBM"">IBM</a> and Microsoft's <a href=""/wiki/OS/2"" title=""OS/2"">OS/2</a> operating system known as ""NT OS/2"". NT OS/2 was intended to be a secure, <a class=""mw-redirect"" href=""/wiki/Multi-user"" title=""Multi-user"">multi-user</a> operating system with <a href=""/wiki/POSIX"" title=""POSIX"">POSIX</a> compatibility and a modular, <a href=""/wiki/Software_portability"" title=""Software portability"">portable</a> <a href=""/wiki/Kernel_(operating_system)"" title=""Kernel (operating system)"">kernel</a> with <a class=""mw-redirect"" href=""/wiki/Preemptive_multitasking"" title=""Preemptive multitasking"">preemptive multitasking</a> and support for multiple processor architectures. However, following the successful release of <a href=""/wiki/Windows_3.0"" title=""Windows 3.0"">Windows 3.0</a>, the NT development team decided to rework the project to use an extended <a href=""/wiki/32-bit_computing"" title=""32-bit computing"">32-bit</a> port of the <a href=""/wiki/Windows_API"" title=""Windows API"">Windows API</a> known as Win32 instead of those of OS/2. Win32 maintained a similar structure to the Windows <a href=""/wiki/API"" title=""API"">APIs</a> (allowing existing Windows applications to easily be <a href=""/wiki/Porting"" title=""Porting"">ported</a> to the platform), but also supported the capabilities of the existing NT kernel. Following its approval by Microsoft's staff, development continued on what was now Windows NT, the first 32-bit version of Windows. However, IBM objected to the changes, and ultimately continued OS/2 development on its own.<sup class=""reference"" id=""cite_ref-insident_29-0""><a href=""#cite_note-insident-29"">[29]</a></sup><sup class=""reference"" id=""cite_ref-thurrott-nt_30-0""><a href=""#cite_note-thurrott-nt-30"">[30]</a></sup>
</p>, <p>Windows NT was the first Windows operating system based on a <a href=""/wiki/Hybrid_kernel"" title=""Hybrid kernel"">hybrid kernel</a>. The hybrid kernel was designed as a modified <a href=""/wiki/Microkernel"" title=""Microkernel"">microkernel</a>, influenced by the <a href=""/wiki/Mach_(kernel)"" title=""Mach (kernel)"">Mach microkernel</a> developed by <a href=""/wiki/Richard_Rashid"" title=""Richard Rashid"">Richard Rashid</a> at Carnegie Mellon University, but without meeting all of the criteria of a pure microkernel.
</p>, <p>The first release of the resulting operating system, <a href=""/wiki/Windows_NT_3.1"" title=""Windows NT 3.1"">Windows NT 3.1</a> (named to associate it with <a class=""mw-redirect"" href=""/wiki/Windows_3.1"" title=""Windows 3.1"">Windows 3.1</a>) was released in July 1993, with versions for desktop <a href=""/wiki/Workstation"" title=""Workstation"">workstations</a> and <a href=""/wiki/Server_(computing)"" title=""Server (computing)"">servers</a>. <a href=""/wiki/Windows_NT_3.5"" title=""Windows NT 3.5"">Windows NT 3.5</a> was released in September 1994, focusing on performance improvements and support for <a href=""/wiki/Novell"" title=""Novell"">Novell</a>'s <a href=""/wiki/NetWare"" title=""NetWare"">NetWare</a>, and was followed up by <a href=""/wiki/Windows_NT_3.51"" title=""Windows NT 3.51"">Windows NT 3.51</a> in May 1995, which included additional improvements and support for the <a href=""/wiki/PowerPC"" title=""PowerPC"">PowerPC</a> architecture. <a href=""/wiki/Windows_NT_4.0"" title=""Windows NT 4.0"">Windows NT 4.0</a> was released in June 1996, introducing the redesigned interface of <a href=""/wiki/Windows_95"" title=""Windows 95"">Windows 95</a> to the NT series. On February 17, 2000, Microsoft released <a href=""/wiki/Windows_2000"" title=""Windows 2000"">Windows 2000</a>, a successor to NT 4.0. The Windows NT name was dropped at this point in order to put a greater focus on the Windows brand.<sup class=""reference"" id=""cite_ref-thurrott-nt_30-1""><a href=""#cite_note-thurrott-nt-30"">[30]</a></sup>
</p>, <p>The next major version of Windows NT, <a href=""/wiki/Windows_XP"" title=""Windows XP"">Windows XP</a>, was released on October 25, 2001. The introduction of Windows XP aimed to unify the consumer-oriented <a href=""/wiki/Windows_9x"" title=""Windows 9x"">Windows 9x</a> series with the architecture introduced by Windows NT, a change which Microsoft promised would provide better performance over its DOS-based predecessors. Windows XP would also introduce a redesigned user interface (including an updated Start menu and a ""task-oriented"" <a class=""mw-redirect"" href=""/wiki/File_Explorer"" title=""File Explorer"">Windows Explorer</a>), streamlined multimedia and networking features, <a href=""/wiki/Internet_Explorer_6"" title=""Internet Explorer 6"">Internet Explorer 6</a>, integration with Microsoft's <a href=""/wiki/Microsoft_account"" title=""Microsoft account"">.NET Passport</a> services, a ""<a href=""/wiki/Windows_XP#Backwards_compatibility"" title=""Windows XP"">compatibility mode</a>"" to help provide <a class=""mw-redirect"" href=""/wiki/Backwards_compatibility"" title=""Backwards compatibility"">backwards compatibility</a> with software designed for previous versions of Windows, and <a class=""mw-redirect"" href=""/wiki/Windows_Remote_Assistance"" title=""Windows Remote Assistance"">Remote Assistance</a> functionality.<sup class=""reference"" id=""cite_ref-cnet-xpreview_31-0""><a href=""#cite_note-cnet-xpreview-31"">[31]</a></sup><sup class=""reference"" id=""cite_ref-32""><a href=""#cite_note-32"">[32]</a></sup>
</p>, <p>At retail, Windows XP was now marketed in two main <a href=""/wiki/Stock_keeping_unit"" title=""Stock keeping unit"">editions</a>: the ""Home"" edition was targeted towards consumers, while the ""Professional"" edition was targeted towards business environments and <a href=""/wiki/Power_user"" title=""Power user"">power users</a>, and included additional security and networking features. Home and Professional were later accompanied by the ""Media Center"" edition (designed for <a href=""/wiki/Home_theater_PC"" title=""Home theater PC"">home theater PCs</a>, with an emphasis on support for <a href=""/wiki/DVD"" title=""DVD"">DVD</a> playback, <a href=""/wiki/TV_tuner_card"" title=""TV tuner card"">TV tuner cards</a>, <a href=""/wiki/Digital_video_recorder"" title=""Digital video recorder"">DVR</a> functionality, and remote controls), and the ""Tablet PC"" edition (designed for mobile devices meeting its <a href=""/wiki/Microsoft_Tablet_PC"" title=""Microsoft Tablet PC"">specifications</a> for a <a href=""/wiki/Tablet_computer"" title=""Tablet computer"">tablet computer</a>, with support for <a href=""/wiki/Stylus"" title=""Stylus"">stylus</a> pen input and additional pen-enabled applications).<sup class=""reference"" id=""cite_ref-33""><a href=""#cite_note-33"">[33]</a></sup><sup class=""reference"" id=""cite_ref-34""><a href=""#cite_note-34"">[34]</a></sup><sup class=""reference"" id=""cite_ref-35""><a href=""#cite_note-35"">[35]</a></sup> Mainstream support for Windows XP ended on April 14, 2009. Extended support ended on April 8, 2014.<sup class=""reference"" id=""cite_ref-36""><a href=""#cite_note-36"">[36]</a></sup>
</p>, <p>After Windows 2000, Microsoft also changed its release schedules for server operating systems; the server counterpart of Windows XP, <a href=""/wiki/Windows_Server_2003"" title=""Windows Server 2003"">Windows Server 2003</a>, was released in April 2003.<sup class=""reference"" id=""cite_ref-thurrott-nt_30-2""><a href=""#cite_note-thurrott-nt-30"">[30]</a></sup> It was followed in December 2005, by Windows Server 2003 R2.
</p>, <p>After a lengthy <a href=""/wiki/Development_of_Windows_Vista"" title=""Development of Windows Vista"">development process</a>, <a href=""/wiki/Windows_Vista"" title=""Windows Vista"">Windows Vista</a> was released on November 30, 2006, for volume licensing and January 30, 2007, for consumers. It contained a number of <a href=""/wiki/Features_new_to_Windows_Vista"" title=""Features new to Windows Vista"">new features</a>, from a redesigned shell and user interface to significant <a href=""/wiki/Technical_features_new_to_Windows_Vista"" title=""Technical features new to Windows Vista"">technical changes</a>, with a particular focus on <a href=""/wiki/Security_and_safety_features_new_to_Windows_Vista"" title=""Security and safety features new to Windows Vista"">security features</a>. It was available in a number of <a class=""mw-redirect"" href=""/wiki/Windows_Vista_editions_and_pricing"" title=""Windows Vista editions and pricing"">different editions</a>, and has been subject to <a href=""/wiki/Criticism_of_Windows_Vista"" title=""Criticism of Windows Vista"">some criticism</a>, such as drop of performance, longer boot time, criticism of new UAC, and stricter license agreement. Vista's server counterpart, <a href=""/wiki/Windows_Server_2008"" title=""Windows Server 2008"">Windows Server 2008</a> was released in early 2008.
</p>, <p>On July 22, 2009, <a href=""/wiki/Windows_7"" title=""Windows 7"">Windows 7</a> and <a href=""/wiki/Windows_Server_2008_R2"" title=""Windows Server 2008 R2"">Windows Server 2008 R2</a> were released as RTM (release to manufacturing) while the former was released to the public 3 months later on October 22, 2009. Unlike its predecessor, Windows Vista, which introduced a large number of <a href=""/wiki/Features_new_to_Windows_Vista"" title=""Features new to Windows Vista"">new features</a>, Windows 7 was intended to be a more focused, incremental upgrade to the Windows line, with the goal of being compatible with applications and hardware with which Windows Vista was already compatible.<sup class=""reference"" id=""cite_ref-37""><a href=""#cite_note-37"">[37]</a></sup> Windows 7 has <a href=""/wiki/Multi-touch"" title=""Multi-touch"">multi-touch</a> support, a redesigned <a href=""/wiki/Windows_shell"" title=""Windows shell"">Windows shell</a> with an updated <a href=""/wiki/Taskbar"" title=""Taskbar"">taskbar</a> with revealable <a href=""/wiki/Features_new_to_Windows_7#Jump_lists"" title=""Features new to Windows 7"">jump lists</a> that contain shortcuts to files frequently used with specific applications and shortcuts to tasks within the application,<sup class=""reference"" id=""cite_ref-38""><a href=""#cite_note-38"">[38]</a></sup> a home networking system called <a href=""/wiki/Features_new_to_Windows_7#HomeGroup"" title=""Features new to Windows 7"">HomeGroup</a>,<sup class=""reference"" id=""cite_ref-39""><a href=""#cite_note-39"">[39]</a></sup> and performance improvements.
</p>, <p><a href=""/wiki/Windows_8"" title=""Windows 8"">Windows 8</a>, the successor to Windows 7, was released generally on October 26, 2012. A number of significant changes were made on Windows 8, including the introduction of a user interface based around Microsoft's <a href=""/wiki/Metro_(design_language)"" title=""Metro (design language)"">Metro design language</a> with optimizations for <a href=""/wiki/Multi-touch"" title=""Multi-touch"">touch-based</a> devices such as <a href=""/wiki/Tablet_computer"" title=""Tablet computer"">tablets</a> and all-in-one PCs. These changes include the <a href=""/wiki/Start_menu#Third_version"" title=""Start menu"">Start screen</a>, which uses large tiles that are more convenient for touch interactions and allow for the display of continually updated information, and a new class of <a href=""/wiki/Application_software"" title=""Application software"">apps</a> which are designed primarily for use on touch-based devices. The new Windows version required a minimum resolution of 1024×768 pixels,<sup class=""reference"" id=""cite_ref-zdnet-smallertab_40-0""><a href=""#cite_note-zdnet-smallertab-40"">[40]</a></sup> effectively making it unfit for <a href=""/wiki/Netbook"" title=""Netbook"">netbooks</a> with 800×600-pixel screens.
</p>, <p>Other changes include increased integration with <a href=""/wiki/Cloud_computing"" title=""Cloud computing"">cloud services</a> and other online platforms (such as <a href=""/wiki/Social_networking_service"" title=""Social networking service"">social networks</a> and Microsoft's own <a href=""/wiki/OneDrive"" title=""OneDrive"">OneDrive</a> (formerly SkyDrive) and <a class=""mw-redirect"" href=""/wiki/Xbox_Live"" title=""Xbox Live"">Xbox Live</a> services), the <a class=""mw-redirect"" href=""/wiki/Windows_Store"" title=""Windows Store"">Windows Store</a> service for software distribution, and a new variant known as <a href=""/wiki/Windows_RT"" title=""Windows RT"">Windows RT</a> for use on devices that utilize the <a class=""mw-redirect"" href=""/wiki/ARM_architecture"" title=""ARM architecture"">ARM architecture</a>, and a new keyboard shortcut for <a href=""/wiki/Screenshot"" title=""Screenshot"">screenshots</a>.<sup class=""reference"" id=""cite_ref-41""><a href=""#cite_note-41"">[41]</a></sup><sup class=""reference"" id=""cite_ref-pcw-testdrivertm_42-0""><a href=""#cite_note-pcw-testdrivertm-42"">[42]</a></sup><sup class=""reference"" id=""cite_ref-bi-windows8_43-0""><a href=""#cite_note-bi-windows8-43"">[43]</a></sup><sup class=""reference"" id=""cite_ref-bw-win8editions_44-0""><a href=""#cite_note-bw-win8editions-44"">[44]</a></sup><sup class=""reference"" id=""cite_ref-bw8-buildingarm_45-0""><a href=""#cite_note-bw8-buildingarm-45"">[45]</a></sup><sup class=""reference"" id=""cite_ref-verge-talkswin8_46-0""><a href=""#cite_note-verge-talkswin8-46"">[46]</a></sup><sup class=""reference"" id=""cite_ref-pcw-building_47-0""><a href=""#cite_note-pcw-building-47"">[47]</a></sup> An update to Windows 8, called <a href=""/wiki/Windows_8.1"" title=""Windows 8.1"">Windows 8.1</a>,<sup class=""reference"" id=""cite_ref-48""><a href=""#cite_note-48"">[48]</a></sup> was released on October 17, 2013, and includes features such as new live tile sizes, deeper <a href=""/wiki/OneDrive"" title=""OneDrive"">OneDrive</a> integration, and many other revisions. <a href=""/wiki/Windows_8"" title=""Windows 8"">Windows 8</a> and <a href=""/wiki/Windows_8.1"" title=""Windows 8.1"">Windows 8.1</a> have been subject to some criticism, such as removal of the <a href=""/wiki/Start_menu"" title=""Start menu"">Start menu</a>.
</p>, <p>On September 30, 2014, Microsoft announced <a href=""/wiki/Windows_10"" title=""Windows 10"">Windows 10</a> as the successor to Windows 8.1. It was released on July 29, 2015, and addresses shortcomings in the user interface first introduced with Windows 8. Changes on PC include the return of the Start Menu, a <a href=""/wiki/Virtual_desktop"" title=""Virtual desktop"">virtual desktop</a> system, and the ability to run Windows Store apps within windows on the desktop rather than in full-screen mode. <a href=""/wiki/Windows_10"" title=""Windows 10"">Windows 10</a> is said to be available to update from qualified <a href=""/wiki/Windows_7"" title=""Windows 7"">Windows 7</a> with SP1, <a href=""/wiki/Windows_8.1"" title=""Windows 8.1"">Windows 8.1</a> and <a href=""/wiki/Windows_Phone_8.1"" title=""Windows Phone 8.1"">Windows Phone 8.1</a> devices from the Get Windows 10 Application (for <a href=""/wiki/Windows_7"" title=""Windows 7"">Windows 7</a>, <a href=""/wiki/Windows_8.1"" title=""Windows 8.1"">Windows 8.1</a>) or <a href=""/wiki/Windows_Update"" title=""Windows Update"">Windows Update</a> (<a href=""/wiki/Windows_7"" title=""Windows 7"">Windows 7</a>).<sup class=""reference"" id=""cite_ref-49""><a href=""#cite_note-49"">[49]</a></sup>
</p>, <p>In February 2017, Microsoft announced the migration of its Windows source code repository from <a href=""/wiki/Perforce"" title=""Perforce"">Perforce</a> to <a href=""/wiki/Git"" title=""Git"">Git</a>. This migration involved 3.5 million separate files in a 300 gigabyte repository.<sup class=""reference"" id=""cite_ref-PBright_50-0""><a href=""#cite_note-PBright-50"">[50]</a></sup> By May 2017, 90 percent of its engineering team was using Git, in about 8500 commits and 1760 Windows builds per day.<sup class=""reference"" id=""cite_ref-PBright_50-1""><a href=""#cite_note-PBright-50"">[50]</a></sup>
</p>, <p>In June 2021, shortly before Microsoft's announcement of Windows 11, Microsoft updated their lifecycle policy pages for Windows 10, revealing that support for their last release of Windows 10 will be October 14, 2025.<sup class=""reference"" id=""cite_ref-Windows10HomeAndProLifecycle_51-0""><a href=""#cite_note-Windows10HomeAndProLifecycle-51"">[51]</a></sup><sup class=""reference"" id=""cite_ref-Windows10EntAndEduLifecycle_52-0""><a href=""#cite_note-Windows10EntAndEduLifecycle-52"">[52]</a></sup>
</p>, <p>On June 24, 2021, <a href=""/wiki/Windows_11"" title=""Windows 11"">Windows 11</a> was announced as the successor to Windows 10 during a livestream. The new operating system was designed to be more user-friendly and understandable. It was released on October 5, 2021.<sup class=""reference"" id=""cite_ref-53""><a href=""#cite_note-53"">[53]</a></sup><sup class=""reference"" id=""cite_ref-54""><a href=""#cite_note-54"">[54]</a></sup> Windows 11 is a free upgrade to some Windows 10 users as of now.
</p>, <p>In July 2021, Microsoft announced it will start selling subscriptions to virtualized Windows desktops as part of a new <i>Windows 365</i> service in the following month. It is not a standalone version of Microsoft Windows, but a web service that provides access to <a href=""/wiki/Windows_10"" title=""Windows 10"">Windows 10</a> and <a href=""/wiki/Windows_11"" title=""Windows 11"">Windows 11</a> built on top of Azure Virtual Desktop. The new service will allow for <a href=""/wiki/Cross-platform_software"" title=""Cross-platform software"">cross-platform usage</a>, aiming to make the operating system available for both Apple and Android users. The <a class=""mw-redirect"" href=""/wiki/Subscription_service"" title=""Subscription service"">subscription service</a> will be accessible through any <a href=""/wiki/Operating_system"" title=""Operating system"">operating system</a> with a <a href=""/wiki/Web_browser"" title=""Web browser"">web browser</a>. The new service is an attempt at capitalizing on the growing trend, fostered during the <a href=""/wiki/COVID-19_pandemic"" title=""COVID-19 pandemic"">COVID-19 pandemic</a>, for businesses to adopt a hybrid <a href=""/wiki/Remote_work"" title=""Remote work"">remote work</a> environment, in which ""employees split their time between the office and home"". As the service will be accessible through web browsers, Microsoft will be able to bypass the need to publish the service through <a href=""/wiki/Google_Play"" title=""Google Play"">Google Play</a> or the <a href=""/wiki/App_Store_(iOS/iPadOS)"" title=""App Store (iOS/iPadOS)"">Apple App Store</a>.<sup class=""reference"" id=""cite_ref-55""><a href=""#cite_note-55"">[55]</a></sup><sup class=""reference"" id=""cite_ref-56""><a href=""#cite_note-56"">[56]</a></sup><sup class=""reference"" id=""cite_ref-57""><a href=""#cite_note-57"">[57]</a></sup><sup class=""reference"" id=""cite_ref-58""><a href=""#cite_note-58"">[58]</a></sup><sup class=""reference"" id=""cite_ref-59""><a href=""#cite_note-59"">[59]</a></sup>
</p>, <p>Microsoft announced Windows 365 availability to business and enterprise customers on August 2, 2021.<sup class=""reference"" id=""cite_ref-60""><a href=""#cite_note-60"">[60]</a></sup>
</p>, <p>Multilingual support has been built into Windows since Windows 3.0. The language for both the keyboard and the interface can be changed through the Region and Language Control Panel. Components for all supported input languages, such as <a class=""mw-redirect"" href=""/wiki/Input_Method_Editor"" title=""Input Method Editor"">Input Method Editors</a>, are automatically installed during Windows installation (in Windows XP and earlier, files for East Asian languages, such as Chinese, and right-to-left scripts, such as Arabic, may need to be installed separately, also from the said Control Panel). Third-party IMEs may also be installed if a user feels that the provided one is insufficient for their needs.
</p>, <p>Interface languages for the operating system are free for download, but some languages are limited to certain editions of Windows. <a href=""/wiki/Language_Interface_Pack"" title=""Language Interface Pack"">Language Interface Packs</a> (LIPs) are redistributable and may be downloaded from Microsoft's Download Center and installed for any edition of Windows (XP or later) –  they translate most, but not all, of the Windows interface, and require a certain base language (the language which Windows originally shipped with). This is used for most languages in emerging markets. Full Language Packs, which translates the complete operating system, are only available for specific editions of Windows (Ultimate and Enterprise editions of Windows Vista and 7, and all editions of Windows 8, 8.1 and RT except Single Language). They do not require a specific base language, and are commonly used for more popular languages such as French or Chinese. These languages cannot be downloaded through the Download Center, but available as optional updates through the <a href=""/wiki/Windows_Update"" title=""Windows Update"">Windows Update</a> service (except Windows 8).
</p>, <p>The interface language of installed applications is not affected by changes in the Windows interface language. The availability of languages depends on the application developers themselves.
</p>, <p><a href=""/wiki/Windows_8"" title=""Windows 8"">Windows 8</a> and <a href=""/wiki/Windows_Server_2012"" title=""Windows Server 2012"">Windows Server 2012</a> introduces a new Language Control Panel where both the interface and input languages can be simultaneously changed, and language packs, regardless of type, can be downloaded from a central location. The PC Settings app in <a href=""/wiki/Windows_8.1"" title=""Windows 8.1"">Windows 8.1</a> and <a href=""/wiki/Windows_Server_2012_R2"" title=""Windows Server 2012 R2"">Windows Server 2012 R2</a> also includes a counterpart settings page for this. Changing the interface language also changes the language of preinstalled <a class=""mw-redirect"" href=""/wiki/Windows_Store"" title=""Windows Store"">Windows Store</a> apps (such as Mail, Maps and News) and certain other Microsoft-developed apps (such as Remote Desktop). The above limitations for language packs are however still in effect, except that full language packs can be installed for any edition except Single Language, which caters to emerging markets.
</p>, <p>Windows NT included support for several platforms before the <a href=""/wiki/X86"" title=""X86"">x86</a>-based <a href=""/wiki/Personal_computer"" title=""Personal computer"">personal computer</a> became dominant in the professional world. <a href=""/wiki/Windows_NT_4.0"" title=""Windows NT 4.0"">Windows NT 4.0</a> and its predecessors supported <a href=""/wiki/PowerPC"" title=""PowerPC"">PowerPC</a>, <a href=""/wiki/DEC_Alpha"" title=""DEC Alpha"">DEC Alpha</a> and <a href=""/wiki/MIPS_architecture"" title=""MIPS architecture"">MIPS</a> R4000 (although some of the platforms implement <a href=""/wiki/64-bit_computing"" title=""64-bit computing"">64-bit computing</a>, the OS treated them as 32-bit). Windows 2000 dropped support for all platforms, except the third generation x86 (known as <a href=""/wiki/IA-32"" title=""IA-32"">IA-32</a>) or newer in 32-bit mode. The client line of Windows NT family still runs on IA-32 but the <a href=""/wiki/Windows_Server"" title=""Windows Server"">Windows Server</a> line ceased supporting this platform with the release of <a href=""/wiki/Windows_Server_2008_R2"" title=""Windows Server 2008 R2"">Windows Server 2008 R2</a>.
</p>, <p>With the introduction of the Intel Itanium architecture (<a href=""/wiki/IA-64"" title=""IA-64"">IA-64</a>), Microsoft released new versions of Windows to support it. Itanium versions of <a href=""/wiki/Windows_XP"" title=""Windows XP"">Windows XP</a> and <a href=""/wiki/Windows_Server_2003"" title=""Windows Server 2003"">Windows Server 2003</a> were released at the same time as their mainstream x86 counterparts. Windows XP 64-Bit Edition, released in 2005, is the last Windows client operating systems to support Itanium. Windows Server line continues to support this platform until <a href=""/wiki/Windows_Server_2012"" title=""Windows Server 2012"">Windows Server 2012</a>; Windows Server 2008 R2 is the last Windows operating system to support Itanium architecture.
</p>, <p>On April 25, 2005, Microsoft released <a href=""/wiki/Windows_XP_Professional_x64_Edition"" title=""Windows XP Professional x64 Edition"">Windows XP Professional x64 Edition</a> and Windows Server 2003 x64 Editions to support <a href=""/wiki/X86-64"" title=""X86-64"">x86-64</a> (or simply x64), the 64-bit version of x86 architecture. <a href=""/wiki/Windows_Vista"" title=""Windows Vista"">Windows Vista</a> was the first client version of Windows NT to be released simultaneously in IA-32 and x64 editions. x64 is still supported.
</p>, <p>An edition of Windows 8 known as <a href=""/wiki/Windows_RT"" title=""Windows RT"">Windows RT</a> was specifically created for computers with <a class=""mw-redirect"" href=""/wiki/ARM_architecture"" title=""ARM architecture"">ARM architecture</a> and while ARM is still used for Windows smartphones with Windows 10, tablets with Windows RT will not be updated. Starting from <a class=""mw-redirect"" href=""/wiki/Windows_10_Fall_Creators_Update"" title=""Windows 10 Fall Creators Update"">Windows 10 Fall Creators Update</a> (version 1709) and later includes support for PCs with <a class=""mw-redirect"" href=""/wiki/ARM_architecture"" title=""ARM architecture"">ARM architecture</a>.<sup class=""reference"" id=""cite_ref-61""><a href=""#cite_note-61"">[61]</a></sup>
</p>, <p><a href=""/wiki/Windows_11"" title=""Windows 11"">Windows 11</a> is the first version to drop support for 32-bit hardware.<sup class=""reference"" id=""cite_ref-62""><a href=""#cite_note-62"">[62]</a></sup>
</p>, <p>Windows CE (officially known as <i>Windows Embedded Compact</i>), is an edition of Windows that runs on <a href=""/wiki/Handheld_PC"" title=""Handheld PC"">minimalistic computers</a>, like satellite navigation systems and some mobile phones. Windows Embedded Compact is based on its own dedicated kernel, dubbed Windows CE kernel. Microsoft licenses Windows CE to <a class=""mw-redirect"" href=""/wiki/OEM"" title=""OEM"">OEMs</a> and device makers. The OEMs and device makers can modify and create their own user interfaces and experiences, while Windows CE provides the technical foundation to do so.
</p>, <p>Windows CE was used in the <a href=""/wiki/Dreamcast"" title=""Dreamcast"">Dreamcast</a> along with Sega's own proprietary OS for the console. Windows CE was the core from which <a href=""/wiki/Windows_Mobile"" title=""Windows Mobile"">Windows Mobile</a> was derived. Its successor, <a href=""/wiki/Windows_Phone_7"" title=""Windows Phone 7"">Windows Phone 7</a>, was based on components from both <a href=""/wiki/Windows_Embedded_CE_6.0"" title=""Windows Embedded CE 6.0"">Windows CE 6.0 R3</a> and <a href=""/wiki/Windows_Embedded_Compact_7"" title=""Windows Embedded Compact 7"">Windows CE 7.0</a>. <a href=""/wiki/Windows_Phone_8"" title=""Windows Phone 8"">Windows Phone 8</a> however, is based on the same NT-kernel as Windows 8.
</p>, <p>Windows Embedded Compact is not to be confused with <a class=""mw-redirect"" href=""/wiki/Windows_XP_Embedded"" title=""Windows XP Embedded"">Windows XP Embedded</a> or <a class=""mw-redirect"" href=""/wiki/Windows_NT_4.0_Embedded"" title=""Windows NT 4.0 Embedded"">Windows NT 4.0 Embedded</a>, modular editions of Windows based on Windows NT kernel.
</p>, <p>Xbox OS is an unofficial name given to the version of Windows that runs on <a href=""/wiki/Xbox"" title=""Xbox"">Xbox</a> consoles.<sup class=""reference"" id=""cite_ref-63""><a href=""#cite_note-63"">[63]</a></sup> From <a href=""/wiki/Xbox_One"" title=""Xbox One"">Xbox One</a> onwards it is an implementation with an emphasis on virtualization (using <a href=""/wiki/Hyper-V"" title=""Hyper-V"">Hyper-V</a>) as it is three operating systems running at once, consisting of the core <a href=""/wiki/Operating_system"" title=""Operating system"">operating system</a>, a second implemented for games and a more Windows-like environment for applications.<sup class=""reference"" id=""cite_ref-xboxonethreesystems_64-0""><a href=""#cite_note-xboxonethreesystems-64"">[64]</a></sup>
Microsoft updates Xbox One's OS every month, and these updates can be downloaded from the Xbox Live service to the Xbox and subsequently installed, or by using offline recovery images downloaded via a PC.<sup class=""reference"" id=""cite_ref-65""><a href=""#cite_note-65"">[65]</a></sup> It was originally based on NT 6.2 (Windows 8) kernel, and the latest version runs on an NT 10.0 base. This system is sometimes referred to as ""Windows 10 on Xbox One"" or ""OneCore"".<sup class=""reference"" id=""cite_ref-66""><a href=""#cite_note-66"">[66]</a></sup><sup class=""reference"" id=""cite_ref-67""><a href=""#cite_note-67"">[67]</a></sup>
Xbox One and <a href=""/wiki/Xbox_Series_X_and_Series_S"" title=""Xbox Series X and Series S"">Xbox Series</a> operating systems also allow limited (due to licensing restrictions and testing resources) backward compatibility with previous generation hardware,<sup class=""reference"" id=""cite_ref-68""><a href=""#cite_note-68"">[68]</a></sup> and the Xbox 360's system is backwards compatible with the original Xbox.<sup class=""reference"" id=""cite_ref-69""><a href=""#cite_note-69"">[69]</a></sup>
</p>, <p>Up to and including every version before <a href=""/wiki/Windows_2000"" title=""Windows 2000"">Windows 2000</a>, Microsoft used an in-house version control system named Source Library Manager (SLM). Shortly after Windows 2000 was released, Microsoft switched to a fork of Perforce named Source Depot.<sup class=""reference"" id=""cite_ref-70""><a href=""#cite_note-70"">[70]</a></sup> This system was used up until 2017 once the system couldn't keep up with the size of Windows. Microsoft had begun to integrate Git into <a class=""mw-redirect"" href=""/wiki/Team_Foundation_Server"" title=""Team Foundation Server"">Team Foundation Server</a> in 2013, but Windows continued to rely on Source Depot.<sup class=""noprint Inline-Template Template-Fact"" style=""white-space:nowrap;"">[<i><a href=""/wiki/Wikipedia:Citation_needed"" title=""Wikipedia:Citation needed""><span title=""This claim needs references to reliable sources. (October 2021)"">citation needed</span></a></i>]</sup> The Windows code was divided among 65 different repositories with a kind of virtualization layer to produce unified view of all of the code.
</p>, <p>In 2017 Microsoft announced that it would start using <a href=""/wiki/Git"" title=""Git"">Git</a>, an open source version control system created by <a href=""/wiki/Linus_Torvalds"" title=""Linus Torvalds"">Linus Torvalds</a> and in May 2017 they reported that has completed migration into the Git repository.<sup class=""reference"" id=""cite_ref-71""><a href=""#cite_note-71"">[71]</a></sup><sup class=""reference"" id=""cite_ref-:0_72-0""><a href=""#cite_note-:0-72"">[72]</a></sup><sup class=""reference"" id=""cite_ref-PBright_50-2""><a href=""#cite_note-PBright-50"">[50]</a></sup>
</p>, <p>Because of its large, decades-long history, however, the Windows codebase is not especially well suited to the decentralized nature of <a href=""/wiki/Linux"" title=""Linux"">Linux</a> development that Git was originally created to manage.<sup class=""noprint Inline-Template Template-Fact"" style=""white-space:nowrap;"">[<i><a href=""/wiki/Wikipedia:Citation_needed"" title=""Wikipedia:Citation needed""><span title=""This claim needs references to reliable sources. (October 2021)"">citation needed</span></a></i>]</sup> Each Git repository contains a complete history of all the files, which proved unworkable for Windows developers because cloning the whole repository takes several hours.<sup class=""noprint Inline-Template Template-Fact"" style=""white-space:nowrap;"">[<i><a href=""/wiki/Wikipedia:Citation_needed"" title=""Wikipedia:Citation needed""><span title=""This claim needs references to reliable sources. (October 2021)"">citation needed</span></a></i>]</sup> Microsoft has been working on a new project called the <a href=""/wiki/Virtual_File_System_for_Git"" title=""Virtual File System for Git"">Virtual File System for Git</a> (VFSForGit) to address these challenges.<sup class=""reference"" id=""cite_ref-:0_72-1""><a href=""#cite_note-:0-72"">[72]</a></sup>
</p>, <p>In 2021 the VFS for Git has been superseded by <b>Scalar</b>.<sup class=""reference"" id=""cite_ref-73""><a href=""#cite_note-73"">[73]</a></sup>
</p>, <p><b>Version market share</b><br/>
As a percentage of desktop and laptop systems using Windows,<sup class=""reference"" id=""cite_ref-76""><a href=""#cite_note-76"">[76]</a></sup> according to <a href=""/wiki/StatCounter"" title=""StatCounter"">StatCounter</a> data from April 2022.<sup class=""reference"" id=""cite_ref-77""><a href=""#cite_note-77"">[77]</a></sup>
</p>, <p><br/>
Use of the latest version <a href=""/wiki/Windows_10"" title=""Windows 10"">Windows 10</a> has exceeded Windows 7 globally since early 2018.<sup class=""reference"" id=""cite_ref-78""><a href=""#cite_note-78"">[78]</a></sup>
</p>, <p>For desktop and laptop computers, according to <a href=""/wiki/Net_Applications"" title=""Net Applications"">Net Applications</a> and <a href=""/wiki/StatCounter"" title=""StatCounter"">StatCounter</a>, which track the use of operating systems in devices that are active on the Web, Windows was the most used operating-system family in August 2021, with around 91% usage share according to Net Applications<sup class=""reference"" id=""cite_ref-79""><a href=""#cite_note-79"">[79]</a></sup> and around 76% usage share according to StatCounter.<sup class=""reference"" id=""cite_ref-80""><a href=""#cite_note-80"">[80]</a></sup>
</p>, <p>Including personal computers of all kinds (e.g., desktops, laptops, mobile devices, and game consoles), Windows OSes accounted for 32.67% of usage share in August 2021, compared to Android (highest, at 46.03%), <a href=""/wiki/IOS"" title=""IOS"">iOS</a>'s 13.76%, <a href=""/wiki/IPadOS"" title=""IPadOS"">iPadOS</a>'s 2.81%, and macOS's 2.51%, according to Net Applications<sup class=""reference"" id=""cite_ref-81""><a href=""#cite_note-81"">[81]</a></sup> and 30.73% of usage share in August 2021, compared to Android (highest, at 42.56%), iOS/iPadOS's 16.53%, and macOS's 6.51%, according to StatCounter.<sup class=""reference"" id=""cite_ref-82""><a href=""#cite_note-82"">[82]</a></sup>
</p>, <p>Those statistics do not include servers (including so-called <a href=""/wiki/Cloud_computing"" title=""Cloud computing"">cloud computing</a>, where Microsoft is known not to be a leader, with Linux used more than Windows), as Net Applications and StatCounter use web browsing as a proxy for all use.
</p>, <p>Consumer versions of Windows were originally designed for ease-of-use on a single-user PC without a network connection, and did not have security features built in from the outset.<sup class=""reference"" id=""cite_ref-83""><a href=""#cite_note-83"">[83]</a></sup> However, Windows NT and its successors are designed for security (including on a network) and multi-user PCs, but were not initially designed with Internet security in mind as much, since, when it was first developed in the early 1990s, Internet use was less prevalent.<sup class=""reference"" id=""cite_ref-84""><a href=""#cite_note-84"">[84]</a></sup>
</p>, <p>These design issues combined with programming errors (e.g. <a href=""/wiki/Buffer_overflow"" title=""Buffer overflow"">buffer overflows</a>) and the popularity of Windows means that it is a frequent target of <a href=""/wiki/Computer_worm"" title=""Computer worm"">computer worm</a> and <a href=""/wiki/Computer_virus"" title=""Computer virus"">virus</a> writers. In June 2005, <a href=""/wiki/Bruce_Schneier"" title=""Bruce Schneier"">Bruce Schneier</a>'s <i>Counterpane Internet Security</i> reported that it had seen over 1,000 new viruses and worms in the previous six months.<sup class=""reference"" id=""cite_ref-85""><a href=""#cite_note-85"">[85]</a></sup> In 2005, <a href=""/wiki/Kaspersky_Lab"" title=""Kaspersky Lab"">Kaspersky Lab</a> found around 11,000 malicious programs –  viruses, Trojans, back-doors, and exploits written for Windows.<sup class=""reference"" id=""cite_ref-Patrizio_86-0""><a href=""#cite_note-Patrizio-86"">[86]</a></sup>
</p>, <p>Microsoft releases security patches through its <a href=""/wiki/Windows_Update"" title=""Windows Update"">Windows Update</a> service approximately once a month (usually the <a href=""/wiki/Patch_Tuesday"" title=""Patch Tuesday"">second Tuesday</a> of the month), although critical updates are made available at shorter intervals when necessary.<sup class=""reference"" id=""cite_ref-87""><a href=""#cite_note-87"">[87]</a></sup> In versions of Windows after and including Windows 2000 SP3 and Windows XP, updates can be automatically downloaded and installed if the user selects to do so. As a result, Service Pack 2 for Windows XP, as well as Service Pack 1 for Windows Server 2003, were installed by users more quickly than it otherwise might have been.<sup class=""reference"" id=""cite_ref-88""><a href=""#cite_note-88"">[88]</a></sup>
</p>, <p>While the <a href=""/wiki/Windows_9x"" title=""Windows 9x"">Windows 9x</a> series offered the option of having profiles for multiple users, they had no concept of <a href=""/wiki/Principle_of_least_privilege"" title=""Principle of least privilege"">access privileges</a>, and did not allow concurrent access; and so were not true <a class=""mw-redirect"" href=""/wiki/Multi-user"" title=""Multi-user"">multi-user</a> operating systems. In addition, they implemented only partial <a href=""/wiki/Memory_protection"" title=""Memory protection"">memory protection</a>. They were accordingly widely criticised for lack of security.
</p>, <p>The <a href=""/wiki/Windows_NT"" title=""Windows NT"">Windows NT</a> series of operating systems, by contrast, are true multi-user, and implement absolute memory protection. However, a lot of the advantages of being a true multi-user operating system were nullified by the fact that, prior to Windows Vista, the first user account created during the setup process was an <a class=""mw-redirect"" href=""/wiki/Windows_administrator"" title=""Windows administrator"">administrator</a> account, which was also the default for new accounts. Though <a href=""/wiki/Windows_XP"" title=""Windows XP"">Windows XP</a> did have limited accounts, the majority of home users did not change to an account type with fewer rights – partially due to the number of programs which unnecessarily required administrator rights – and so most home users ran as administrator all the time.
</p>, <p><a href=""/wiki/Windows_Vista"" title=""Windows Vista"">Windows Vista</a> changes this<sup class=""reference"" id=""cite_ref-89""><a href=""#cite_note-89"">[89]</a></sup> by introducing a privilege elevation system called <a href=""/wiki/User_Account_Control"" title=""User Account Control"">User Account Control</a>. When logging in as a standard user, a logon session is created and a <a class=""mw-redirect"" href=""/wiki/Token_(Windows_NT_architecture)"" title=""Token (Windows NT architecture)"">token</a> containing only the most basic privileges is assigned. In this way, the new logon session is incapable of making changes that would affect the entire system. When logging in as a user in the Administrators group, two separate tokens are assigned. The first token contains all privileges typically awarded to an administrator, and the second is a restricted token similar to what a standard user would receive. User applications, including the <a href=""/wiki/Windows_shell"" title=""Windows shell"">Windows shell</a>, are then started with the restricted token, resulting in a reduced privilege environment even under an Administrator account. When an application requests higher privileges or ""Run as administrator"" is clicked, UAC will prompt for confirmation and, if consent is given (including administrator credentials if the account requesting the elevation is not a member of the administrators group), start the process using the unrestricted token.<sup class=""reference"" id=""cite_ref-kennykerr_90-0""><a href=""#cite_note-kennykerr-90"">[90]</a></sup>
</p>, <p>Leaked documents published by <a href=""/wiki/WikiLeaks"" title=""WikiLeaks"">WikiLeaks</a>, codenamed <a href=""/wiki/Vault_7"" title=""Vault 7"">Vault 7</a> and dated from 2013 to 2016, detail the capabilities of the <a href=""/wiki/Central_Intelligence_Agency"" title=""Central Intelligence Agency"">CIA</a> to perform electronic surveillance and cyber warfare,<sup class=""reference"" id=""cite_ref-91""><a href=""#cite_note-91"">[91]</a></sup> such as the ability to compromise <a href=""/wiki/Operating_system"" title=""Operating system"">operating systems</a> such as Microsoft Windows.<sup class=""reference"" id=""cite_ref-92""><a href=""#cite_note-92"">[92]</a></sup>
</p>, <p>In August 2019, computer experts reported that the <a href=""/wiki/BlueKeep"" title=""BlueKeep"">BlueKeep</a> <a href=""/wiki/Vulnerability_(computing)"" title=""Vulnerability (computing)"">security vulnerability</a>, <a class=""mw-redirect"" href=""/wiki/CVE_(identifier)"" title=""CVE (identifier)"">CVE</a>-<style data-mw-deduplicate=""TemplateStyles:r1067248974"">.mw-parser-output cite.citation{font-style:inherit;word-wrap:break-word}.mw-parser-output .citation q{quotes:""\""""""\""""""'""""'""}.mw-parser-output .citation:target{background-color:rgba(0,127,255,0.133)}.mw-parser-output .id-lock-free a,.mw-parser-output .citation .cs1-lock-free a{background:linear-gradient(transparent,transparent),url(""//upload.wikimedia.org/wikipedia/commons/6/65/Lock-green.svg"")right 0.1em center/9px no-repeat}.mw-parser-output .id-lock-limited a,.mw-parser-output .id-lock-registration a,.mw-parser-output .citation .cs1-lock-limited a,.mw-parser-output .citation .cs1-lock-registration a{background:linear-gradient(transparent,transparent),url(""//upload.wikimedia.org/wikipedia/commons/d/d6/Lock-gray-alt-2.svg"")right 0.1em center/9px no-repeat}.mw-parser-output .id-lock-subscription a,.mw-parser-output .citation .cs1-lock-subscription a{background:linear-gradient(transparent,transparent),url(""//upload.wikimedia.org/wikipedia/commons/a/aa/Lock-red-alt-2.svg"")right 0.1em center/9px no-repeat}.mw-parser-output .cs1-ws-icon a{background:linear-gradient(transparent,transparent),url(""//upload.wikimedia.org/wikipedia/commons/4/4c/Wikisource-logo.svg"")right 0.1em center/12px no-repeat}.mw-parser-output .cs1-code{color:inherit;background:inherit;border:none;padding:inherit}.mw-parser-output .cs1-hidden-error{display:none;color:#d33}.mw-parser-output .cs1-visible-error{color:#d33}.mw-parser-output .cs1-maint{display:none;color:#3a3;margin-left:0.3em}.mw-parser-output .cs1-format{font-size:95%}.mw-parser-output .cs1-kern-left{padding-left:0.2em}.mw-parser-output .cs1-kern-right{padding-right:0.2em}.mw-parser-output .citation .mw-selflink{font-weight:inherit}</style><a class=""external text"" href=""https://cve.org/CVERecord?id=CVE-2019-0708"" rel=""nofollow"">2019-0708</a>, that potentially affects older unpatched Microsoft Windows versions via the program's <a href=""/wiki/Remote_Desktop_Protocol"" title=""Remote Desktop Protocol"">Remote Desktop Protocol</a>, allowing for the possibility of <a class=""mw-redirect"" href=""/wiki/Remote_code_execution"" title=""Remote code execution"">remote code execution</a>, may now include related flaws, collectively named <i><a class=""mw-redirect"" href=""/wiki/DejaBlue"" title=""DejaBlue"">DejaBlue</a></i>, affecting newer Windows versions (i.e., <a href=""/wiki/Windows_7"" title=""Windows 7"">Windows 7</a> and all recent versions) as well.<sup class=""reference"" id=""cite_ref-WRD-20190813_93-0""><a href=""#cite_note-WRD-20190813-93"">[93]</a></sup> In addition, experts reported a <a href=""/wiki/Microsoft"" title=""Microsoft"">Microsoft</a> <a class=""mw-redirect"" href=""/wiki/Security_vulnerability"" title=""Security vulnerability"">security vulnerability</a>, <a class=""mw-redirect"" href=""/wiki/CVE_(identifier)"" title=""CVE (identifier)"">CVE</a>-<link href=""mw-data:TemplateStyles:r1067248974"" rel=""mw-deduplicated-inline-style""/><a class=""external text"" href=""https://cve.org/CVERecord?id=CVE-2019-1162"" rel=""nofollow"">2019-1162</a>, based on <a class=""mw-redirect"" href=""/wiki/Legacy_code"" title=""Legacy code"">legacy code</a> involving <a href=""/wiki/Text_Services_Framework#ctfmon"" title=""Text Services Framework"">Microsoft CTF and ctfmon (ctfmon.exe)</a>, that affects all <a class=""mw-redirect"" href=""/wiki/Windows"" title=""Windows"">Windows</a> versions from the older <a href=""/wiki/Windows_XP"" title=""Windows XP"">Windows XP</a> version to the most recent <a href=""/wiki/Windows_10"" title=""Windows 10"">Windows 10</a> versions; a patch to correct the flaw is currently available.<sup class=""reference"" id=""cite_ref-TP-20190814_94-0""><a href=""#cite_note-TP-20190814-94"">[94]</a></sup>
</p>, <p>All Windows versions from Windows NT 3 have been based on a file system permission system referred to as <a href=""/wiki/AGDLP"" title=""AGDLP"">AGDLP</a> (Accounts, Global, Domain Local, Permissions) in which file permissions are applied to the file/folder in the form of a 'local group' which then has other 'global groups' as members. These global groups then hold other groups or users depending on different Windows versions used. This system varies from other vendor products such as <a href=""/wiki/Linux"" title=""Linux"">Linux</a> and <a href=""/wiki/NetWare"" title=""NetWare"">NetWare</a> due to the 'static' allocation of permission being applied directly to the file or folder. However using this process of AGLP/AGDLP/AGUDLP allows a small number of static permissions to be applied and allows for easy changes to the account groups without reapplying the file permissions on the files and folders.
</p>, <p>Owing to the operating system's popularity, a number of applications have been released that aim to provide compatibility with Windows applications, either as a <a href=""/wiki/Compatibility_layer"" title=""Compatibility layer"">compatibility layer</a> for another operating system, or as a standalone system that can run software written for Windows out of the box. These include:
</p>]"
Steve Jobs,"[<p class=""mw-empty-elt"">
</p>, <p><b>Steven Paul Jobs</b> (February 24, 1955 – October 5, 2011) was an American <a href=""/wiki/Entrepreneurship"" title=""Entrepreneurship"">entrepreneur</a>, <a href=""/wiki/Industrial_design"" title=""Industrial design"">inventor</a>, <a href=""/wiki/Business_magnate"" title=""Business magnate"">business magnate</a>, <a href=""/wiki/Media_proprietor"" title=""Media proprietor"">media proprietor</a>, and investor. He was the co-founder, chairman, and <a href=""/wiki/Chief_executive_officer"" title=""Chief executive officer"">CEO</a> of <a href=""/wiki/Apple_Inc."" title=""Apple Inc."">Apple</a>; the chairman and majority shareholder of <a href=""/wiki/Pixar"" title=""Pixar"">Pixar</a>; a member of <a href=""/wiki/The_Walt_Disney_Company"" title=""The Walt Disney Company"">The Walt Disney Company</a>'s board of directors following its acquisition of Pixar; and the founder, chairman, and CEO of <a href=""/wiki/NeXT"" title=""NeXT"">NeXT</a>. He is widely recognized as a pioneer of the <a href=""/wiki/History_of_personal_computers"" title=""History of personal computers"">personal computer revolution</a> of the 1970s and 1980s, along with his early business partner and fellow Apple co-founder <a href=""/wiki/Steve_Wozniak"" title=""Steve Wozniak"">Steve Wozniak</a>.
</p>, <p>Born in San Francisco to a <a class=""mw-redirect"" href=""/wiki/Syrian"" title=""Syrian"">Syrian</a> father and a <a class=""mw-redirect"" href=""/wiki/German-American"" title=""German-American"">German-American</a> mother, Jobs was adopted shortly after his birth. Jobs attended <a href=""/wiki/Reed_College"" title=""Reed College"">Reed College</a> in 1972 before withdrawing that same year, and traveled through India in 1974 <a href=""/wiki/Hippie_trail"" title=""Hippie trail"">seeking enlightenment</a> and studying <a href=""/wiki/Buddhism_in_the_West#Emerging_mainstream_western_Buddhism"" title=""Buddhism in the West"">Zen Buddhism</a>. He and Wozniak co-founded Apple in 1976 to sell Wozniak's <a href=""/wiki/Apple_I"" title=""Apple I"">Apple I</a> personal computer. Together, the duo gained fame and wealth a year later with the <a href=""/wiki/Apple_II"" title=""Apple II"">Apple II</a>, one of the first highly successful mass-produced <a href=""/wiki/Microcomputer"" title=""Microcomputer"">microcomputers</a>. Jobs saw the commercial potential of the <a href=""/wiki/Xerox_Alto"" title=""Xerox Alto"">Xerox Alto</a> in 1979, which was <a href=""/wiki/Computer_mouse"" title=""Computer mouse"">mouse</a>-driven and had a <a href=""/wiki/Graphical_user_interface"" title=""Graphical user interface"">graphical user interface</a> (GUI). This led to the development of the unsuccessful <a href=""/wiki/Apple_Lisa"" title=""Apple Lisa"">Apple Lisa</a> in 1983, followed by the breakthrough <a href=""/wiki/Macintosh_128K"" title=""Macintosh 128K"">Macintosh</a> in 1984, the first mass-produced computer with a GUI. The Macintosh introduced the <a href=""/wiki/Desktop_publishing"" title=""Desktop publishing"">desktop publishing</a> industry in 1985 with the addition of the Apple <a href=""/wiki/LaserWriter"" title=""LaserWriter"">LaserWriter</a>, the first <a href=""/wiki/Laser_printing"" title=""Laser printing"">laser printer</a> to feature <a href=""/wiki/Vector_graphics"" title=""Vector graphics"">vector graphics</a>.
</p>, <p>Jobs was forced out of Apple in 1985 after a long power struggle with the company's board and its then-CEO <a href=""/wiki/John_Sculley"" title=""John Sculley"">John Sculley</a>. That same year, Jobs took a few Apple employees with him to found <a href=""/wiki/NeXT"" title=""NeXT"">NeXT</a>, a <a class=""mw-redirect"" href=""/wiki/Computer_platform"" title=""Computer platform"">computer platform</a> development company that specialized in computers for higher-education and business markets. In addition, he helped to develop the <a href=""/wiki/Visual_effects"" title=""Visual effects"">visual effects</a> industry when he funded the computer graphics division of <a href=""/wiki/George_Lucas"" title=""George Lucas"">George Lucas</a>'s <a href=""/wiki/Lucasfilm"" title=""Lucasfilm"">Lucasfilm</a> in 1986. The new company was Pixar, which produced the first 3D <a href=""/wiki/Computer_animation"" title=""Computer animation"">computer animated</a> feature film <i><a href=""/wiki/Toy_Story"" title=""Toy Story"">Toy Story</a></i> (1995) and went on to become a major <a href=""/wiki/Animation_studio"" title=""Animation studio"">animation studio</a>, producing <a href=""/wiki/List_of_Pixar_films"" title=""List of Pixar films"">over 20 films</a> since.
</p>, <p>Jobs became CEO of Apple in 1997, following the company's acquisition of NeXT. He was largely responsible for helping revive Apple, which had been on the verge of bankruptcy. He worked closely with English designer <a href=""/wiki/Jony_Ive"" title=""Jony Ive"">Jony Ive</a> to develop a line of products that had larger cultural ramifications, beginning in 1997 with the ""<a href=""/wiki/Think_different"" title=""Think different"">Think different</a>"" advertising campaign and leading to the <a href=""/wiki/Apple_Store"" title=""Apple Store"">Apple Store</a>, <a class=""mw-redirect"" href=""/wiki/App_Store_(iOS)"" title=""App Store (iOS)"">App Store</a>, <a href=""/wiki/IMac"" title=""IMac"">iMac</a>, <a href=""/wiki/IPad"" title=""IPad"">iPad</a>, <a href=""/wiki/IPod"" title=""IPod"">iPod</a>, <a href=""/wiki/IPhone"" title=""IPhone"">iPhone</a>, <a href=""/wiki/ITunes"" title=""ITunes"">iTunes</a>, and <a href=""/wiki/ITunes_Store"" title=""ITunes Store"">iTunes Store</a>. In 2001, the original <a href=""/wiki/Classic_Mac_OS"" title=""Classic Mac OS"">Mac OS</a> was replaced with the completely new Mac OS X (now known as <a href=""/wiki/MacOS"" title=""MacOS"">macOS</a>), based on NeXT's <a href=""/wiki/NeXTSTEP"" title=""NeXTSTEP"">NeXTSTEP</a> platform, giving the OS a modern <a href=""/wiki/Unix"" title=""Unix"">Unix</a>-based foundation for the first time. Jobs was diagnosed with a <a href=""/wiki/Pancreatic_neuroendocrine_tumor"" title=""Pancreatic neuroendocrine tumor"">pancreatic neuroendocrine tumor</a> in 2003. He died of <a href=""/wiki/Respiratory_arrest"" title=""Respiratory arrest"">respiratory arrest</a> related to the tumor at age 56 on October 5, 2011.
</p>, <p>Steven Paul Jobs was born in <a class=""mw-redirect"" href=""/wiki/San_Francisco,_California"" title=""San Francisco, California"">San Francisco, California</a>, on February 24, 1955, the son of Joanne Carole Schieble and Abdulfattah Jandali (<a class=""mw-redirect"" href=""/wiki/Arabic_language"" title=""Arabic language"">Arabic</a>: <span dir=""rtl"" lang=""ar"">عبد الفتاح الجندلي</span>). He was adopted by Clara (née Hagopian) and Paul Reinhold Jobs.<sup class=""reference"" id=""cite_ref-2""><a href=""#cite_note-2"">[2]</a></sup>
</p>, <p>Jandali, Jobs's biological father, was Syrian and went by the name ""John"". He grew up in an <a class=""mw-redirect"" href=""/wiki/Arab_Muslim"" title=""Arab Muslim"">Arab Muslim</a> household in <a href=""/wiki/Homs"" title=""Homs"">Homs</a>, Syria.<sup class=""reference"" id=""cite_ref-sg_3-0""><a href=""#cite_note-sg-3"">[3]</a></sup> While an undergraduate at the <a href=""/wiki/American_University_of_Beirut"" title=""American University of Beirut"">American University of Beirut</a> in <a href=""/wiki/Lebanon"" title=""Lebanon"">Lebanon</a>, he was a student activist and spent time in prison for his political activities.<sup class=""reference"" id=""cite_ref-sg_3-1""><a href=""#cite_note-sg-3"">[3]</a></sup> He pursued a PhD at the <a class=""mw-redirect"" href=""/wiki/University_of_Wisconsin"" title=""University of Wisconsin"">University of Wisconsin</a>, where he met Schieble, an American Catholic of German and Swiss descent.<sup class=""reference"" id=""cite_ref-sg_3-2""><a href=""#cite_note-sg-3"">[3]</a></sup><sup class=""reference"" id=""cite_ref-graff_4-0""><a href=""#cite_note-graff-4"">[4]</a></sup> As a doctoral candidate, Jandali was a <a class=""mw-redirect"" href=""/wiki/Graduate_teaching_assistant"" title=""Graduate teaching assistant"">teaching assistant</a> for a course Schieble was taking, although both were the same age.<sup class=""reference"" id=""cite_ref-5""><a href=""#cite_note-5"">[5]</a></sup> Novelist <a href=""/wiki/Mona_Simpson"" title=""Mona Simpson"">Mona Simpson</a>, Jobs's biological sister, noted that Schieble's parents were not happy that their daughter was dating a Muslim.<sup class=""reference"" id=""cite_ref-conv_6-0""><a href=""#cite_note-conv-6"">[6]</a></sup> <a href=""/wiki/Walter_Isaacson"" title=""Walter Isaacson"">Walter Isaacson</a>, author of the biography <a href=""/wiki/Steve_Jobs_(book)"" title=""Steve Jobs (book)""><i>Steve Jobs</i></a>, additionally states that Schieble's father ""threatened to cut her off completely"" if she continued the relationship.<sup class=""reference"" id=""cite_ref-graff_4-1""><a href=""#cite_note-graff-4"">[4]</a></sup>
</p>, <p>Jobs's adoptive father was a <a href=""/wiki/United_States_Coast_Guard"" title=""United States Coast Guard"">Coast Guard</a> mechanic.<sup class=""reference"" id=""cite_ref-auto_7-0""><a href=""#cite_note-auto-7"">[7]</a></sup> After leaving the Coast Guard, he married Hagopian, an American of Armenian descent, in 1946.<sup class=""reference"" id=""cite_ref-:0_8-0""><a href=""#cite_note-:0-8"">[8]</a></sup> Their attempts to start a family were halted after Hagopian had an <a href=""/wiki/Ectopic_pregnancy"" title=""Ectopic pregnancy"">ectopic pregnancy</a>, leading them to consider adoption in 1955.<sup class=""reference"" id=""cite_ref-auto_7-1""><a href=""#cite_note-auto-7"">[7]</a></sup><sup class=""reference"" id=""cite_ref-:0_8-1""><a href=""#cite_note-:0-8"">[8]</a></sup><sup class=""reference"" id=""cite_ref-FOOTNOTEIsaacson20152_9-0""><a href=""#cite_note-FOOTNOTEIsaacson20152-9"">[9]</a></sup> Hagopian's parents were survivors of the <a class=""mw-redirect"" href=""/wiki/Armenian_Genocide"" title=""Armenian Genocide"">Armenian Genocide</a>.<sup class=""reference"" id=""cite_ref-10""><a href=""#cite_note-10"">[10]</a></sup>
</p>, <p>""Of all the inventions of humans, the <a href=""/wiki/Computer"" title=""Computer"">computer</a> is going to rank near or at the top as history unfolds and we look back. It is the most awesome tool that we have ever invented. I feel incredibly lucky to be at exactly the right place in <a href=""/wiki/Silicon_Valley"" title=""Silicon Valley"">Silicon Valley</a>, at <a class=""mw-redirect"" href=""/wiki/Microcomputer_revolution"" title=""Microcomputer revolution"">exactly the right time, historically</a>, where this invention has taken form.""
</p>, <p><cite class=""left-aligned"" style="""">—Steve Jobs, 1995. From the documentary, <i><a href=""/wiki/Steve_Jobs:_The_Lost_Interview"" title=""Steve Jobs: The Lost Interview"">Steve Jobs: The Lost Interview</a>.</i><sup class=""reference"" id=""cite_ref-sjtli_11-0""><a href=""#cite_note-sjtli-11"">[11]</a></sup></cite>
</p>, <p>Schieble became pregnant with Jobs in 1954, when she and Jandali spent the summer with his family in Homs. According to Jandali, Schieble deliberately did not involve him in the process: ""Without telling me, Joanne upped and left to move to San Francisco to have the baby without anyone knowing, including me.""<sup class=""reference"" id=""cite_ref-12""><a href=""#cite_note-12"">[12]</a></sup>
</p>, <p>Schieble gave birth to Jobs in San Francisco on February 24, 1955, and chose an adoptive couple for him that was ""Catholic, well-educated, and wealthy"",<sup class=""reference"" id=""cite_ref-bite_13-0""><a href=""#cite_note-bite-13"">[13]</a></sup><sup class=""reference"" id=""cite_ref-NYT_obit_14-0""><a href=""#cite_note-NYT_obit-14"">[14]</a></sup> but the couple later changed their mind.<sup class=""reference"" id=""cite_ref-bite_13-1""><a href=""#cite_note-bite-13"">[13]</a></sup> Jobs was then placed with Paul and Clara Jobs, neither of whom had a college education, and Schieble refused to sign the adoption papers.<sup class=""reference"" id=""cite_ref-FOOTNOTEIsaacson20113–4_15-0""><a href=""#cite_note-FOOTNOTEIsaacson20113–4-15"">[15]</a></sup> She then took the matter to court in an attempt to have her baby placed with a different family,<sup class=""reference"" id=""cite_ref-bite_13-2""><a href=""#cite_note-bite-13"">[13]</a></sup> and only consented to releasing the baby to Paul and Clara after the couple pledged to pay for the boy's college education.<sup class=""reference"" id=""cite_ref-16""><a href=""#cite_note-16"">[16]</a></sup> Jobs's cousin, <a href=""/wiki/Bassma_Al_Jandaly"" title=""Bassma Al Jandaly"">Bassma Al Jandaly</a>, maintains that Jobs's birth name was Abdul Lateef Jandali.<sup class=""reference"" id=""cite_ref-17""><a href=""#cite_note-17"">[17]</a></sup>
</p>, <p>In his youth, Steve's parents took him to a Lutheran church.<sup class=""reference"" id=""cite_ref-FOOTNOTEIsaacson2011[httpswwwgooglecombookseditionSteve_Jobs8U2oAAAAQBAJhlengbpv1dqstevejobs22lutheran22pgPA14printsecfrontcover_14]_18-0""><a href=""#cite_note-FOOTNOTEIsaacson2011[httpswwwgooglecombookseditionSteve_Jobs8U2oAAAAQBAJhlengbpv1dqstevejobs22lutheran22pgPA14printsecfrontcover_14]-18"">[18]</a></sup> When Jobs was in high school, Clara admitted to his girlfriend, <a href=""/wiki/Chrisann_Brennan"" title=""Chrisann Brennan"">Chrisann Brennan</a>, that she ""was too frightened to love [Steve] for the first six months of his life ... I was scared they were going to take him away from me. Even after we won the case, Steve was so difficult a child that by the time he was two I felt we had made a mistake. I wanted to return him.""<sup class=""reference"" id=""cite_ref-bite_13-3""><a href=""#cite_note-bite-13"">[13]</a></sup> When Chrisann shared this comment with Steve, he stated that he was already aware,<sup class=""reference"" id=""cite_ref-bite_13-4""><a href=""#cite_note-bite-13"">[13]</a></sup> and would later say he was deeply loved and indulged by Paul and Clara.<sup class=""reference"" id=""cite_ref-bsj_19-0""><a href=""#cite_note-bsj-19"">[19]</a></sup><sup class=""noprint Inline-Template"" style=""white-space:nowrap;"">[<i><a href=""/wiki/Wikipedia:Citing_sources"" title=""Wikipedia:Citing sources""><span title="" (March 2018)"">page needed</span></a></i>]</sup> Many years later, Jobs's wife Laurene also noted that ""he felt he had been really blessed by having the two of them as parents.""<sup class=""reference"" id=""cite_ref-bsj_19-1""><a href=""#cite_note-bsj-19"">[19]</a></sup><sup class=""noprint Inline-Template"" style=""white-space:nowrap;"">[<i><a href=""/wiki/Wikipedia:Citing_sources"" title=""Wikipedia:Citing sources""><span title="" (March 2018)"">page needed</span></a></i>]</sup> Jobs would become upset when Paul and Clara were referred to as his ""adoptive parents""; he regarded them as his parents ""1,000%"". With regard to his biological parents, Jobs referred to them as ""my sperm and egg bank. That's not harsh, it's just the way it was, a sperm bank thing, nothing more.""<sup class=""reference"" id=""cite_ref-auto_7-2""><a href=""#cite_note-auto-7"">[7]</a></sup>
</p>, <p>""I always thought of myself as a humanities person as a kid, but I liked electronics… then I read something that one of my heroes, <a href=""/wiki/Edwin_H._Land"" title=""Edwin H. Land"">Edwin Land</a> of <a href=""/wiki/Polaroid_Corporation"" title=""Polaroid Corporation"">Polaroid</a>, said about the importance of people who could stand at the intersection of humanities and sciences, and I decided that's what I wanted to do.""
</p>, <p><cite class=""left-aligned"" style="""">From Steve Jobs<sup class=""reference"" id=""cite_ref-FOOTNOTEIsaacson201116_20-0""><a href=""#cite_note-FOOTNOTEIsaacson201116-20"">[20]</a></sup></cite>
</p>, <p>Paul Jobs worked in several jobs that included a try as a machinist,<sup class=""reference"" id=""cite_ref-21""><a href=""#cite_note-21"">[21]</a></sup>
several other jobs,<sup class=""reference"" id=""cite_ref-22""><a href=""#cite_note-22"">[22]</a></sup> and then ""back to work as a machinist.""
</p>, <p>Paul and Clara adopted Jobs's sister Patricia in 1957<sup class=""reference"" id=""cite_ref-FOOTNOTEIsaacson20115_23-0""><a href=""#cite_note-FOOTNOTEIsaacson20115-23"">[23]</a></sup> and by 1959 the family had moved to the <a href=""/wiki/Monta_Loma,_Mountain_View"" title=""Monta Loma, Mountain View"">Monta Loma</a> neighborhood in <a href=""/wiki/Mountain_View,_California"" title=""Mountain View, California"">Mountain View, California</a>.<sup class=""reference"" id=""cite_ref-24""><a href=""#cite_note-24"">[24]</a></sup> It was during this time that Paul built a workbench in his garage for his son in order to ""pass along his love of mechanics.""<sup class=""reference"" id=""cite_ref-FOOTNOTEIsaacson20115–6_25-0""><a href=""#cite_note-FOOTNOTEIsaacson20115–6-25"">[25]</a></sup> Jobs, meanwhile, admired his father's craftsmanship ""because he knew how to build anything. If we needed a cabinet, he would build it. When he built our fence, he gave me a hammer so I could work with him ... I wasn't that into fixing cars ... but I was eager to hang out with my dad.""<sup class=""reference"" id=""cite_ref-FOOTNOTEIsaacson20115–6_25-1""><a href=""#cite_note-FOOTNOTEIsaacson20115–6-25"">[25]</a></sup> By the time he was ten, Jobs was deeply involved in electronics and befriended many of the engineers who lived in the neighborhood.<sup class=""reference"" id=""cite_ref-journeyisreward_26-0""><a href=""#cite_note-journeyisreward-26"">[26]</a></sup><sup class=""noprint Inline-Template"" style=""white-space:nowrap;"">[<i><a href=""/wiki/Wikipedia:Citing_sources"" title=""Wikipedia:Citing sources""><span title="" (March 2018)"">page needed</span></a></i>]</sup> He had difficulty making friends with children his own age, however, and was seen by his classmates as a ""loner.""<sup class=""reference"" id=""cite_ref-journeyisreward_26-1""><a href=""#cite_note-journeyisreward-26"">[26]</a></sup><sup class=""noprint Inline-Template"" style=""white-space:nowrap;"">[<i><a href=""/wiki/Wikipedia:Citing_sources"" title=""Wikipedia:Citing sources""><span title="" (March 2018)"">page needed</span></a></i>]</sup>
</p>, <p>Jobs had difficulty functioning in a traditional classroom, tended to resist authority figures, frequently misbehaved, and was suspended a few times.<sup class=""reference"" id=""cite_ref-journeyisreward_26-2""><a href=""#cite_note-journeyisreward-26"">[26]</a></sup><sup class=""noprint Inline-Template"" style=""white-space:nowrap;"">[<i><a href=""/wiki/Wikipedia:Citing_sources"" title=""Wikipedia:Citing sources""><span title="" (March 2018)"">page needed</span></a></i>]</sup> Clara had taught him to read as a toddler, and Jobs stated that he was ""pretty bored in school and [had] turned into a little terror... you should have seen us in the third grade, we basically destroyed the teacher.""<sup class=""reference"" id=""cite_ref-journeyisreward_26-3""><a href=""#cite_note-journeyisreward-26"">[26]</a></sup><sup class=""noprint Inline-Template"" style=""white-space:nowrap;"">[<i><a href=""/wiki/Wikipedia:Citing_sources"" title=""Wikipedia:Citing sources""><span title="" (March 2018)"">page needed</span></a></i>]</sup> He frequently played pranks on others at Monta Loma Elementary School in Mountain View.<sup class=""reference"" id=""cite_ref-FOOTNOTEIsaacson201112–13_28-0""><a href=""#cite_note-FOOTNOTEIsaacson201112–13-28"">[28]</a></sup> His father Paul (who was abused as a child) never reprimanded him, however, and instead blamed the school for not challenging his brilliant son.<sup class=""reference"" id=""cite_ref-FOOTNOTEIsaacson201112–13_28-1""><a href=""#cite_note-FOOTNOTEIsaacson201112–13-28"">[28]</a></sup>
</p>, <p>Jobs would later credit his fourth grade teacher, Imogene ""Teddy"" Hill, with turning him around: ""She taught an advanced fourth grade class and it took her about a month to get hip to my situation. She bribed me into learning. She would say, 'I really want you to finish this workbook. I'll give you five bucks if you finish it.' That really kindled a passion in me for learning things! I learned more that year than I think I learned in any other year in school. They wanted me to skip the next two years in grade school and go straight to junior high to learn a foreign language but my parents very wisely wouldn't let it happen.""<sup class=""reference"" id=""cite_ref-journeyisreward_26-4""><a href=""#cite_note-journeyisreward-26"">[26]</a></sup><sup class=""noprint Inline-Template"" style=""white-space:nowrap;"">[<i><a href=""/wiki/Wikipedia:Citing_sources"" title=""Wikipedia:Citing sources""><span title="" (March 2018)"">page needed</span></a></i>]</sup> Jobs skipped the 5th grade and transferred to the 6th grade at Crittenden Middle School in Mountain View<sup class=""reference"" id=""cite_ref-journeyisreward_26-5""><a href=""#cite_note-journeyisreward-26"">[26]</a></sup><sup class=""noprint Inline-Template"" style=""white-space:nowrap;"">[<i><a href=""/wiki/Wikipedia:Citing_sources"" title=""Wikipedia:Citing sources""><span title="" (March 2018)"">page needed</span></a></i>]</sup> where he became a ""socially awkward loner"".<sup class=""reference"" id=""cite_ref-FOOTNOTEIsaacson201513_29-0""><a href=""#cite_note-FOOTNOTEIsaacson201513-29"">[29]</a></sup> Jobs was often ""bullied"" at Crittenden Middle, and in the middle of 7th grade, he gave his parents an ultimatum: they had to either take him out of Crittenden or he would drop out of school.<sup class=""reference"" id=""cite_ref-FOOTNOTEIsaacson201113–14_30-0""><a href=""#cite_note-FOOTNOTEIsaacson201113–14-30"">[30]</a></sup>
</p>, <p>Though the Jobs family was not well off, they used all their savings in 1967 to buy a new home, allowing Jobs to change schools.<sup class=""reference"" id=""cite_ref-journeyisreward_26-6""><a href=""#cite_note-journeyisreward-26"">[26]</a></sup><sup class=""noprint Inline-Template"" style=""white-space:nowrap;"">[<i><a href=""/wiki/Wikipedia:Citing_sources"" title=""Wikipedia:Citing sources""><span title="" (March 2018)"">page needed</span></a></i>]</sup> The new house (a three-bedroom home on Crist Drive in <a href=""/wiki/Los_Altos,_California"" title=""Los Altos, California"">Los Altos, California</a>) was in the better <a href=""/wiki/Cupertino_Union_School_District"" title=""Cupertino Union School District"">Cupertino School District</a>, <a href=""/wiki/Cupertino,_California"" title=""Cupertino, California"">Cupertino, California</a>,<sup class=""reference"" id=""cite_ref-FOOTNOTEIsaacson201114_31-0""><a href=""#cite_note-FOOTNOTEIsaacson201114-31"">[31]</a></sup> and was embedded in an environment that was even more heavily populated with engineering families than the Mountain View area was.<sup class=""reference"" id=""cite_ref-journeyisreward_26-7""><a href=""#cite_note-journeyisreward-26"">[26]</a></sup><sup class=""noprint Inline-Template"" style=""white-space:nowrap;"">[<i><a href=""/wiki/Wikipedia:Citing_sources"" title=""Wikipedia:Citing sources""><span title="" (March 2018)"">page needed</span></a></i>]</sup> The house was declared a historic site in 2013, as it was the first site for Apple Computer;<sup class=""reference"" id=""cite_ref-hissite_27-1""><a href=""#cite_note-hissite-27"">[27]</a></sup> as of 2013, it was owned by Jobs's sister, Patty, and occupied by his step-mother, Marilyn.<sup class=""reference"" id=""cite_ref-piece_32-0""><a href=""#cite_note-piece-32"">[32]</a></sup>
</p>, <p>When he was 13 in 1968, Jobs was given a summer job by <a href=""/wiki/Bill_Hewlett"" title=""Bill Hewlett"">Bill Hewlett</a> (of <a href=""/wiki/Hewlett-Packard"" title=""Hewlett-Packard"">Hewlett-Packard</a>) after Jobs cold-called him to ask for parts for an electronics project.<sup class=""reference"" id=""cite_ref-journeyisreward_26-8""><a href=""#cite_note-journeyisreward-26"">[26]</a></sup><sup class=""noprint Inline-Template"" style=""white-space:nowrap;"">[<i><a href=""/wiki/Wikipedia:Citing_sources"" title=""Wikipedia:Citing sources""><span title="" (March 2018)"">page needed</span></a></i>]</sup>
</p>, <p>The location of the Los Altos home meant that Jobs would be able to attend nearby <a class=""mw-redirect"" href=""/wiki/Homestead_High_School_(Cupertino,_California)"" title=""Homestead High School (Cupertino, California)"">Homestead High School</a>, which had strong ties to <a href=""/wiki/Silicon_Valley"" title=""Silicon Valley"">Silicon Valley</a>.<sup class=""reference"" id=""cite_ref-FOOTNOTEIsaacson201116_20-1""><a href=""#cite_note-FOOTNOTEIsaacson201116-20"">[20]</a></sup> He began his first year there in late 1968 along with <a href=""/wiki/Bill_Fernandez"" title=""Bill Fernandez"">Bill Fernandez</a>.<sup class=""reference"" id=""cite_ref-journeyisreward_26-9""><a href=""#cite_note-journeyisreward-26"">[26]</a></sup><sup class=""noprint Inline-Template"" style=""white-space:nowrap;"">[<i><a href=""/wiki/Wikipedia:Citing_sources"" title=""Wikipedia:Citing sources""><span title="" (March 2018)"">page needed</span></a></i>]</sup> (Fernandez introduced Jobs to Steve Wozniak, and would later be Apple's first employee.) Neither Jobs nor Fernandez (whose father was a lawyer) came from engineering households and thus decided to enroll in John McCollum's ""Electronics 1.""<sup class=""reference"" id=""cite_ref-journeyisreward_26-10""><a href=""#cite_note-journeyisreward-26"">[26]</a></sup><sup class=""noprint Inline-Template"" style=""white-space:nowrap;"">[<i><a href=""/wiki/Wikipedia:Citing_sources"" title=""Wikipedia:Citing sources""><span title="" (March 2018)"">page needed</span></a></i>]</sup> McCollum and the rebellious Jobs (who had grown his hair long and become involved in the growing counterculture) would eventually clash and Jobs began to lose interest in the class.<sup class=""reference"" id=""cite_ref-journeyisreward_26-11""><a href=""#cite_note-journeyisreward-26"">[26]</a></sup><sup class=""noprint Inline-Template"" style=""white-space:nowrap;"">[<i><a href=""/wiki/Wikipedia:Citing_sources"" title=""Wikipedia:Citing sources""><span title="" (March 2018)"">page needed</span></a></i>]</sup>
</p>, <p>He underwent a change during mid-1970: ""I got stoned for the first time; I discovered Shakespeare, <a href=""/wiki/Dylan_Thomas"" title=""Dylan Thomas"">Dylan Thomas</a>, and all that classic stuff. I read <i><a class=""mw-redirect"" href=""/wiki/Moby_Dick"" title=""Moby Dick"">Moby Dick</a></i> and went back as a junior taking creative writing classes.""<sup class=""reference"" id=""cite_ref-journeyisreward_26-12""><a href=""#cite_note-journeyisreward-26"">[26]</a></sup><sup class=""noprint Inline-Template"" style=""white-space:nowrap;"">[<i><a href=""/wiki/Wikipedia:Citing_sources"" title=""Wikipedia:Citing sources""><span title="" (March 2018)"">page needed</span></a></i>]</sup> Jobs also later noted to his official biographer that ""I started to listen to music a whole lot, and I started to read more outside of just science and technology—<a href=""/wiki/William_Shakespeare"" title=""William Shakespeare"">Shakespeare</a>, <a href=""/wiki/Plato"" title=""Plato"">Plato</a>. I loved <i><a href=""/wiki/King_Lear"" title=""King Lear"">King Lear</a></i> ... when I was a senior I had this phenomenal <a href=""/wiki/AP_English_Literature_and_Composition"" title=""AP English Literature and Composition"">AP English class</a>. The teacher was this guy who looked like <a href=""/wiki/Ernest_Hemingway"" title=""Ernest Hemingway"">Ernest Hemingway</a>. He took a bunch of us snowshoeing in Yosemite.""<sup class=""reference"" id=""cite_ref-FOOTNOTEIsaacson201519_33-0""><a href=""#cite_note-FOOTNOTEIsaacson201519-33"">[33]</a></sup> During his last two years at Homestead High, Jobs developed two different interests: electronics and literature.<sup class=""reference"" id=""cite_ref-FOOTNOTEIsaacson201519_33-1""><a href=""#cite_note-FOOTNOTEIsaacson201519-33"">[33]</a></sup> These dual interests were particularly reflected during Jobs's senior year as his best friends were Wozniak and his first girlfriend, the artistic Homestead junior <a href=""/wiki/Chrisann_Brennan"" title=""Chrisann Brennan"">Chrisann Brennan</a>.<sup class=""noprint Inline-Template Template-Fact"" style=""white-space:nowrap;"">[<i><a href=""/wiki/Wikipedia:Citation_needed"" title=""Wikipedia:Citation needed""><span title=""This claim needs references to reliable sources. (September 2020)"">citation needed</span></a></i>]</sup>
</p>, <p>In 1971 after Wozniak began attending <a href=""/wiki/University_of_California,_Berkeley"" title=""University of California, Berkeley"">University of California, Berkeley</a>, Jobs would visit him there a few times a week. This experience led him to study in nearby <a href=""/wiki/Stanford_University"" title=""Stanford University"">Stanford University</a>'s student union. Jobs also decided that rather than join the electronics club, he would put on light shows with a friend for Homestead's avant-garde Jazz program. He was described by a Homestead classmate as ""kind of a brain and kind of a hippie ... but he never fit into either group. He was smart enough to be a nerd, but wasn't nerdy. And he was too intellectual for the hippies, who just wanted to get wasted all the time. He was kind of an outsider. In high school everything revolved around what group you were in, and if you weren't in a carefully defined group, you weren't anybody. He was an individual, in a world where individuality was suspect."" By his senior year in late 1971, he was taking freshman English class at Stanford and working on a Homestead underground film project with Chrisann Brennan.<sup class=""reference"" id=""cite_ref-journeyisreward_26-13""><a href=""#cite_note-journeyisreward-26"">[26]</a></sup><sup class=""noprint Inline-Template"" style=""white-space:nowrap;"">[<i><a href=""/wiki/Wikipedia:Citing_sources"" title=""Wikipedia:Citing sources""><span title="" (March 2018)"">page needed</span></a></i>]</sup>
</p>, <p>Around that time, Wozniak designed a low-cost digital ""<a href=""/wiki/Blue_box"" title=""Blue box"">blue box</a>"" to generate the necessary tones to manipulate the telephone network, allowing free long-distance calls. Jobs decided then to sell them and split the profit with Wozniak. The clandestine sales of the illegal blue boxes went well and perhaps planted the seed in Jobs's mind that electronics could be both fun and profitable.<sup class=""reference"" id=""cite_ref-Joomla_34-0""><a href=""#cite_note-Joomla-34"">[34]</a></sup> Jobs, in a 1994 interview, recalled that it took six months for him and Wozniak to figure out how to build the blue boxes.<sup class=""reference"" id=""cite_ref-Steve_Jobs_1994_Uncut_Interview_35-0""><a href=""#cite_note-Steve_Jobs_1994_Uncut_Interview-35"">[35]</a></sup> Jobs later reflected that had it not been for Wozniak's blue boxes, ""there wouldn't have been an Apple"".<sup class=""reference"" id=""cite_ref-FOOTNOTEIsaacson201530_36-0""><a href=""#cite_note-FOOTNOTEIsaacson201530-36"">[36]</a></sup> He states it showed them that they could take on large companies and beat them.<sup class=""reference"" id=""cite_ref-37""><a href=""#cite_note-37"">[37]</a></sup><sup class=""reference"" id=""cite_ref-Steve_Jobs:_Visionary_Entrepreneur_38-0""><a href=""#cite_note-Steve_Jobs:_Visionary_Entrepreneur-38"">[38]</a></sup>
</p>, <p>By his senior year of high school, Jobs began using <a href=""/wiki/Lysergic_acid_diethylamide"" title=""Lysergic acid diethylamide"">LSD</a>.<sup class=""reference"" id=""cite_ref-FOOTNOTEIsaacson201119_39-0""><a href=""#cite_note-FOOTNOTEIsaacson201119-39"">[39]</a></sup> He later recalled that on one occasion he consumed it in a wheat field outside Sunnyvale, and experienced ""the most wonderful feeling of my life up to that point"".<sup class=""reference"" id=""cite_ref-FOOTNOTEIsaacson201131–32_40-0""><a href=""#cite_note-FOOTNOTEIsaacson201131–32-40"">[40]</a></sup> In mid-1972, after graduation and before leaving for <a href=""/wiki/Reed_College"" title=""Reed College"">Reed College</a>, Jobs and Brennan rented a house from their other roommate, Al.<sup class=""reference"" id=""cite_ref-rs_41-0""><a href=""#cite_note-rs-41"">[41]</a></sup>
</p>, <p>""I was interested in <a href=""/wiki/Counterculture_of_the_1960s#Religion,_spirituality_and_the_occult"" title=""Counterculture of the 1960s"">Eastern mysticism</a> which hit the shores about then. At <a href=""/wiki/Reed_College"" title=""Reed College"">Reed</a> there was a constant flow of people stopping by – from <a href=""/wiki/Timothy_Leary"" title=""Timothy Leary"">Timothy Leary</a> and <a class=""mw-redirect"" href=""/wiki/Richard_Alpert"" title=""Richard Alpert"">Richard Alpert</a>, to <a href=""/wiki/Gary_Snyder"" title=""Gary Snyder"">Gary Snyder</a>. There was a constant flow of intellectual questioning about the truth of life. That was the time when every college student in the country read <a href=""/wiki/Be_Here_Now_(book)"" title=""Be Here Now (book)"">Be Here Now</a> and <a href=""/wiki/Diet_for_a_Small_Planet"" title=""Diet for a Small Planet"">Diet for a Small Planet</a>.""
</p>, <p><cite class=""left-aligned"" style="""">—Steve Jobs<sup class=""reference"" id=""cite_ref-journeyisreward_26-14""><a href=""#cite_note-journeyisreward-26"">[26]</a></sup><sup class=""noprint Inline-Template"" style=""white-space:nowrap;"">[<i><a href=""/wiki/Wikipedia:Citing_sources"" title=""Wikipedia:Citing sources""><span title="" (March 2018)"">page needed</span></a></i>]</sup></cite>
</p>, <p>In September 1972, Jobs enrolled at <a href=""/wiki/Reed_College"" title=""Reed College"">Reed College</a> in <a href=""/wiki/Portland,_Oregon"" title=""Portland, Oregon"">Portland, Oregon</a>.<sup class=""reference"" id=""cite_ref-42""><a href=""#cite_note-42"">[42]</a></sup> He insisted on applying only to Reed although it was an expensive school that Paul and Clara could ill afford.<sup class=""reference"" id=""cite_ref-FOOTNOTEIsaacson201533_43-0""><a href=""#cite_note-FOOTNOTEIsaacson201533-43"">[43]</a></sup> Jobs soon befriended <a href=""/wiki/Robert_Friedland"" title=""Robert Friedland"">Robert Friedland</a>,<sup class=""reference"" id=""cite_ref-FOOTNOTEIsaacson201537_44-0""><a href=""#cite_note-FOOTNOTEIsaacson201537-44"">[44]</a></sup> who was Reed's <a href=""/wiki/Student_government_president"" title=""Student government president"">student body president</a> at that time.<sup class=""reference"" id=""cite_ref-journeyisreward_26-15""><a href=""#cite_note-journeyisreward-26"">[26]</a></sup><sup class=""noprint Inline-Template"" style=""white-space:nowrap;"">[<i><a href=""/wiki/Wikipedia:Citing_sources"" title=""Wikipedia:Citing sources""><span title="" (March 2018)"">page needed</span></a></i>]</sup> Brennan remained involved with Jobs while he was at Reed. He later asked her to come and live with him in a house he rented near the Reed campus, but she refused.
</p>, <p>After just one semester, Jobs dropped out of Reed College without telling his parents.<sup class=""reference"" id=""cite_ref-FOOTNOTESchlender201630_45-0""><a href=""#cite_note-FOOTNOTESchlender201630-45"">[45]</a></sup> Jobs later explained that he decided to drop out because he did not want to spend his parents' money on an education that seemed meaningless to him.<sup class=""reference"" id=""cite_ref-FOOTNOTEIsaacson201540–41_46-0""><a href=""#cite_note-FOOTNOTEIsaacson201540–41-46"">[46]</a></sup> He continued to attend by auditing his classes,<sup class=""reference"" id=""cite_ref-FOOTNOTEIsaacson201540–41_46-1""><a href=""#cite_note-FOOTNOTEIsaacson201540–41-46"">[46]</a></sup> which included a course on <a href=""/wiki/Calligraphy"" title=""Calligraphy"">calligraphy</a> that was taught by <a href=""/wiki/Robert_Palladino"" title=""Robert Palladino"">Robert Palladino</a>. In a 2005 commencement speech at <a href=""/wiki/Stanford_University"" title=""Stanford University"">Stanford University</a>, Jobs stated that during this period, he slept on the floor in friends' dorm rooms, returned Coke bottles for food money, and got weekly free meals at the local <a href=""/wiki/International_Society_for_Krishna_Consciousness"" title=""International Society for Krishna Consciousness"">Hare Krishna</a> temple. In that same speech, Jobs said: ""If I had never dropped in on that single <a href=""/wiki/Calligraphy"" title=""Calligraphy"">calligraphy</a> course in college, the Mac would have never had multiple <a href=""/wiki/Typeface"" title=""Typeface"">typefaces</a> or proportionally spaced fonts.""<sup class=""reference"" id=""cite_ref-commencementJune2005_47-0""><a href=""#cite_note-commencementJune2005-47"">[47]</a></sup>
</p>, <p>I was lucky to get into computers when it was a very young and idealistic industry. There weren't many degrees offered in computer science, so people in computers were brilliant people from mathematics, physics, music, zoology, whatever. They loved it, and no one was really in it for the money [...] There are people around here who start companies just to make money, but the great companies, well, that's not what they're about.""
</p>, <p><cite class=""left-aligned"" style="""">—Steve Jobs<sup class=""reference"" id=""cite_ref-threefaces_48-0""><a href=""#cite_note-threefaces-48"">[48]</a></sup></cite>
</p>, <p>In February 1974, Jobs returned to his parents' home in Los Altos and began looking for a job.<sup class=""reference"" id=""cite_ref-FOOTNOTEIsaacson201142–43_49-0""><a href=""#cite_note-FOOTNOTEIsaacson201142–43-49"">[49]</a></sup> He was soon hired by <a href=""/wiki/Atari,_Inc."" title=""Atari, Inc."">Atari, Inc.</a> in <a href=""/wiki/Los_Gatos,_California"" title=""Los Gatos, California"">Los Gatos, California</a>, which gave him a job as a <a href=""/wiki/Technician"" title=""Technician"">technician</a>.<sup class=""reference"" id=""cite_ref-FOOTNOTEIsaacson201142–43_49-1""><a href=""#cite_note-FOOTNOTEIsaacson201142–43-49"">[49]</a></sup><sup class=""reference"" id=""cite_ref-intoday1_50-0""><a href=""#cite_note-intoday1-50"">[50]</a></sup> Back in 1973, <a href=""/wiki/Steve_Wozniak"" title=""Steve Wozniak"">Steve Wozniak</a> designed his own version of the classic video game <i><a href=""/wiki/Pong"" title=""Pong"">Pong</a></i> and gave the board to Jobs. According to Wozniak, Atari only hired Jobs because he took the board down to the company, and they thought that he had built it himself.<sup class=""reference"" id=""cite_ref-51""><a href=""#cite_note-51"">[51]</a></sup> Atari's cofounder <a href=""/wiki/Nolan_Bushnell"" title=""Nolan Bushnell"">Nolan Bushnell</a> later described him as ""difficult but valuable"", pointing out that ""he was very often the smartest guy in the room, and he would let people know that.""<sup class=""reference"" id=""cite_ref-MercuryBushnell_52-0""><a href=""#cite_note-MercuryBushnell-52"">[52]</a></sup>
</p>, <p>During this period, Jobs and Brennan remained involved with each other while continuing to see other people. By early 1974, Jobs was living what Brennan describes as a ""simple life"" in a <a href=""/wiki/Los_Gatos,_California"" title=""Los Gatos, California"">Los Gatos</a> cabin, working at <a href=""/wiki/Atari"" title=""Atari"">Atari</a>, and saving money for <a href=""/wiki/Hippie_trail"" title=""Hippie trail"">his impending trip</a> to <a href=""/wiki/India"" title=""India"">India</a>.<sup class=""noprint Inline-Template Template-Fact"" style=""white-space:nowrap;"">[<i><a href=""/wiki/Wikipedia:Citation_needed"" title=""Wikipedia:Citation needed""><span title=""This claim needs references to reliable sources. (September 2020)"">citation needed</span></a></i>]</sup>
</p>, <p>Jobs traveled to India in mid-1974<sup class=""reference"" id=""cite_ref-53""><a href=""#cite_note-53"">[53]</a></sup> to visit <a href=""/wiki/Neem_Karoli_Baba"" title=""Neem Karoli Baba"">Neem Karoli Baba</a><sup class=""reference"" id=""cite_ref-Il_santone_della_Silicon_Valley_che_ha_conquistato_i_tecno-boss_54-0""><a href=""#cite_note-Il_santone_della_Silicon_Valley_che_ha_conquistato_i_tecno-boss-54"">[54]</a></sup> at his Kainchi <a href=""/wiki/Ashram"" title=""Ashram"">ashram</a> with his Reed friend (and eventual Apple employee) <a href=""/wiki/Daniel_Kottke"" title=""Daniel Kottke"">Daniel Kottke</a>, in search of <a class=""mw-redirect"" href=""/wiki/Spiritual_enlightenment"" title=""Spiritual enlightenment"">spiritual enlightenment</a>. When they got to the Neem Karoli ashram, it was almost deserted because Neem Karoli Baba had died in September 1973.<sup class=""reference"" id=""cite_ref-intoday1_50-1""><a href=""#cite_note-intoday1-50"">[50]</a></sup> Then they made a long trek up a dry riverbed to an ashram of <a href=""/wiki/Haidakhan_Babaji"" title=""Haidakhan Babaji"">Haidakhan Babaji</a>.<sup class=""reference"" id=""cite_ref-intoday1_50-2""><a href=""#cite_note-intoday1-50"">[50]</a></sup>
</p>, <p>After seven months, Jobs left India<sup class=""reference"" id=""cite_ref-55""><a href=""#cite_note-55"">[55]</a></sup> and returned to the US ahead of Daniel Kottke.<sup class=""reference"" id=""cite_ref-intoday1_50-3""><a href=""#cite_note-intoday1-50"">[50]</a></sup> Jobs had changed his appearance; his head was shaved and he wore traditional Indian clothing.<sup class=""reference"" id=""cite_ref-Andrews_56-0""><a href=""#cite_note-Andrews-56"">[56]</a></sup><sup class=""reference"" id=""cite_ref-scotsman_57-0""><a href=""#cite_note-scotsman-57"">[57]</a></sup> During this time, Jobs experimented with <a href=""/wiki/Psychedelic_drug"" title=""Psychedelic drug"">psychedelics</a>, later calling his <a class=""mw-redirect"" href=""/wiki/LSD"" title=""LSD"">LSD</a> experiences ""one of the two or three most important things [he had] done in [his] life"".<sup class=""reference"" id=""cite_ref-Markoff2005_58-0""><a href=""#cite_note-Markoff2005-58"">[58]</a></sup><sup class=""reference"" id=""cite_ref-SecClear_59-0""><a href=""#cite_note-SecClear-59"">[59]</a></sup> He spent a period at the All One Farm, a <a href=""/wiki/History_of_the_hippie_movement#New_Communalism"" title=""History of the hippie movement"">commune</a> in <a href=""/wiki/Oregon"" title=""Oregon"">Oregon</a> that was owned by <a href=""/wiki/Robert_Friedland#Early_life_and_education"" title=""Robert Friedland"">Robert Friedland</a>. Brennan joined him there for a period.<sup class=""noprint Inline-Template Template-Fact"" style=""white-space:nowrap;"">[<i><a href=""/wiki/Wikipedia:Citation_needed"" title=""Wikipedia:Citation needed""><span title=""This claim needs references to reliable sources. (September 2020)"">citation needed</span></a></i>]</sup>
</p>, <p>During this time period, Jobs and Brennan both became practitioners of <a href=""/wiki/Zen"" title=""Zen"">Zen</a> <a href=""/wiki/Buddhism"" title=""Buddhism"">Buddhism</a> through the Zen master <a href=""/wiki/K%C5%8Dbun_Chino_Otogawa"" title=""Kōbun Chino Otogawa"">Kōbun Chino Otogawa</a>. Jobs was living in his parents' backyard toolshed, which he had converted into a bedroom.<sup class=""noprint Inline-Template Template-Fact"" style=""white-space:nowrap;"">[<i><a href=""/wiki/Wikipedia:Citation_needed"" title=""Wikipedia:Citation needed""><span title=""This claim needs references to reliable sources. (September 2020)"">citation needed</span></a></i>]</sup> Jobs engaged in lengthy <a href=""/wiki/Retreat_(spiritual)#Buddhism"" title=""Retreat (spiritual)"">meditation retreats</a> at the <a href=""/wiki/Tassajara_Zen_Mountain_Center"" title=""Tassajara Zen Mountain Center"">Tassajara Zen Mountain Center</a>, the oldest <a href=""/wiki/S%C5%8Dt%C5%8D"" title=""Sōtō"">Sōtō Zen</a> monastery in the US.<sup class=""reference"" id=""cite_ref-60""><a href=""#cite_note-60"">[60]</a></sup> He considered taking up monastic residence at <a href=""/wiki/Eihei-ji"" title=""Eihei-ji"">Eihei-ji</a> in <a href=""/wiki/Japan"" title=""Japan"">Japan</a>, and maintained a lifelong appreciation for Zen.<sup class=""reference"" id=""cite_ref-61""><a href=""#cite_note-61"">[61]</a></sup>
</p>, <p>In mid-1975, after returning to Atari, Jobs was assigned to create a <a class=""mw-redirect"" href=""/wiki/Circuit_board"" title=""Circuit board"">circuit board</a> for the <a href=""/wiki/Arcade_game"" title=""Arcade game"">arcade</a> video game <i><a class=""mw-redirect"" href=""/wiki/Breakout_(arcade_game)"" title=""Breakout (arcade game)"">Breakout</a></i>.<sup class=""reference"" id=""cite_ref-FOOTNOTEIsaacson201152–54_62-0""><a href=""#cite_note-FOOTNOTEIsaacson201152–54-62"">[62]</a></sup> According to Bushnell, Atari offered <span style=""white-space: nowrap"">US$100</span> for each <a href=""/wiki/Transistor%E2%80%93transistor_logic"" title=""Transistor–transistor logic"">TTL</a> chip that was eliminated in the machine. Jobs had little specialized knowledge of circuit board design and made a deal with Wozniak to split the fee evenly between them if Wozniak could minimize the number of chips. Much to the amazement of Atari engineers, Wozniak reduced the TTL count to 46, a design so tight that it was impossible to reproduce on an assembly line.<sup class=""reference"" id=""cite_ref-63""><a href=""#cite_note-63"">[63]</a></sup> According to Wozniak, Jobs told him that Atari gave them only $700 (instead of the $5,000 paid out), and that Wozniak's share was thus $350.<sup class=""reference"" id=""cite_ref-breakout_64-0""><a href=""#cite_note-breakout-64"">[64]</a></sup> Wozniak did not learn about the actual bonus until ten years later, but said that if Jobs had told him about it and explained that he needed the money, Wozniak would have given it to him.<sup class=""reference"" id=""cite_ref-FOOTNOTEIsaacson2015104–107_65-0""><a href=""#cite_note-FOOTNOTEIsaacson2015104–107-65"">[65]</a></sup>
</p>, <p>Jobs and Wozniak attended meetings of the <a href=""/wiki/Homebrew_Computer_Club"" title=""Homebrew Computer Club"">Homebrew Computer Club</a> in 1975, which was a stepping stone to the development and marketing of the first Apple computer.<sup class=""reference"" id=""cite_ref-NYT_obit_14-1""><a href=""#cite_note-NYT_obit-14"">[14]</a></sup>
</p>, <p>""Basically <a href=""/wiki/Steve_Wozniak"" title=""Steve Wozniak"">Steve Wozniak</a> and I invented the Apple because we wanted a personal computer. Not only couldn't we afford the computers that were on the market, those computers were impractical for us to use. We needed a <a href=""/wiki/Volkswagen_Type_2"" title=""Volkswagen Type 2"">Volkswagen</a>. The Volkswagen isn't as fast or comfortable as other ways of traveling, but the VW owners can go where they want, when they want and with whom they want. The VW owners have personal control of their car.""
</p>, <p><cite class=""left-aligned"" style="""">—Steve Jobs<sup class=""reference"" id=""cite_ref-journeyisreward_26-16""><a href=""#cite_note-journeyisreward-26"">[26]</a></sup><sup class=""noprint Inline-Template"" style=""white-space:nowrap;"">[<i><a href=""/wiki/Wikipedia:Citing_sources"" title=""Wikipedia:Citing sources""><span title="" (March 2018)"">page needed</span></a></i>]</sup></cite>
</p>, <p>By March 1976, Wozniak completed the basic design of the <a href=""/wiki/Apple_I"" title=""Apple I"">Apple I</a> computer and showed it to Jobs, who suggested that they sell it; Wozniak was at first skeptical of the idea but later agreed.<sup class=""reference"" id=""cite_ref-FOOTNOTELinzmayer20045–6_66-0""><a href=""#cite_note-FOOTNOTELinzmayer20045–6-66"">[66]</a></sup> In April of that same year, Jobs, Wozniak, and administrative overseer <a href=""/wiki/Ronald_Wayne"" title=""Ronald Wayne"">Ronald Wayne</a> founded Apple Computer Company (now called <a href=""/wiki/Apple_Inc."" title=""Apple Inc."">Apple Inc.</a>) as a <a class=""mw-redirect"" href=""/wiki/Business_partnership"" title=""Business partnership"">business partnership</a> in Jobs's parents' Crist Drive home on April 1, 1976.<sup class=""reference"" id=""cite_ref-FOOTNOTELinzmayer20046–8_67-0""><a href=""#cite_note-FOOTNOTELinzmayer20046–8-67"">[67]</a></sup> The operation originally started in Jobs's bedroom and later moved to the garage.<sup class=""reference"" id=""cite_ref-FOOTNOTELinzmayer20046–8_67-1""><a href=""#cite_note-FOOTNOTELinzmayer20046–8-67"">[67]</a></sup><sup class=""reference"" id=""cite_ref-Linzmayer01_68-0""><a href=""#cite_note-Linzmayer01-68"">[68]</a></sup> Wayne stayed only a short time, leaving Jobs and Wozniak as the active primary cofounders of the company.<sup class=""reference"" id=""cite_ref-CNN_69-0""><a href=""#cite_note-CNN-69"">[69]</a></sup> The two decided on the name ""Apple"" after Jobs returned from the All One Farm commune in Oregon and told Wozniak about his time spent in the farm's <a class=""mw-redirect"" href=""/wiki/Apple_orchard"" title=""Apple orchard"">apple orchard</a>.<sup class=""reference"" id=""cite_ref-70""><a href=""#cite_note-70"">[70]</a></sup> Jobs originally planned to produce bare <a href=""/wiki/Printed_circuit_board"" title=""Printed circuit board"">printed circuit boards</a> of the Apple I and sell them to computer hobbyists for $50 each.<sup class=""reference"" id=""cite_ref-FOOTNOTELinzmayer5–7_71-0""><a href=""#cite_note-FOOTNOTELinzmayer5–7-71"">[71]</a></sup><sup class=""reference"" id=""cite_ref-FOOTNOTESchlender201639–40_72-0""><a href=""#cite_note-FOOTNOTESchlender201639–40-72"">[72]</a></sup> To raise the money they needed to build the first batch of the circuit boards, Wozniak sold his <a href=""/wiki/HP-65"" title=""HP-65"">HP scientific calculator</a> and Jobs sold his <a href=""/wiki/Volkswagen_Type_2"" title=""Volkswagen Type 2"">Volkswagen van</a>.<sup class=""reference"" id=""cite_ref-FOOTNOTELinzmayer5–7_71-1""><a href=""#cite_note-FOOTNOTELinzmayer5–7-71"">[71]</a></sup><sup class=""reference"" id=""cite_ref-FOOTNOTESchlender201639–40_72-1""><a href=""#cite_note-FOOTNOTESchlender201639–40-72"">[72]</a></sup> Later that year, computer retailer <a href=""/wiki/Paul_Terrell"" title=""Paul Terrell"">Paul Terrell</a> purchased 50 fully assembled units of the Apple I from them for $500 each.<sup class=""reference"" id=""cite_ref-FOOTNOTEIsaacson201566–68_73-0""><a href=""#cite_note-FOOTNOTEIsaacson201566–68-73"">[73]</a></sup><sup class=""reference"" id=""cite_ref-FOOTNOTELinzmayer7–9_74-0""><a href=""#cite_note-FOOTNOTELinzmayer7–9-74"">[74]</a></sup> Eventually about 200 Apple I computers were produced in total.<sup class=""reference"" id=""cite_ref-AppleStoryPart1_75-0""><a href=""#cite_note-AppleStoryPart1-75"">[75]</a></sup>
</p>, <p>A neighbor on Crist Drive recalled Jobs as an odd individual who would greet his clients ""with his underwear hanging out, barefoot and hippie-like"".<sup class=""reference"" id=""cite_ref-piece_32-1""><a href=""#cite_note-piece-32"">[32]</a></sup> Another neighbor, Larry Waterland, who had just earned his PhD in chemical engineering at Stanford, recalled dismissing Jobs's budding business: ""'You punched cards, put them in a big deck,' he said about the mainframe machines of that time. 'Steve took me over to the garage. He had a circuit board with a chip on it, a DuMont TV set, a Panasonic cassette tape deck and a keyboard. He said, 'This is an Apple computer.' I said, 'You've got to be joking.' I dismissed the whole idea.'""<sup class=""reference"" id=""cite_ref-piece_32-2""><a href=""#cite_note-piece-32"">[32]</a></sup> Jobs's friend from Reed College and India, <a href=""/wiki/Daniel_Kottke"" title=""Daniel Kottke"">Daniel Kottke</a>, recalled that as an early Apple employee, he ""was the only person who worked in the garage ... Woz would show up once a week with his latest code. Steve Jobs didn't get his hands dirty in that sense."" Kottke also stated that much of the early work took place in Jobs's kitchen, where he spent hours on the phone trying to find investors for the company.<sup class=""reference"" id=""cite_ref-piece_32-3""><a href=""#cite_note-piece-32"">[32]</a></sup>
</p>, <p>They received funding from a then-semi-retired <a href=""/wiki/Intel"" title=""Intel"">Intel</a> product marketing manager and engineer <a href=""/wiki/Mike_Markkula"" title=""Mike Markkula"">Mike Markkula</a>.<sup class=""reference"" id=""cite_ref-Markkula1997_76-0""><a href=""#cite_note-Markkula1997-76"">[76]</a></sup> <a href=""/wiki/Scott_McNealy"" title=""Scott McNealy"">Scott McNealy</a>, one of the cofounders of <a href=""/wiki/Sun_Microsystems"" title=""Sun Microsystems"">Sun Microsystems</a>, said that Jobs broke a ""<a href=""/wiki/Glass_ceiling"" title=""Glass ceiling"">glass age ceiling</a>"" in Silicon Valley because he'd created a very successful company at a young age.<sup class=""reference"" id=""cite_ref-Steve_Jobs:_Visionary_Entrepreneur_38-1""><a href=""#cite_note-Steve_Jobs:_Visionary_Entrepreneur-38"">[38]</a></sup> Markkula brought Apple to the attention of <a href=""/wiki/Arthur_Rock"" title=""Arthur Rock"">Arthur Rock</a>, which after looking at the crowded Apple booth at the Home Brew Computer Show, started with a $60,000 investment and went on the Apple board.<sup class=""reference"" id=""cite_ref-77""><a href=""#cite_note-77"">[77]</a></sup> Jobs was not pleased when Markkula recruited <a href=""/wiki/Michael_Scott_(Apple)"" title=""Michael Scott (Apple)"">Mike Scott</a> from <a href=""/wiki/National_Semiconductor"" title=""National Semiconductor"">National Semiconductor</a> in February 1977 to serve as the first president and CEO of Apple.<sup class=""reference"" id=""cite_ref-FOOTNOTEIsaacson201181–83_78-0""><a href=""#cite_note-FOOTNOTEIsaacson201181–83-78"">[78]</a></sup><sup class=""reference"" id=""cite_ref-FOOTNOTELinzmayer200411_79-0""><a href=""#cite_note-FOOTNOTELinzmayer200411-79"">[79]</a></sup>
</p>, <p>""For what characterizes Apple is that its scientific staff always acted and performed like artists – in a field filled with dry personalities limited by the rational and binary worlds they inhabit, Apple's engineering teams had passion. They always believed that what they were doing was important and, most of all, fun. Working at Apple was never just a job; it was also a crusade, a mission, to bring better computer power to people. At its roots that attitude came from Steve Jobs. It was ""<a href=""/wiki/Power_to_the_people_(slogan)"" title=""Power to the people (slogan)"">Power to the People</a>"", the slogan of the sixties, rewritten in technology for the eighties and called <a href=""/wiki/Macintosh#1984:_Debut"" title=""Macintosh"">Macintosh</a>.""
</p>, <p><cite class=""left-aligned"" style="""">—Jeffrey S. Young, 1987. From the book, <i>Steve Jobs: The Journey is the Reward</i> (published 1988).<sup class=""reference"" id=""cite_ref-journeyisreward_26-17""><a href=""#cite_note-journeyisreward-26"">[26]</a></sup><sup class=""noprint Inline-Template"" style=""white-space:nowrap;"">[<i><a href=""/wiki/Wikipedia:Citing_sources"" title=""Wikipedia:Citing sources""><span title="" (March 2018)"">page needed</span></a></i>]</sup></cite>
</p>, <p>After Brennan returned from her own journey to India, she and Jobs fell in love again, as Brennan noted changes in him that she attributes to <a href=""/wiki/K%C5%8Dbun_Chino_Otogawa"" title=""Kōbun Chino Otogawa"">Kobun</a> (whom she was also still following). It was also at this time that Jobs displayed a prototype Apple I computer for Brennan and his parents in their living room. Brennan notes a shift in this time period, where the two main influences on Jobs were Apple Inc. and <a class=""mw-redirect"" href=""/wiki/Kobun_Chino_Otogawa"" title=""Kobun Chino Otogawa"">Kobun</a>. By early 1977, she and Jobs would spend time together at her home at <a href=""/wiki/Hidden_Villa"" title=""Hidden Villa"">Duveneck Ranch in Los Altos</a>, which served as a hostel and environmental education center.
</p>, <p>In April 1977, Jobs and Wozniak introduced the <a href=""/wiki/Apple_II"" title=""Apple II"">Apple II</a> at the <a href=""/wiki/West_Coast_Computer_Faire"" title=""West Coast Computer Faire"">West Coast Computer Faire</a>.<sup class=""reference"" id=""cite_ref-FOOTNOTELinzmayer200412_80-0""><a href=""#cite_note-FOOTNOTELinzmayer200412-80"">[80]</a></sup> It is the first consumer product to have been sold by Apple Computer. Primarily designed by Wozniak, Jobs oversaw the development of its unusual case and <a href=""/wiki/Rod_Holt"" title=""Rod Holt"">Rod Holt</a> developed the unique power supply.<sup class=""reference"" id=""cite_ref-wozorg_81-0""><a href=""#cite_note-wozorg-81"">[81]</a></sup> During the design stage, Jobs argued that the Apple II should have two <a class=""mw-redirect"" href=""/wiki/Expansion_slot"" title=""Expansion slot"">expansion slots</a>, while Wozniak wanted eight. After a heated argument, Wozniak threatened that Jobs should ""go get himself another computer"". They later decided to go with eight slots.<sup class=""reference"" id=""cite_ref-iWoz_82-0""><a href=""#cite_note-iWoz-82"">[82]</a></sup> The Apple II became one of the first highly successful mass-produced microcomputer products in the world.<sup class=""reference"" id=""cite_ref-Ars_Technica_2005-12-15_83-0""><a href=""#cite_note-Ars_Technica_2005-12-15-83"">[83]</a></sup>
</p>, <p>As Jobs became more successful with his new company, his relationship with Brennan grew more complex. In 1977, the success of Apple was now a part of their relationship, and Brennan, <a href=""/wiki/Daniel_Kottke"" title=""Daniel Kottke"">Daniel Kottke</a>, and Jobs moved into a house near the Apple office in <a href=""/wiki/Cupertino,_California"" title=""Cupertino, California"">Cupertino</a>.<sup class=""noprint Inline-Template Template-Fact"" style=""white-space:nowrap;"">[<i><a href=""/wiki/Wikipedia:Citation_needed"" title=""Wikipedia:Citation needed""><span title=""This claim needs references to reliable sources. (September 2020)"">citation needed</span></a></i>]</sup> Brennan eventually took a position in the shipping department at Apple.<sup class=""reference"" id=""cite_ref-standingphoto_84-0""><a href=""#cite_note-standingphoto-84"">[84]</a></sup> Brennan's relationship with Jobs deteriorated as his position with Apple grew, and she began to consider ending the relationship. In October 1977, Brennan was approached by <a href=""/wiki/Rod_Holt"" title=""Rod Holt"">Rod Holt</a>, who asked her to take ""a paid apprenticeship designing blueprints for the Apples"".<sup class=""noprint Inline-Template Template-Fact"" style=""white-space:nowrap;"">[<i><a href=""/wiki/Wikipedia:Citation_needed"" title=""Wikipedia:Citation needed""><span title=""This claim needs references to reliable sources. (September 2020)"">citation needed</span></a></i>]</sup> Both Holt and Jobs believed that it would be a good position for her, given her artistic abilities. Holt was particularly eager that she take the position and puzzled by her ambivalence toward it. Brennan's decision, however, was overshadowed by the fact that she realized she was pregnant and that Jobs was the father. It took her a few days to tell Jobs, whose face, according to Brennan ""turned ugly"" at the news. At the same time, according to Brennan, at the beginning of her third trimester, Jobs said to her: ""I never wanted to ask that you get an abortion. I just didn't want to do that.""<sup class=""noprint Inline-Template Template-Fact"" style=""white-space:nowrap;"">[<i><a href=""/wiki/Wikipedia:Citation_needed"" title=""Wikipedia:Citation needed""><span title=""This claim needs references to reliable sources. (September 2020)"">citation needed</span></a></i>]</sup> He also refused to discuss the pregnancy with her.<sup class=""reference"" id=""cite_ref-FOOTNOTEIsaacson201588–89_85-0""><a href=""#cite_note-FOOTNOTEIsaacson201588–89-85"">[85]</a></sup> Brennan turned down the internship and decided to leave Apple. She stated that Jobs told her ""If you give up this baby for adoption, you will be sorry"" and ""I am never going to help you.""<sup class=""noprint Inline-Template Template-Fact"" style=""white-space:nowrap;"">[<i><a href=""/wiki/Wikipedia:Citation_needed"" title=""Wikipedia:Citation needed""><span title=""This claim needs references to reliable sources. (September 2020)"">citation needed</span></a></i>]</sup> According to Brennan, Jobs ""started to seed people with the notion that I slept around and he was infertile, which meant that this could not be his child."" A few weeks before she was due to give birth, Brennan was invited to deliver her baby at the All One Farm. She accepted the offer.<sup class=""noprint Inline-Template Template-Fact"" style=""white-space:nowrap;"">[<i><a href=""/wiki/Wikipedia:Citation_needed"" title=""Wikipedia:Citation needed""><span title=""This claim needs references to reliable sources. (September 2020)"">citation needed</span></a></i>]</sup> When Jobs was 23 (the same age as his biological parents when they had him)<sup class=""reference"" id=""cite_ref-FOOTNOTEIsaacson201588–89_85-1""><a href=""#cite_note-FOOTNOTEIsaacson201588–89-85"">[85]</a></sup> Brennan gave birth to her baby, <a href=""/wiki/Lisa_Brennan-Jobs"" title=""Lisa Brennan-Jobs"">Lisa Brennan</a>, on May 17, 1978.<sup class=""reference"" id=""cite_ref-pens_86-0""><a href=""#cite_note-pens-86"">[86]</a></sup> Jobs went there for the birth after he was contacted by <a href=""/wiki/Robert_Friedland"" title=""Robert Friedland"">Robert Friedland</a>, their mutual friend and the farm owner. While distant, Jobs worked with her on a name for the baby, which they discussed while sitting in the fields on a blanket. Brennan suggested the name ""Lisa"" which Jobs also liked and notes that Jobs was very attached to the name ""Lisa"" while he ""was also publicly denying paternity."" She would discover later that during this time, Jobs was preparing to unveil a new kind of computer that he wanted to give a female name (his first choice was ""Claire"" after <a href=""/wiki/Clare_of_Assisi"" title=""Clare of Assisi"">St. Clare</a>). She also stated that she never gave him permission to use the baby's name for a computer and he hid the plans from her. Jobs also worked with his team to come up with the phrase, ""Local Integrated Software Architecture"" as an <a href=""/wiki/Backronym"" title=""Backronym"">alternative explanation</a> for the <a href=""/wiki/Apple_Lisa"" title=""Apple Lisa"">Apple Lisa</a>.<sup class=""reference"" id=""cite_ref-Bullock_87-0""><a href=""#cite_note-Bullock-87"">[87]</a></sup> Decades later, however, Jobs admitted to his biographer <a href=""/wiki/Walter_Isaacson"" title=""Walter Isaacson"">Walter Isaacson</a> that ""obviously, it was named for my daughter"".<sup class=""reference"" id=""cite_ref-FOOTNOTEIsaacson201593_88-0""><a href=""#cite_note-FOOTNOTEIsaacson201593-88"">[88]</a></sup>
</p>, <p>When Jobs denied paternity, a <a href=""/wiki/DNA_paternity_testing"" title=""DNA paternity testing"">DNA test</a> established him as Lisa's father.<sup class=""noprint Inline-Template"" style=""margin-left:0.1em; white-space:nowrap;"">[<i><a href=""/wiki/Wikipedia:Please_clarify"" title=""Wikipedia:Please clarify""><span title=""The text near this tag may need clarification or removal of jargon. (August 2019)"">clarification needed</span></a></i>]</sup> It required him to give Brennan $385 a month in addition to returning the welfare money she had received. Jobs gave her $500 a month at the time when Apple went public and Jobs became a millionaire. Later, Brennan agreed to give an interview with <a href=""/wiki/Michael_Moritz"" title=""Michael Moritz"">Michael Moritz</a> for <i><a href=""/wiki/Time_(magazine)"" title=""Time (magazine)"">Time</a></i> magazine for its <a href=""/wiki/Time_Person_of_the_Year"" title=""Time Person of the Year"">Time Person of the Year</a> special, released on January 3, 1983, in which she discussed her relationship with Jobs. Rather than name Jobs the Person of the Year, the magazine named the computer<sup class=""noprint Inline-Template"" style=""margin-left:0.1em; white-space:nowrap;"">[<i><a href=""/wiki/Wikipedia:Please_clarify"" title=""Wikipedia:Please clarify""><span title=""The text near this tag may need clarification or removal of jargon. (August 2019)"">clarification needed</span></a></i>]</sup> the ""Machine of the Year"".<sup class=""reference"" id=""cite_ref-89""><a href=""#cite_note-89"">[89]</a></sup> In the issue, Jobs questioned the reliability of the paternity test (which stated that the ""probability of paternity for Jobs, Steven... is 94.1%"").<sup class=""reference"" id=""cite_ref-machineofthe_year1_90-0""><a href=""#cite_note-machineofthe_year1-90"">[90]</a></sup> Jobs responded by arguing that ""28% of the male population of the United States could be the father"".<sup class=""reference"" id=""cite_ref-machineofthe_year1_90-1""><a href=""#cite_note-machineofthe_year1-90"">[90]</a></sup> <i>Time</i> also noted that ""the baby girl and the machine on which Apple has placed so much hope for the future share the same name: Lisa"".<sup class=""reference"" id=""cite_ref-machineofthe_year1_90-2""><a href=""#cite_note-machineofthe_year1-90"">[90]</a></sup>
</p>, <p>Jobs was worth over $1 million in 1978, when he was just 23 years old. His net worth grew to over $250 million by the time he was 25, according to estimates.<sup class=""reference"" id=""cite_ref-91""><a href=""#cite_note-91"">[91]</a></sup> He was also one of the youngest ""people ever to make the <a class=""mw-redirect"" href=""/wiki/Forbes_(magazine)"" title=""Forbes (magazine)"">Forbes</a> list of the nation's richest people—and one of only a handful to have done it themselves, without inherited wealth"".<sup class=""reference"" id=""cite_ref-journeyisreward_26-18""><a href=""#cite_note-journeyisreward-26"">[26]</a></sup><sup class=""noprint Inline-Template"" style=""white-space:nowrap;"">[<i><a href=""/wiki/Wikipedia:Citing_sources"" title=""Wikipedia:Citing sources""><span title="" (March 2018)"">page needed</span></a></i>]</sup>
</p>, <p>In 1982, Jobs bought an apartment on the top two floors of <a href=""/wiki/The_San_Remo"" title=""The San Remo"">The San Remo</a>, a Manhattan building with a politically progressive reputation. Although he never lived there,<sup class=""reference"" id=""cite_ref-wirdem_92-0""><a href=""#cite_note-wirdem-92"">[92]</a></sup> he spent years renovating it with the help of <a href=""/wiki/I._M._Pei"" title=""I. M. Pei"">I. M. Pei</a>. In 2003, he sold it to <a href=""/wiki/U2"" title=""U2"">U2</a> singer <a class=""mw-redirect"" href=""/wiki/Bono_(U2)"" title=""Bono (U2)"">Bono</a>.
</p>, <p>In 1983, Jobs lured <a href=""/wiki/John_Sculley"" title=""John Sculley"">John Sculley</a> away from <a class=""mw-redirect"" href=""/wiki/Pepsi-Cola"" title=""Pepsi-Cola"">Pepsi-Cola</a> to serve as Apple's CEO, asking, ""Do you want to spend the rest of your life selling sugared water, or do you want a chance to change the world?""<sup class=""reference"" id=""cite_ref-FOOTNOTEIsaacson2015386–387_93-0""><a href=""#cite_note-FOOTNOTEIsaacson2015386–387-93"">[93]</a></sup>
</p>, <p>In 1984, Jobs bought the <a href=""/wiki/Jackling_House"" title=""Jackling House"">Jackling House</a> and estate, and resided there for a decade. After that, he leased it out for several years until 2000 when he stopped maintaining the house, allowing exposure to the weather to degrade it. In 2004, Jobs received permission from the town of Woodside to demolish the house in order to build a smaller contemporary styled one. After a few years in court, the house was finally demolished in 2011, a few months before he died.<sup class=""reference"" id=""cite_ref-sfch11_94-0""><a href=""#cite_note-sfch11-94"">[94]</a></sup>
</p>, <p>Jobs began directing the development of the <a href=""/wiki/Macintosh"" title=""Macintosh"">Macintosh</a> in 1981, when he took over the project from early Apple employee <a href=""/wiki/Jef_Raskin"" title=""Jef Raskin"">Jef Raskin</a>, who conceived the computer (Wozniak, who with Raskin had heavy influence over the program early on in its development, was on leave during this time due to an airplane crash earlier that year<sup class=""reference"" id=""cite_ref-TheVerge_95-0""><a href=""#cite_note-TheVerge-95"">[95]</a></sup>).<sup class=""reference"" id=""cite_ref-96""><a href=""#cite_note-96"">[96]</a></sup><sup class=""reference"" id=""cite_ref-FOOTNOTEIsaacson2015109–112_97-0""><a href=""#cite_note-FOOTNOTEIsaacson2015109–112-97"">[97]</a></sup> On January 22, 1984, Apple aired a <a href=""/wiki/Super_Bowl"" title=""Super Bowl"">Super Bowl</a> television commercial titled ""<a class=""mw-redirect"" href=""/wiki/1984_(television_commercial)"" title=""1984 (television commercial)"">1984</a>"", which ended with the words: ""On January 24th, Apple Computer will introduce Macintosh. And you'll see why 1984 won't be like <i><a href=""/wiki/Nineteen_Eighty-Four"" title=""Nineteen Eighty-Four"">1984</a></i>.""<sup class=""reference"" id=""cite_ref-FOOTNOTELinzmayer2004110–113_98-0""><a href=""#cite_note-FOOTNOTELinzmayer2004110–113-98"">[98]</a></sup> On January 24, 1984, an emotional Jobs introduced the Macintosh to a wildly enthusiastic audience at Apple's annual shareholders meeting held in the <a href=""/wiki/De_Anza_College#The_Flint_Center_for_the_Performing_Arts"" title=""De Anza College"">Flint Auditorium</a>;<sup class=""reference"" id=""cite_ref-FOOTNOTEIsaacson2015167–170_99-0""><a href=""#cite_note-FOOTNOTEIsaacson2015167–170-99"">[99]</a></sup><sup class=""reference"" id=""cite_ref-100""><a href=""#cite_note-100"">[100]</a></sup> Macintosh engineer <a href=""/wiki/Andy_Hertzfeld"" title=""Andy Hertzfeld"">Andy Hertzfeld</a> described the scene as ""pandemonium"".<sup class=""reference"" id=""cite_ref-The_Times_They_Are_A-Changin_101-0""><a href=""#cite_note-The_Times_They_Are_A-Changin-101"">[101]</a></sup> The Macintosh was based on <a href=""/wiki/Apple_Lisa"" title=""Apple Lisa"">The Lisa</a> (and <a class=""mw-redirect"" href=""/wiki/PARC_user_interface"" title=""PARC user interface"">Xerox PARC's</a> <a href=""/wiki/Computer_mouse"" title=""Computer mouse"">mouse</a>-driven <a href=""/wiki/Graphical_user_interface"" title=""Graphical user interface"">graphical user interface</a>),<sup class=""reference"" id=""cite_ref-Wired_102-0""><a href=""#cite_note-Wired-102"">[102]</a></sup><sup class=""reference"" id=""cite_ref-Jobsjourney_103-0""><a href=""#cite_note-Jobsjourney-103"">[103]</a></sup> and it was widely acclaimed by the media with strong initial sales supporting it.<sup class=""reference"" id=""cite_ref-FOOTNOTEIsaacson2015185–187_104-0""><a href=""#cite_note-FOOTNOTEIsaacson2015185–187-104"">[104]</a></sup><sup class=""reference"" id=""cite_ref-FOOTNOTESchlender201684–88_105-0""><a href=""#cite_note-FOOTNOTESchlender201684–88-105"">[105]</a></sup> However, the computer's slow processing speed and limited range of available software led to a rapid sales decline in the second half of 1984.<sup class=""reference"" id=""cite_ref-FOOTNOTEIsaacson2015185–187_104-1""><a href=""#cite_note-FOOTNOTEIsaacson2015185–187-104"">[104]</a></sup><sup class=""reference"" id=""cite_ref-FOOTNOTESchlender201684–88_105-1""><a href=""#cite_note-FOOTNOTESchlender201684–88-105"">[105]</a></sup><sup class=""reference"" id=""cite_ref-FOOTNOTELinzmayer200498_106-0""><a href=""#cite_note-FOOTNOTELinzmayer200498-106"">[106]</a></sup>
</p>, <p>Sculley's and Jobs's respective visions for the company greatly differed. The former favored <a href=""/wiki/Open_architecture"" title=""Open architecture"">open architecture</a> computers like the Apple II, sold to education, small business, and home markets less vulnerable to IBM. Jobs wanted the company to focus on the <a class=""mw-redirect"" href=""/wiki/Closed_architecture"" title=""Closed architecture"">closed architecture</a> Macintosh as a business alternative to the IBM PC. President and CEO Sculley had little control over chairman of the board Jobs's Macintosh division; it and the Apple II division operated like separate companies, duplicating services.<sup class=""reference"" id=""cite_ref-robbeloth198510_11_108-0""><a href=""#cite_note-robbeloth198510_11-108"">[108]</a></sup> Although its products provided 85 percent of Apple's sales in early 1985, the company's January 1985 <a class=""mw-redirect"" href=""/wiki/Annual_meeting"" title=""Annual meeting"">annual meeting</a> did not mention the Apple II division or employees. Many left, including Wozniak, who stated that the company had ""been going in the wrong direction for the last five years"" and sold most of his stock.<sup class=""reference"" id=""cite_ref-rice19850415_109-0""><a href=""#cite_note-rice19850415-109"">[109]</a></sup> Despite being frustrated with the company's (including Jobs himself) dismissal of the Apple II employees in favor of the Macintosh, Wozniak left amicably and remained an honorary employee of Apple, maintaining a friendship with Jobs until his death.<sup class=""reference"" id=""cite_ref-Steve_Thumbs_110-0""><a href=""#cite_note-Steve_Thumbs-110"">[110]</a></sup><sup class=""reference"" id=""cite_ref-wozemployee_111-0""><a href=""#cite_note-wozemployee-111"">[111]</a></sup><sup class=""reference"" id=""cite_ref-112""><a href=""#cite_note-112"">[112]</a></sup>
</p>, <p>By early 1985, the Macintosh's failure to defeat the IBM PC became clear,<sup class=""reference"" id=""cite_ref-FOOTNOTEIsaacson2015185–187_104-2""><a href=""#cite_note-FOOTNOTEIsaacson2015185–187-104"">[104]</a></sup><sup class=""reference"" id=""cite_ref-FOOTNOTESchlender201684–88_105-2""><a href=""#cite_note-FOOTNOTESchlender201684–88-105"">[105]</a></sup> and it strengthened Sculley's position in the company. In May 1985, Sculley—encouraged by Arthur Rock—decided to reorganize Apple, and proposed a plan to the board that would remove Jobs from the Macintosh group and put him in charge of ""New Product Development"". This move would effectively render Jobs powerless within Apple.<sup class=""reference"" id=""cite_ref-journeyisreward_26-19""><a href=""#cite_note-journeyisreward-26"">[26]</a></sup><sup class=""noprint Inline-Template"" style=""white-space:nowrap;"">[<i><a href=""/wiki/Wikipedia:Citing_sources"" title=""Wikipedia:Citing sources""><span title="" (March 2018)"">page needed</span></a></i>]</sup> In response, Jobs then developed a plan to get rid of Sculley and take over Apple. However, Jobs was confronted after the plan was leaked, and he said that he would leave Apple. The Board declined his resignation and asked him to reconsider. Sculley also told Jobs that he had all of the votes needed to go ahead with the reorganization. A few months later, on September 17, 1985, Jobs submitted a letter of resignation to the Apple Board. Five additional senior Apple employees also resigned and joined Jobs in his new venture, <a href=""/wiki/NeXT"" title=""NeXT"">NeXT</a>.<sup class=""reference"" id=""cite_ref-journeyisreward_26-20""><a href=""#cite_note-journeyisreward-26"">[26]</a></sup><sup class=""noprint Inline-Template"" style=""white-space:nowrap;"">[<i><a href=""/wiki/Wikipedia:Citing_sources"" title=""Wikipedia:Citing sources""><span title="" (March 2018)"">page needed</span></a></i>]</sup>
</p>, <p>The Macintosh's struggle continued after Jobs left Apple. Though marketed and received in fanfare, the expensive Macintosh was a hard sell.<sup class=""reference"" id=""cite_ref-swaine1_113-0""><a href=""#cite_note-swaine1-113"">[113]</a></sup><sup class=""reference nowrap""><span title=""Page / location: 308–309"">: 308–309 </span></sup> In 1985, <a href=""/wiki/Bill_Gates"" title=""Bill Gates"">Bill Gates</a>'s then-developing company, <a href=""/wiki/Microsoft"" title=""Microsoft"">Microsoft</a>, threatened to stop developing Mac applications unless it was granted ""a license for the Mac operating system software. Microsoft was developing its graphical user interface ... for DOS, which it was calling <a href=""/wiki/Microsoft_Windows"" title=""Microsoft Windows"">Windows</a> and didn't want Apple to sue over the similarities between the Windows GUI and the Mac interface.""<sup class=""reference"" id=""cite_ref-swaine1_113-1""><a href=""#cite_note-swaine1-113"">[113]</a></sup><sup class=""reference nowrap""><span title=""Page / location: 321"">: 321 </span></sup> Sculley granted Microsoft the license which later led to problems for Apple.<sup class=""reference"" id=""cite_ref-swaine1_113-2""><a href=""#cite_note-swaine1-113"">[113]</a></sup><sup class=""reference nowrap""><span title=""Page / location: 321"">: 321 </span></sup> In addition, cheap <a href=""/wiki/IBM_PC_compatible"" title=""IBM PC compatible"">IBM PC clones</a> that ran on Microsoft software and had a graphical user interface began to appear. Although the Macintosh preceded the clones, it was far more expensive, so ""through the late 1980s, the Windows user interface was getting better and better and was thus taking increasingly more share from Apple"".<sup class=""reference"" id=""cite_ref-swaine1_113-3""><a href=""#cite_note-swaine1-113"">[113]</a></sup><sup class=""reference nowrap""><span title=""Page / location: 322"">: 322 </span></sup> Windows-based IBM-PC clones also led to the development of additional GUIs such as IBM's TopView or Digital Research's GEM,<sup class=""reference"" id=""cite_ref-swaine1_113-4""><a href=""#cite_note-swaine1-113"">[113]</a></sup><sup class=""reference nowrap""><span title=""Page / location: 322"">: 322 </span></sup> and thus ""the graphical user interface was beginning to be taken for granted, undermining the most apparent advantage of the Mac...it seemed clear as the 1980s wound down that Apple couldn't go it alone indefinitely against the whole IBM-clone market.""<sup class=""reference"" id=""cite_ref-swaine1_113-5""><a href=""#cite_note-swaine1-113"">[113]</a></sup><sup class=""reference nowrap""><span title=""Page / location: 322"">: 322 </span></sup>
</p>, <p>Following his resignation from Apple in 1985, Jobs founded <a href=""/wiki/NeXT"" title=""NeXT"">NeXT Inc.</a><sup class=""reference"" id=""cite_ref-Apple_PC_Week_114-0""><a href=""#cite_note-Apple_PC_Week-114"">[114]</a></sup> with $7 million. A year later he was running out of money, and he sought venture capital with no product on the horizon. Eventually, Jobs attracted the attention of billionaire <a href=""/wiki/Ross_Perot"" title=""Ross Perot"">Ross Perot</a>, who invested heavily in the company.<sup class=""reference"" id=""cite_ref-FOOTNOTELinzmayer2004208_115-0""><a href=""#cite_note-FOOTNOTELinzmayer2004208-115"">[115]</a></sup> The NeXT computer was shown to the world in what was considered Jobs's comeback event,<sup class=""reference"" id=""cite_ref-116""><a href=""#cite_note-116"">[116]</a></sup> a lavish invitation-only gala <a href=""/wiki/NeXT_Introduction"" title=""NeXT Introduction"">launch event</a><sup class=""reference"" id=""cite_ref-117""><a href=""#cite_note-117"">[117]</a></sup> that was described as a multimedia extravaganza.<sup class=""reference"" id=""cite_ref-118""><a href=""#cite_note-118"">[118]</a></sup> The celebration was held at the <a href=""/wiki/Louise_M._Davies_Symphony_Hall"" title=""Louise M. Davies Symphony Hall"">Louise M. Davies Symphony Hall</a>, San Francisco, California, on Wednesday, October 12, 1988. <a href=""/wiki/Steve_Wozniak"" title=""Steve Wozniak"">Steve Wozniak</a> said in a 2013 interview that while Jobs was at NeXT he was ""really getting his head together"".<sup class=""reference"" id=""cite_ref-TheVerge_95-1""><a href=""#cite_note-TheVerge-95"">[95]</a></sup>
</p>, <p>NeXT workstations were first released in 1990 and priced at <span style=""white-space: nowrap"">US$9,999</span>. Like the <a href=""/wiki/Apple_Lisa"" title=""Apple Lisa"">Apple Lisa</a>, the NeXT workstation was technologically advanced and designed for the education sector, but was largely dismissed as cost-prohibitive for educational institutions.<sup class=""reference"" id=""cite_ref-wired_119-0""><a href=""#cite_note-wired-119"">[119]</a></sup> The NeXT workstation was known for its technical strengths, chief among them its <a class=""mw-redirect"" href=""/wiki/Object-oriented"" title=""Object-oriented"">object-oriented</a> software development system. Jobs marketed NeXT products to the financial, scientific, and academic community, highlighting its innovative, experimental new technologies, such as the <a class=""mw-redirect"" href=""/wiki/Mach_kernel"" title=""Mach kernel"">Mach kernel</a>, the <a href=""/wiki/Digital_signal_processor"" title=""Digital signal processor"">digital signal processor</a> chip, and the built-in <a href=""/wiki/Ethernet"" title=""Ethernet"">Ethernet</a> port. Making use of a NeXT computer, English computer scientist <a href=""/wiki/Tim_Berners-Lee"" title=""Tim Berners-Lee"">Tim Berners-Lee</a> invented the <a href=""/wiki/World_Wide_Web"" title=""World Wide Web"">World Wide Web</a> in 1990 at <a href=""/wiki/CERN"" title=""CERN"">CERN</a> in Switzerland.<sup class=""reference"" id=""cite_ref-120""><a href=""#cite_note-120"">[120]</a></sup>
</p>, <p>The revised, second generation <a href=""/wiki/NeXTcube"" title=""NeXTcube"">NeXTcube</a> was released in 1990. Jobs touted it as the first ""interpersonal"" computer that would replace the personal computer. With its innovative <a class=""mw-redirect"" href=""/wiki/NeXTMail"" title=""NeXTMail"">NeXTMail</a> multimedia email system, NeXTcube could share voice, image, graphics, and video in email for the first time. ""Interpersonal computing is going to revolutionize human communications and groupwork"", Jobs told reporters.<sup class=""reference"" id=""cite_ref-google_121-0""><a href=""#cite_note-google-121"">[121]</a></sup> Jobs ran NeXT with an obsession for aesthetic perfection, as evidenced by the development of and attention to NeXTcube's magnesium case.<sup class=""reference"" id=""cite_ref-atheneum_122-0""><a href=""#cite_note-atheneum-122"">[122]</a></sup> This put considerable strain on NeXT's hardware division, and in 1993, after having sold only 50,000 machines, NeXT transitioned fully to software development with the release of <a href=""/wiki/NeXTSTEP"" title=""NeXTSTEP"">NeXTSTEP</a>/<a href=""/wiki/Intel"" title=""Intel"">Intel</a>.<sup class=""reference"" id=""cite_ref-OGrady_123-0""><a href=""#cite_note-OGrady-123"">[123]</a></sup> The company reported its first yearly profit of $1.03 million in 1994.<sup class=""reference"" id=""cite_ref-FOOTNOTELinzmayer2004213_124-0""><a href=""#cite_note-FOOTNOTELinzmayer2004213-124"">[124]</a></sup> In 1996, NeXT Software, Inc. released <a href=""/wiki/WebObjects"" title=""WebObjects"">WebObjects</a>, a framework for Web application development. After NeXT was acquired by Apple Inc. in 1997, WebObjects was used to build and run the <a href=""/wiki/Apple_Store"" title=""Apple Store"">Apple Store</a>,<sup class=""reference"" id=""cite_ref-OGrady_123-1""><a href=""#cite_note-OGrady-123"">[123]</a></sup> <a href=""/wiki/MobileMe"" title=""MobileMe"">MobileMe</a> services, and the <a href=""/wiki/ITunes_Store"" title=""ITunes Store"">iTunes Store</a>.
</p>, <p>In 1986, Jobs funded the spinout of The Graphics Group (later renamed <a href=""/wiki/Pixar"" title=""Pixar"">Pixar</a>) from <a href=""/wiki/Lucasfilm"" title=""Lucasfilm"">Lucasfilm</a>'s computer graphics division for the price of $10 million, $5 million of which was given to the company as capital and $5 million of which was paid to Lucasfilm for technology rights.<sup class=""reference"" id=""cite_ref-Pixar_Founding_Documents_125-0""><a href=""#cite_note-Pixar_Founding_Documents-125"">[125]</a></sup>
</p>, <p>The first film produced by Pixar with its <a href=""/wiki/The_Walt_Disney_Company"" title=""The Walt Disney Company"">Disney</a> partnership, <i><a href=""/wiki/Toy_Story"" title=""Toy Story"">Toy Story</a></i> (1995), with Jobs credited as executive producer,<sup class=""noprint Inline-Template Template-Fact"" style=""white-space:nowrap;"">[<i><a href=""/wiki/Wikipedia:Citation_needed"" title=""Wikipedia:Citation needed""><span title=""This claim needs references to reliable sources. (July 2021)"">citation needed</span></a></i>]</sup> brought financial success and critical acclaim to the studio when it was released. Over the course of Jobs's life, under Pixar's creative chief <a href=""/wiki/John_Lasseter"" title=""John Lasseter"">John Lasseter</a>, the company produced box-office hits <i><a href=""/wiki/A_Bug%27s_Life"" title=""A Bug's Life"">A Bug's Life</a></i> (1998); <i><a href=""/wiki/Toy_Story_2"" title=""Toy Story 2"">Toy Story 2</a></i> (1999); <i><a href=""/wiki/Monsters,_Inc."" title=""Monsters, Inc."">Monsters, Inc.</a></i> (2001); <i><a href=""/wiki/Finding_Nemo"" title=""Finding Nemo"">Finding Nemo</a></i> (2003); <i><a href=""/wiki/The_Incredibles"" title=""The Incredibles"">The Incredibles</a></i> (2004); <i><a href=""/wiki/Cars_(film)"" title=""Cars (film)"">Cars</a></i> (2006); <i><a href=""/wiki/Ratatouille_(film)"" title=""Ratatouille (film)"">Ratatouille</a></i> (2007); <i><a href=""/wiki/WALL-E"" title=""WALL-E"">WALL-E</a></i> (2008); <i><a href=""/wiki/Up_(2009_film)"" title=""Up (2009 film)"">Up</a></i> (2009); <i><a href=""/wiki/Toy_Story_3"" title=""Toy Story 3"">Toy Story 3</a></i> (2010); and <i><a href=""/wiki/Cars_2"" title=""Cars 2"">Cars 2</a></i> (2011). <i><a href=""/wiki/Brave_(2012_film)"" title=""Brave (2012 film)"">Brave</a></i> (2012), Pixar's first film to be produced since Jobs's death, honored him with a tribute for his contributions to the studio.<sup class=""reference"" id=""cite_ref-126""><a href=""#cite_note-126"">[126]</a></sup> <i>Finding Nemo</i>, <i>The Incredibles</i>, <i>Ratatouille</i>, <i>WALL-E</i>, <i>Up</i>, <i>Toy Story 3</i> and <i>Brave</i> each received the <a href=""/wiki/Academy_Award_for_Best_Animated_Feature"" title=""Academy Award for Best Animated Feature"">Academy Award for Best Animated Feature</a>, an award introduced in 2001.<sup class=""reference"" id=""cite_ref-127""><a href=""#cite_note-127"">[127]</a></sup><sup class=""reference"" id=""cite_ref-128""><a href=""#cite_note-128"">[128]</a></sup>
</p>, <p>In 2003 and 2004, as Pixar's contract with Disney was running out, Jobs and Disney chief executive <a href=""/wiki/Michael_Eisner"" title=""Michael Eisner"">Michael Eisner</a> tried but failed to negotiate a new partnership,<sup class=""reference"" id=""cite_ref-vanityfair_129-0""><a href=""#cite_note-vanityfair-129"">[129]</a></sup> and in January 2004, Jobs announced that he would never deal with Disney again.<sup class=""reference"" id=""cite_ref-iger20190918_130-0""><a href=""#cite_note-iger20190918-130"">[130]</a></sup> Pixar would seek a new partner to distribute its films after its contract expired.
</p>, <p>In October 2005, <a href=""/wiki/Bob_Iger"" title=""Bob Iger"">Bob Iger</a> replaced Eisner at Disney, and Iger quickly worked to mend relations with Jobs and Pixar. On January 24, 2006, Jobs and Iger announced that Disney had agreed to purchase Pixar in an all-stock transaction worth $7.4 billion. When the deal closed, Jobs became <a href=""/wiki/The_Walt_Disney_Company"" title=""The Walt Disney Company"">The Walt Disney Company</a>'s largest single shareholder with approximately seven percent of the company's stock.<sup class=""reference"" id=""cite_ref-DisneyBuysPixar_131-0""><a href=""#cite_note-DisneyBuysPixar-131"">[131]</a></sup> Jobs's holdings in Disney far exceeded those of Eisner, who holds 1.7%, and of Disney family member <a href=""/wiki/Roy_E._Disney"" title=""Roy E. Disney"">Roy E. Disney</a>, who until his 2009 death held about 1% of the company's stock and whose criticisms of Eisner—especially that he soured Disney's relationship with Pixar—accelerated Eisner's ousting. Upon completion of the merger, Jobs received 7% of Disney shares, and joined the board of directors as the largest individual shareholder.<sup class=""reference"" id=""cite_ref-DisneyBuysPixar_131-1""><a href=""#cite_note-DisneyBuysPixar-131"">[131]</a></sup><sup class=""reference"" id=""cite_ref-Disney_132-0""><a href=""#cite_note-Disney-132"">[132]</a></sup><sup class=""reference"" id=""cite_ref-133""><a href=""#cite_note-133"">[133]</a></sup> Upon Jobs's death his shares in Disney were transferred to the Steven P. Jobs Trust led by <a class=""mw-redirect"" href=""/wiki/Laurene_Jobs"" title=""Laurene Jobs"">Laurene Jobs</a>.<sup class=""reference"" id=""cite_ref-134""><a href=""#cite_note-134"">[134]</a></sup>
</p>, <p>After Jobs's death Iger recalled in 2019 that many warned him about Jobs, ""that he would bully me and everyone else"". Iger wrote, ""Who wouldn't want Steve Jobs to have influence over how a company is run?"", and that as an active Disney board member ""he rarely created trouble for me. Not never but rarely"". He speculated that they would have seriously considered merging Disney and Apple had Jobs lived.<sup class=""reference"" id=""cite_ref-iger20190918_130-1""><a href=""#cite_note-iger20190918-130"">[130]</a></sup> <a href=""/wiki/Floyd_Norman"" title=""Floyd Norman"">Floyd Norman</a>, of Pixar, described Jobs as a ""mature, mellow individual"" who never interfered with the creative process of the filmmakers.<sup class=""reference"" id=""cite_ref-Steve_Jobs:_A_Tough_Act_to_Follow_135-0""><a href=""#cite_note-Steve_Jobs:_A_Tough_Act_to_Follow-135"">[135]</a></sup> In early June 2014, Pixar cofounder and <a href=""/wiki/Walt_Disney_Animation_Studios"" title=""Walt Disney Animation Studios"">Walt Disney Animation Studios</a> President Ed Catmull revealed that Jobs once advised him to ""just explain it to them until they understand"" in disagreements. Catmull released the book <i><a href=""/wiki/Creativity,_Inc."" title=""Creativity, Inc."">Creativity, Inc.</a></i> in 2014, in which he recounts numerous experiences of working with Jobs. Regarding his own manner of dealing with Jobs, Catmull writes:<sup class=""reference"" id=""cite_ref-Creativity_136-0""><a href=""#cite_note-Creativity-136"">[136]</a></sup><sup class=""noprint Inline-Template"" style=""white-space:nowrap;"">[<i><a href=""/wiki/Wikipedia:Citing_sources"" title=""Wikipedia:Citing sources""><span title="" (March 2018)"">page needed</span></a></i>]</sup>
</p>, <p>In all the 26 years with Steve, Steve and I never had one of these loud verbal arguments and it's not my nature to do that. ... but we did disagree fairly frequently about things. ... I would say something to him and he would immediately shoot it down because he could think faster than I could. ... I would then wait a week ... I'd call him up and I give my counter argument to what he had said and he'd immediately shoot it down. So I had to wait another week, and sometimes this went on for months. But in the end one of three things happened. About a third of the time he said, 'Oh, I get it, you're right.' And that was the end of it. And it was another third of the time in which [I'd] say, 'Actually I think he is right.' The other third of the time, where we didn't reach consensus, he just let me do it my way, never said anything more about it.<sup class=""reference"" id=""cite_ref-137""><a href=""#cite_note-137"">[137]</a></sup></p>, <p>In 1996, Apple announced that it would buy <a href=""/wiki/NeXT"" title=""NeXT"">NeXT</a> for $427 million. The deal was finalized in February 1997,<sup class=""reference"" id=""cite_ref-archive_138-0""><a href=""#cite_note-archive-138"">[138]</a></sup> bringing Jobs back to the company he had cofounded. Jobs became <i>de facto</i> chief after then-CEO <a href=""/wiki/Gil_Amelio"" title=""Gil Amelio"">Gil Amelio</a> was ousted in July 1997. He was formally named interim chief executive on September 16.<sup class=""reference"" id=""cite_ref-Apple_Formally_Names_Jobs_as_Interim_Chief_139-0""><a href=""#cite_note-Apple_Formally_Names_Jobs_as_Interim_Chief-139"">[139]</a></sup> In March 1998, to concentrate Apple's efforts on returning to profitability, Jobs terminated a number of projects, such as <a class=""mw-redirect"" href=""/wiki/Newton_(platform)"" title=""Newton (platform)"">Newton</a>, <a href=""/wiki/Cyberdog"" title=""Cyberdog"">Cyberdog</a>, and <a href=""/wiki/OpenDoc"" title=""OpenDoc"">OpenDoc</a>. In the coming months, many employees developed a fear of encountering Jobs while riding in the elevator, ""afraid that they might not have a job when the doors opened. The reality was that Jobs's summary executions were rare, but a handful of victims was enough to terrorize a whole company.""<sup class=""reference"" id=""cite_ref-The_once_and_future_Steve_Jobs_140-0""><a href=""#cite_note-The_once_and_future_Steve_Jobs-140"">[140]</a></sup> Jobs changed the licensing program for <a class=""mw-redirect"" href=""/wiki/Macintosh_clones"" title=""Macintosh clones"">Macintosh clones</a>, making it too costly for the manufacturers to continue making machines.
</p>, <p>With the purchase of NeXT, much of the company's technology found its way into Apple products, most notably <a href=""/wiki/NeXTSTEP"" title=""NeXTSTEP"">NeXTSTEP</a>, which evolved into <a class=""mw-redirect"" href=""/wiki/Mac_OS_X"" title=""Mac OS X"">Mac OS X</a>. Under Jobs's guidance, the company increased sales significantly with the introduction of the <a href=""/wiki/IMac"" title=""IMac"">iMac</a> and other new products; since then, appealing designs and powerful branding have worked well for Apple. At the 2000 Macworld Expo, Jobs officially dropped the ""interim"" modifier from his title at Apple and became permanent CEO.<sup class=""reference"" id=""cite_ref-norr_141-0""><a href=""#cite_note-norr-141"">[141]</a></sup> Jobs quipped at the time that he would be using the title ""iCEO"".<sup class=""reference"" id=""cite_ref-Jobs_announces_new_MacOS,_becomes_142-0""><a href=""#cite_note-Jobs_announces_new_MacOS,_becomes-142"">[142]</a></sup>
</p>, <p>The company subsequently branched out, introducing and improving upon other digital appliances. With the introduction of the <a href=""/wiki/IPod"" title=""IPod"">iPod</a> portable music player, iTunes digital music software, and the <a href=""/wiki/ITunes_Store"" title=""ITunes Store"">iTunes Store</a>, the company made forays into consumer electronics and music distribution. On June 29, 2007, Apple entered the cellular phone business with the introduction of the <a href=""/wiki/IPhone"" title=""IPhone"">iPhone</a>, a <a href=""/wiki/Multi-touch"" title=""Multi-touch"">multi-touch</a> display cell phone, which also included the features of an iPod and, with its own mobile browser, revolutionized the mobile browsing scene. While nurturing open-ended innovation, Jobs also reminded his employees that ""real artists ship"".<sup class=""reference"" id=""cite_ref-Insanely_Great_143-0""><a href=""#cite_note-Insanely_Great-143"">[143]</a></sup>
</p>, <p>Jobs had a public war of words with <a class=""mw-redirect"" href=""/wiki/Dell,_Inc."" title=""Dell, Inc."">Dell Computer</a> CEO <a href=""/wiki/Michael_Dell"" title=""Michael Dell"">Michael Dell</a>, starting in 1987, when Jobs first criticized Dell for making ""un-innovative beige boxes"".<sup class=""reference"" id=""cite_ref-cnet_144-0""><a href=""#cite_note-cnet-144"">[144]</a></sup> On October 6, 1997, at a <a href=""/wiki/Gartner"" title=""Gartner"">Gartner</a> Symposium, when Dell was asked what he would do if he ran the then-troubled Apple Computer company, he said: ""I'd shut it down and give the money back to the shareholders.""<sup class=""reference"" id=""cite_ref-Dell:_Apple_should_close_shop_145-0""><a href=""#cite_note-Dell:_Apple_should_close_shop-145"">[145]</a></sup> Then, in 2006, Jobs sent an email to all employees when Apple's <a href=""/wiki/Market_capitalization"" title=""Market capitalization"">market capitalization</a> rose above Dell's. It read:
</p>, <p>Team, it turned out that Michael Dell wasn't perfect at predicting the future. Based on today's stock market close, Apple is worth more than Dell. Stocks go up and down, and things may be different tomorrow, but I thought it was worth a moment of reflection today. Steve.<sup class=""reference"" id=""cite_ref-Michael_Dell_Should_Eat_His_Words,_Apple_Chief_Suggests_146-0""><a href=""#cite_note-Michael_Dell_Should_Eat_His_Words,_Apple_Chief_Suggests-146"">[146]</a></sup></p>, <p>Jobs was both admired and criticized for his consummate skill at persuasion and salesmanship, which has been dubbed the ""<a href=""/wiki/Reality_distortion_field"" title=""Reality distortion field"">reality distortion field</a>"" and was particularly evident during his keynote speeches (colloquially known as ""<a href=""/wiki/Stevenote"" title=""Stevenote"">Stevenotes</a>"") at <a class=""mw-redirect"" href=""/wiki/Macworld_Conference_%26_Expo"" title=""Macworld Conference &amp; Expo"">Macworld Expos</a> and at <a href=""/wiki/Apple_Worldwide_Developers_Conference"" title=""Apple Worldwide Developers Conference"">Apple Worldwide Developers Conferences</a>.<sup class=""reference"" id=""cite_ref-147""><a href=""#cite_note-147"">[147]</a></sup>
</p>, <p>Jobs usually went to work wearing a black long-sleeved <a class=""mw-redirect"" href=""/wiki/Mock_turtleneck"" title=""Mock turtleneck"">mock turtleneck</a> made by <a href=""/wiki/Issey_Miyake"" title=""Issey Miyake"">Issey Miyake</a>, <a class=""mw-redirect"" href=""/wiki/Levi%27s"" title=""Levi's"">Levi's</a> 501 blue jeans, and <a href=""/wiki/New_Balance"" title=""New Balance"">New Balance</a> 991 sneakers.<sup class=""reference"" id=""cite_ref-latimes_turtleneck_148-0""><a href=""#cite_note-latimes_turtleneck-148"">[148]</a></sup><sup class=""reference"" id=""cite_ref-gizmodo_149-0""><a href=""#cite_note-gizmodo-149"">[149]</a></sup> Jobs told his biographer Walter Isaacson ""...he came to like the idea of having a uniform for himself, both because of its daily convenience (the rationale he claimed) and its ability to convey a signature style.""<sup class=""reference"" id=""cite_ref-latimes_turtleneck_148-1""><a href=""#cite_note-latimes_turtleneck-148"">[148]</a></sup>
</p>, <p>Jobs was a board member at <a href=""/wiki/Gap_Inc."" title=""Gap Inc."">Gap Inc.</a> from 1999 to 2002.<sup class=""reference"" id=""cite_ref-150""><a href=""#cite_note-150"">[150]</a></sup>
</p>, <p>In 2001, Jobs was granted stock options in the amount of 7.5 million shares of Apple with an exercise price of $18.30. It was alleged that the options had been <a class=""mw-redirect"" href=""/wiki/Backdating"" title=""Backdating"">backdated</a>, and that the exercise price should have been $21.10. It was further alleged that Jobs had thereby incurred taxable income of $20,000,000 that he did not report, and that Apple overstated its earnings by that same amount. As a result, Jobs potentially faced a number of criminal charges and civil penalties. The case was the subject of active criminal and civil government investigations,<sup class=""reference"" id=""cite_ref-New_questions_raised_about_Steve_Jobs_151-0""><a href=""#cite_note-New_questions_raised_about_Steve_Jobs-151"">[151]</a></sup> though an independent internal Apple investigation completed on December 29, 2006 found that Jobs was unaware of these issues and that the options granted to him were returned without being exercised in 2003.<sup class=""reference"" id=""cite_ref-Apple_restates,_acknowledges_faked_documents_152-0""><a href=""#cite_note-Apple_restates,_acknowledges_faked_documents-152"">[152]</a></sup>
</p>, <p>In 2005, Jobs responded to criticism of Apple's poor recycling programs for <a class=""mw-redirect"" href=""/wiki/E-waste"" title=""E-waste"">e-waste</a> in the US by lashing out at environmental and other advocates at Apple's annual meeting in Cupertino in April. A few weeks later, Apple announced it would take back iPods for free at its retail stores. The <a href=""/wiki/Computer_recycling#Takeback"" title=""Computer recycling"">Computer TakeBack Campaign</a> responded by flying a banner from a plane over the Stanford University graduation at which Jobs was the commencement speaker. The banner read ""Steve, don't be a mini-player—recycle all e-waste.""
</p>, <p>In 2006, he further expanded Apple's recycling programs to any US customer who buys a new Mac. This program includes shipping and ""environmentally friendly disposal"" of their old systems.<sup class=""reference"" id=""cite_ref-Apple_Improves_Recycling_Plan_153-0""><a href=""#cite_note-Apple_Improves_Recycling_Plan-153"">[153]</a></sup> The success of Apple's unique products and services provided several years of stable financial returns, propelling Apple to become the world's most valuable publicly traded company in 2011.<sup class=""reference"" id=""cite_ref-154""><a href=""#cite_note-154"">[154]</a></sup>
</p>, <p>Jobs was perceived as a demanding perfectionist<sup class=""reference"" id=""cite_ref-abc_155-0""><a href=""#cite_note-abc-155"">[155]</a></sup><sup class=""reference"" id=""cite_ref-abc2_156-0""><a href=""#cite_note-abc2-156"">[156]</a></sup> who always aspired to position his businesses and their products at the forefront of the information technology industry by foreseeing and setting innovation and style trends. He summed up this self-concept at the end of his keynote speech at the <a class=""mw-redirect"" href=""/wiki/Macworld_Conference_%26_Expo#2007"" title=""Macworld Conference &amp; Expo"">Macworld Conference and Expo</a> in January 2007, by quoting ice hockey player <a href=""/wiki/Wayne_Gretzky"" title=""Wayne Gretzky"">Wayne Gretzky</a>:
</p>, <p>There's an old Wayne Gretzky quote that I love. ""I skate to where the puck is going to be, not where it has been."" And we've always tried to do that at Apple. Since the very, very beginning. And we always will.<sup class=""reference"" id=""cite_ref-JOBS_MACWORLD_07_157-0""><a href=""#cite_note-JOBS_MACWORLD_07-157"">[157]</a></sup></p>, <p>On July 1, 2008, a <span style=""white-space: nowrap"">US$7</span> billion class action suit was filed against several members of the Apple board of directors for revenue lost because of alleged securities fraud.<sup class=""reference"" id=""cite_ref-dailytech_158-0""><a href=""#cite_note-dailytech-158"">[158]</a></sup><sup class=""reference"" id=""cite_ref-Apple,_Steve_Jobs,_Executives,_Board,_Sued_For_Securities_Fraud_159-0""><a href=""#cite_note-Apple,_Steve_Jobs,_Executives,_Board,_Sued_For_Securities_Fraud-159"">[159]</a></sup>
</p>, <p>In a 2011 interview with biographer Walter Isaacson, Jobs revealed that he had met with US President <a href=""/wiki/Barack_Obama"" title=""Barack Obama"">Barack Obama</a>, complained about the nation's shortage of software engineers, and told Obama that he was ""headed for a one-term presidency"".<sup class=""reference"" id=""cite_ref-obama_160-0""><a href=""#cite_note-obama-160"">[160]</a></sup> Jobs proposed that any foreign student who got an engineering degree at a US university should automatically be offered a green card. After the meeting, Jobs commented, ""The president is very smart, but he kept explaining to us reasons why things can't get done . . . . It infuriates me.""<sup class=""reference"" id=""cite_ref-obama_160-1""><a href=""#cite_note-obama-160"">[160]</a></sup>
</p>, <p>In October 2003, Jobs was diagnosed with cancer. In mid 2004, he announced to his employees that he had a cancerous tumor in his <a href=""/wiki/Pancreas"" title=""Pancreas"">pancreas</a>.<sup class=""reference"" id=""cite_ref-www-sfgate-MNGMJ816F41_161-0""><a href=""#cite_note-www-sfgate-MNGMJ816F41-161"">[161]</a></sup> The prognosis for <a href=""/wiki/Pancreatic_cancer"" title=""Pancreatic cancer"">pancreatic cancer</a> is usually very poor;<sup class=""reference"" id=""cite_ref-celebritydiagnosis_162-0""><a href=""#cite_note-celebritydiagnosis-162"">[162]</a></sup> Jobs stated that he had a rare, much less aggressive type, known as <a href=""/wiki/Pancreatic_neuroendocrine_tumor"" title=""Pancreatic neuroendocrine tumor"">islet cell neuroendocrine tumor</a>.<sup class=""reference"" id=""cite_ref-www-sfgate-MNGMJ816F41_161-1""><a href=""#cite_note-www-sfgate-MNGMJ816F41-161"">[161]</a></sup>
</p>, <p>Despite his diagnosis, Jobs resisted his doctors' recommendations for medical intervention for nine months,<sup class=""reference"" id=""cite_ref-sjfortune_163-0""><a href=""#cite_note-sjfortune-163"">[163]</a></sup> instead relying on <a href=""/wiki/Alternative_medicine"" title=""Alternative medicine"">alternative medicine</a> to thwart the disease. According to Harvard researcher Ramzi Amri, his choice of alternative treatment ""led to an unnecessarily early death"". Other doctors agree that Jobs's diet was insufficient to address his disease. However, cancer researcher and alternative medicine critic <a href=""/wiki/David_Gorski"" title=""David Gorski"">David Gorski</a> wrote that ""it's impossible to know whether and by how much he might have decreased his chances of surviving his cancer through his flirtation with <a href=""/wiki/Homeopathy"" title=""Homeopathy"">woo</a>. My best guess was that Jobs probably only modestly decreased his chances of survival, if that.""<sup class=""reference"" id=""cite_ref-Fiore_164-0""><a href=""#cite_note-Fiore-164"">[164]</a></sup><sup class=""reference"" id=""cite_ref-Gorski_165-0""><a href=""#cite_note-Gorski-165"">[165]</a></sup> <a href=""/wiki/Barrie_R._Cassileth"" title=""Barrie R. Cassileth"">Barrie R. Cassileth</a>, the chief of <a href=""/wiki/Memorial_Sloan_Kettering_Cancer_Center"" title=""Memorial Sloan Kettering Cancer Center"">Memorial Sloan Kettering Cancer Center</a>'s <a class=""mw-redirect"" href=""/wiki/Integrative_medicine"" title=""Integrative medicine"">integrative medicine</a> department,<sup class=""reference"" id=""cite_ref-bio_166-0""><a href=""#cite_note-bio-166"">[166]</a></sup> on the other hand, said, ""Jobs's faith in alternative medicine likely cost him his life ... He had the only kind of pancreatic cancer that is treatable and curable ... He essentially committed suicide.""<sup class=""reference"" id=""cite_ref-Szabo_167-0""><a href=""#cite_note-Szabo-167"">[167]</a></sup> According to Jobs's biographer, Walter Isaacson, ""for nine months he refused to undergo surgery for his pancreatic cancer – a decision he later regretted as his health declined"".<sup class=""reference"" id=""cite_ref-Potter_168-0""><a href=""#cite_note-Potter-168"">[168]</a></sup> ""Instead, he tried a vegan diet, acupuncture, herbal remedies, and other treatments he found online, and even consulted a psychic. He was also influenced by a doctor who ran a clinic that advised juice fasts, bowel cleansings and other unproven approaches, before finally having surgery in July 2004.""<sup class=""reference"" id=""cite_ref-AP-Fox_169-0""><a href=""#cite_note-AP-Fox-169"">[169]</a></sup> He underwent a <a href=""/wiki/Pancreaticoduodenectomy"" title=""Pancreaticoduodenectomy"">pancreaticoduodenectomy</a> (or ""Whipple procedure"") that appeared to remove the tumor successfully.<sup class=""reference"" id=""cite_ref-Pancreatic_Cancer_Treatment_170-0""><a href=""#cite_note-Pancreatic_Cancer_Treatment-170"">[170]</a></sup><sup class=""reference"" id=""cite_ref-SharePrice_171-0""><a href=""#cite_note-SharePrice-171"">[171]</a></sup> Jobs did not receive <a href=""/wiki/Chemotherapy"" title=""Chemotherapy"">chemotherapy</a> or <a href=""/wiki/Radiation_therapy"" title=""Radiation therapy"">radiation therapy</a>.<sup class=""reference"" id=""cite_ref-www-sfgate-MNGMJ816F41_161-2""><a href=""#cite_note-www-sfgate-MNGMJ816F41-161"">[161]</a></sup><sup class=""reference"" id=""cite_ref-Elmer_172-0""><a href=""#cite_note-Elmer-172"">[172]</a></sup> During Jobs's absence, <a href=""/wiki/Tim_Cook"" title=""Tim Cook"">Tim Cook</a>, head of worldwide sales and operations at Apple, ran the company.<sup class=""reference"" id=""cite_ref-www-sfgate-MNGMJ816F41_161-3""><a href=""#cite_note-www-sfgate-MNGMJ816F41-161"">[161]</a></sup>
</p>, <p>As of January 2006<sup class=""plainlinks noexcerpt noprint asof-tag update"" style=""display:none;""><a class=""external text"" href=""https://en.wikipedia.org/w/index.php?title=Steve_Jobs&amp;action=edit"">[update]</a></sup>, only Jobs's wife, his doctors, and Iger and his wife knew that his cancer had returned. Jobs told Iger privately that he hoped to live to see his son Reed's high school graduation in 2010.<sup class=""reference"" id=""cite_ref-iger20190918_130-2""><a href=""#cite_note-iger20190918-130"">[130]</a></sup> In early August 2006, Jobs delivered the keynote for Apple's annual <a class=""mw-redirect"" href=""/wiki/Worldwide_Developers_Conference"" title=""Worldwide Developers Conference"">Worldwide Developers Conference</a>. His ""thin, almost gaunt"" appearance and unusually ""listless"" delivery,<sup class=""reference"" id=""cite_ref-Has_Steve_Jobs_Lost_His_Magic?_173-0""><a href=""#cite_note-Has_Steve_Jobs_Lost_His_Magic?-173"">[173]</a></sup><sup class=""reference"" id=""cite_ref-Jobs_speech_wasn_174-0""><a href=""#cite_note-Jobs_speech_wasn-174"">[174]</a></sup> together with his choice to delegate significant portions of his keynote to other presenters, inspired a flurry of media and internet speculation about the state of his health.<sup class=""reference"" id=""cite_ref-JobsMojo_175-0""><a href=""#cite_note-JobsMojo-175"">[175]</a></sup> In contrast, according to an <i><a href=""/wiki/Ars_Technica"" title=""Ars Technica"">Ars Technica</a></i> journal report, <a class=""mw-redirect"" href=""/wiki/Worldwide_Developers_Conference"" title=""Worldwide Developers Conference"">Worldwide Developers Conference</a> (WWDC) attendees who saw Jobs in person said he ""looked fine"".<sup class=""reference"" id=""cite_ref-knowandlove_176-0""><a href=""#cite_note-knowandlove-176"">[176]</a></sup> Following the keynote, an Apple spokesperson said that ""Steve's health is robust.""<sup class=""reference"" id=""cite_ref-claburn_177-0""><a href=""#cite_note-claburn-177"">[177]</a></sup>
</p>, <p>Two years later, similar concerns followed Jobs's 2008 WWDC keynote address.<sup class=""reference"" id=""cite_ref-BusTech_178-0""><a href=""#cite_note-BusTech-178"">[178]</a></sup> Apple officials stated that Jobs was victim to a ""common bug"" and was taking antibiotics,<sup class=""reference"" id=""cite_ref-appleinsider_179-0""><a href=""#cite_note-appleinsider-179"">[179]</a></sup> while others surmised his <a href=""/wiki/Cachexia"" title=""Cachexia"">cachectic appearance</a> was due to the Whipple procedure.<sup class=""reference"" id=""cite_ref-Elmer_172-1""><a href=""#cite_note-Elmer-172"">[172]</a></sup> During a July conference call discussing Apple earnings, participants responded to repeated questions about Jobs's health by insisting that it was a ""private matter"". Others said that shareholders had a right to know more, given Jobs's hands-on approach to running his company.<sup class=""reference"" id=""cite_ref-marketingdoctor_180-0""><a href=""#cite_note-marketingdoctor-180"">[180]</a></sup><sup class=""reference"" id=""cite_ref-medpagetoday_181-0""><a href=""#cite_note-medpagetoday-181"">[181]</a></sup> Based on an off-the-record phone conversation with Jobs, <i><a href=""/wiki/The_New_York_Times"" title=""The New York Times"">The New York Times</a></i> reported, ""While his health problems amounted to a good deal more than 'a common bug', they weren't life-threatening and he doesn't have a recurrence of cancer.""<sup class=""reference"" id=""cite_ref-Apple_New_York_Times_Joe_Nocera_182-0""><a href=""#cite_note-Apple_New_York_Times_Joe_Nocera-182"">[182]</a></sup>
</p>, <p>On August 28, 2008, <a href=""/wiki/Bloomberg_News"" title=""Bloomberg News"">Bloomberg</a> mistakenly published a 2500-word <a href=""/wiki/Obituary"" title=""Obituary"">obituary</a> of Jobs in its corporate news service, containing blank spaces for his age and cause of death. News carriers customarily stockpile up-to-date obituaries to facilitate news delivery in the event of a well-known figure's death. Although the error was promptly rectified, many news carriers and blogs reported on it,<sup class=""reference"" id=""cite_ref-Steve_Jobs_183-0""><a href=""#cite_note-Steve_Jobs-183"">[183]</a></sup> intensifying rumors concerning Jobs's health.<sup class=""reference"" id=""cite_ref-Bloomberg_publishes_Jobs_obit_but_why?_184-0""><a href=""#cite_note-Bloomberg_publishes_Jobs_obit_but_why?-184"">[184]</a></sup> Jobs responded at Apple's September 2008 <i>Let's Rock</i> keynote by paraphrasing <a href=""/wiki/Mark_Twain"" title=""Mark Twain"">Mark Twain</a>: ""Reports of my death are greatly exaggerated.""<sup class=""reference"" id=""cite_ref-twaintweet_185-0""><a href=""#cite_note-twaintweet-185"">[185]</a></sup><sup class=""reference"" id=""cite_ref-Apple_posts_186-0""><a href=""#cite_note-Apple_posts-186"">[186]</a></sup> At a subsequent media event, Jobs concluded his presentation with a slide reading ""110/70"", referring to his <a href=""/wiki/Blood_pressure"" title=""Blood pressure"">blood pressure</a>, stating he would not address further questions about his health.<sup class=""reference"" id=""cite_ref-engadget_187-0""><a href=""#cite_note-engadget-187"">[187]</a></sup>
</p>, <p>On December 16, 2008, Apple announced that marketing vice-president <a class=""mw-redirect"" href=""/wiki/Philip_W._Schiller"" title=""Philip W. Schiller"">Phil Schiller</a> would deliver the company's final keynote address at the <a class=""mw-redirect"" href=""/wiki/Macworld_Conference_and_Expo"" title=""Macworld Conference and Expo"">Macworld Conference and Expo</a> 2009, again reviving questions about Jobs's health.<sup class=""reference"" id=""cite_ref-Apple_New_York_Times_Brad_Stone_188-0""><a href=""#cite_note-Apple_New_York_Times_Brad_Stone-188"">[188]</a></sup><sup class=""reference"" id=""cite_ref-HealthDeclining_189-0""><a href=""#cite_note-HealthDeclining-189"">[189]</a></sup> In a statement given on January 5, 2009, on <a class=""mw-redirect"" href=""/wiki/Apple.com"" title=""Apple.com"">Apple.com</a>, Jobs said that he had been suffering from a ""<a class=""mw-redirect"" href=""/wiki/Hormone_imbalance"" title=""Hormone imbalance"">hormone imbalance</a>"" for several months.<sup class=""reference"" id=""cite_ref-Apple_bbc_190-0""><a href=""#cite_note-Apple_bbc-190"">[190]</a></sup><sup class=""reference"" id=""cite_ref-Letter_from_Apple_CEO_Steve_Jobs_191-0""><a href=""#cite_note-Letter_from_Apple_CEO_Steve_Jobs-191"">[191]</a></sup>
</p>, <p>On January 14, 2009, Jobs wrote in an internal Apple memo that in the previous week he had ""learned that my health-related issues are more complex than I originally thought"".<sup class=""reference"" id=""cite_ref-absence_192-0""><a href=""#cite_note-absence-192"">[192]</a></sup> He announced a six-month leave of absence until the end of June 2009, to allow him to better focus on his health. Tim Cook, who previously acted as CEO in Jobs's 2004 absence, became acting CEO of Apple, with Jobs still involved with ""major strategic decisions"".<sup class=""reference"" id=""cite_ref-absence_192-1""><a href=""#cite_note-absence-192"">[192]</a></sup>
</p>, <p>In 2009, Tim Cook offered a portion of his liver to Jobs, since both share a rare blood type and the donor liver can regenerate tissue after such an operation. Jobs yelled, ""I'll never let you do that. I'll never do that.""<sup class=""reference"" id=""cite_ref-193""><a href=""#cite_note-193"">[193]</a></sup>
</p>, <p>In April 2009, Jobs underwent a <a class=""mw-redirect"" href=""/wiki/Liver_transplant"" title=""Liver transplant"">liver transplant</a> at <a href=""/wiki/Methodist_University_Hospital"" title=""Methodist University Hospital"">Methodist University Hospital</a> Transplant Institute in <a href=""/wiki/Memphis,_Tennessee"" title=""Memphis, Tennessee"">Memphis, Tennessee</a>.<sup class=""reference"" id=""cite_ref-cnntrans_194-0""><a href=""#cite_note-cnntrans-194"">[194]</a></sup><sup class=""reference"" id=""cite_ref-celebritydiagnosis3_195-0""><a href=""#cite_note-celebritydiagnosis3-195"">[195]</a></sup><sup class=""reference"" id=""cite_ref-196""><a href=""#cite_note-196"">[196]</a></sup> Jobs's prognosis was described as ""excellent"".<sup class=""reference"" id=""cite_ref-cnntrans_194-1""><a href=""#cite_note-cnntrans-194"">[194]</a></sup>
</p>, <p>On January 17, 2011, a year and a half after Jobs returned to work following the liver transplant, Apple announced that he had been granted a medical leave of absence. Jobs announced his leave in a letter to employees, stating his decision was made ""so he could focus on his health"". As it did at the time of his 2009 medical leave, Apple announced that Tim Cook would run day-to-day operations and that Jobs would continue to be involved in major strategic decisions at the company.<sup class=""reference"" id=""cite_ref-times_jan17_2011_197-0""><a href=""#cite_note-times_jan17_2011-197"">[197]</a></sup><sup class=""reference"" id=""cite_ref-medicalleave_198-0""><a href=""#cite_note-medicalleave-198"">[198]</a></sup> While on leave, Jobs appeared at the <a href=""/wiki/IPad_2"" title=""IPad 2"">iPad 2</a> launch event on March 2, the <a href=""/wiki/Apple_Worldwide_Developers_Conference"" title=""Apple Worldwide Developers Conference"">WWDC</a> keynote introducing <a href=""/wiki/ICloud"" title=""ICloud"">iCloud</a> on June 6, and before the Cupertino City Council on June 7.<sup class=""reference"" id=""cite_ref-UFO_HQ_199-0""><a href=""#cite_note-UFO_HQ-199"">[199]</a></sup>
</p>, <p>On August 24, 2011, Jobs announced his resignation as Apple's CEO, writing to the board, ""I have always said if there ever came a day when I could no longer meet my duties and expectations as Apple's CEO, I would be the first to let you know. Unfortunately, that day has come.""<sup class=""reference"" id=""cite_ref-200""><a href=""#cite_note-200"">[200]</a></sup> Jobs became chairman of the board and named Tim Cook as his successor as CEO.<sup class=""reference"" id=""cite_ref-Apple_Resignation_Letter_201-0""><a href=""#cite_note-Apple_Resignation_Letter-201"">[201]</a></sup><sup class=""reference"" id=""cite_ref-apple-2011pr-jobs-resigns_202-0""><a href=""#cite_note-apple-2011pr-jobs-resigns-202"">[202]</a></sup> Jobs continued to work for Apple until the day before his death six weeks later.<sup class=""reference"" id=""cite_ref-gizmododaybeforehedied_203-0""><a href=""#cite_note-gizmododaybeforehedied-203"">[203]</a></sup><sup class=""reference"" id=""cite_ref-Steve_Jobs_Quits_204-0""><a href=""#cite_note-Steve_Jobs_Quits-204"">[204]</a></sup><sup class=""reference"" id=""cite_ref-Steve_Jobs_Resigns_As_CEO_Of_Apple_205-0""><a href=""#cite_note-Steve_Jobs_Resigns_As_CEO_Of_Apple-205"">[205]</a></sup>
</p>, <p><span class=""anchor"" id=""Illness_and_death""></span>
</p>, <p>Jobs died at his <a href=""/wiki/Palo_Alto,_California"" title=""Palo Alto, California"">Palo Alto, California</a>, home around 3 p.m. (<a href=""/wiki/Pacific_Time_Zone"" title=""Pacific Time Zone"">PDT</a>) on October 5, 2011, due to complications from a <a href=""/wiki/Relapse"" title=""Relapse"">relapse</a> of his previously treated islet-cell <a href=""/wiki/Pancreatic_neuroendocrine_tumor"" title=""Pancreatic neuroendocrine tumor"">pancreatic neuroendocrine tumor</a>,<sup class=""reference"" id=""cite_ref-NYT_obit_14-2""><a href=""#cite_note-NYT_obit-14"">[14]</a></sup><sup class=""reference"" id=""cite_ref-Rare_Pancreatic_Cancer_Caused_Steve_Jobs_206-0""><a href=""#cite_note-Rare_Pancreatic_Cancer_Caused_Steve_Jobs-206"">[206]</a></sup><sup class=""reference"" id=""cite_ref-Steve_Jobs,_Apple_co-founder,_dies_at_56_207-0""><a href=""#cite_note-Steve_Jobs,_Apple_co-founder,_dies_at_56-207"">[207]</a></sup> which resulted in <a href=""/wiki/Respiratory_arrest"" title=""Respiratory arrest"">respiratory arrest</a>.<sup class=""reference"" id=""cite_ref-Respiratory_Arrest_208-0""><a href=""#cite_note-Respiratory_Arrest-208"">[208]</a></sup> He had lost consciousness the day before and died with his wife, children, and sisters at his side.<sup class=""reference"" id=""cite_ref-eulogy_209-0""><a href=""#cite_note-eulogy-209"">[209]</a></sup> His sister, <a href=""/wiki/Mona_Simpson"" title=""Mona Simpson"">Mona Simpson</a>, described his death thus: ""Steve's final words, hours earlier, were monosyllables, repeated three times. Before embarking, he'd looked at his sister Patty, then for a long time at his children, then at his life's partner, Laurene, and then over their shoulders past them. Steve's final words were: 'Oh wow. Oh wow. Oh wow.'"" He then lost consciousness and died several hours later.<sup class=""reference"" id=""cite_ref-eulogy_209-1""><a href=""#cite_note-eulogy-209"">[209]</a></sup> A small private funeral was held on October 7, 2011, the details of which, out of respect for Jobs's family, were not made public.<sup class=""reference"" id=""cite_ref-Steve_Jobs_Funeral_Is_Friday_210-0""><a href=""#cite_note-Steve_Jobs_Funeral_Is_Friday-210"">[210]</a></sup>
</p>, <p>Apple<sup class=""reference"" id=""cite_ref-AppleStatement_211-0""><a href=""#cite_note-AppleStatement-211"">[211]</a></sup> and Pixar each issued announcements of his death.<sup class=""reference"" id=""cite_ref-Pixar_212-0""><a href=""#cite_note-Pixar-212"">[212]</a></sup> Apple announced on the same day that they had no plans for a public service, but were encouraging ""well-wishers"" to send their remembrance messages to an email address created to receive such messages.<sup class=""reference"" id=""cite_ref-Remembering_Steve_Jobs4_213-0""><a href=""#cite_note-Remembering_Steve_Jobs4-213"">[213]</a></sup> <a href=""/wiki/Apple_Inc."" title=""Apple Inc."">Apple</a> and <a href=""/wiki/Microsoft"" title=""Microsoft"">Microsoft</a> both flew their flags at <a class=""mw-redirect"" href=""/wiki/Half-staff"" title=""Half-staff"">half-staff</a> throughout their respective headquarters and campuses.<sup class=""reference"" id=""cite_ref-214""><a href=""#cite_note-214"">[214]</a></sup><sup class=""reference"" id=""cite_ref-215""><a href=""#cite_note-215"">[215]</a></sup>
</p>, <p><a href=""/wiki/Bob_Iger"" title=""Bob Iger"">Bob Iger</a> ordered all <a class=""mw-redirect"" href=""/wiki/Disney"" title=""Disney"">Disney</a> properties, including <a href=""/wiki/Walt_Disney_World"" title=""Walt Disney World"">Walt Disney World</a> and <a href=""/wiki/Disneyland"" title=""Disneyland"">Disneyland</a>, to fly their flags at half-staff from October 6 to 12, 2011.<sup class=""reference"" id=""cite_ref-216""><a href=""#cite_note-216"">[216]</a></sup> For two weeks  following his death, Apple displayed on its corporate Web site a simple page that showed Jobs's name and lifespan next to his grayscale portrait.<sup class=""reference"" id=""cite_ref-Steve_Jobs:_The_homepage_tributes_217-0""><a href=""#cite_note-Steve_Jobs:_The_homepage_tributes-217"">[217]</a></sup><sup class=""reference"" id=""cite_ref-Apple_website_pays_tribute_to_Steve_Jobs_218-0""><a href=""#cite_note-Apple_website_pays_tribute_to_Steve_Jobs-218"">[218]</a></sup><sup class=""reference"" id=""cite_ref-Remembering_Steve_Jobs_219-0""><a href=""#cite_note-Remembering_Steve_Jobs-219"">[219]</a></sup> On October 19, 2011, Apple employees held a private memorial service for Jobs on the Apple campus in Cupertino. Jobs's widow, Laurene, was in attendance, as well as Cook, <a href=""/wiki/William_Campbell_(business_executive)"" title=""William Campbell (business executive)"">Bill Campbell</a>, <a href=""/wiki/Norah_Jones"" title=""Norah Jones"">Norah Jones</a>, <a href=""/wiki/Al_Gore"" title=""Al Gore"">Al Gore</a>, and <a href=""/wiki/Coldplay"" title=""Coldplay"">Coldplay</a>.<sup class=""reference"" id=""cite_ref-cel_220-0""><a href=""#cite_note-cel-220"">[220]</a></sup> Some of Apple's retail stores closed briefly so employees could attend the memorial. A video of the service was uploaded to Apple's website.<sup class=""reference"" id=""cite_ref-cel_220-1""><a href=""#cite_note-cel-220"">[220]</a></sup>
</p>, <p>California Governor <a href=""/wiki/Jerry_Brown"" title=""Jerry Brown"">Jerry Brown</a> declared Sunday, October 16, 2011, to be ""Steve Jobs Day"".<sup class=""reference"" id=""cite_ref-221""><a href=""#cite_note-221"">[221]</a></sup> On that day, an invitation-only memorial was held at <a href=""/wiki/Stanford_University"" title=""Stanford University"">Stanford University</a>. Those in attendance included Apple and other tech company executives, members of the media, celebrities, close friends of Jobs, and politicians, along with Jobs's family. <a href=""/wiki/Bono"" title=""Bono"">Bono</a>, <a href=""/wiki/Yo-Yo_Ma"" title=""Yo-Yo Ma"">Yo-Yo Ma</a>, and <a href=""/wiki/Joan_Baez"" title=""Joan Baez"">Joan Baez</a> performed at the service, which lasted longer than an hour. The service was highly secured, with guards at all of the university's gates, and a helicopter flying overhead from an area news station.<sup class=""reference"" id=""cite_ref-222""><a href=""#cite_note-222"">[222]</a></sup><sup class=""reference"" id=""cite_ref-223""><a href=""#cite_note-223"">[223]</a></sup> Each attendee was given a small brown box as a ""farewell gift"" from Jobs. The box contained a copy of the <i><a href=""/wiki/Autobiography_of_a_Yogi"" title=""Autobiography of a Yogi"">Autobiography of a Yogi</a></i> by <a href=""/wiki/Paramahansa_Yogananda"" title=""Paramahansa Yogananda"">Paramahansa Yogananda</a>.<sup class=""reference"" id=""cite_ref-boxbook_224-0""><a href=""#cite_note-boxbook-224"">[224]</a></sup>
</p>, <p>Childhood friend and fellow Apple co-founder <a href=""/wiki/Steve_Wozniak"" title=""Steve Wozniak"">Steve Wozniak</a>,<sup class=""reference"" id=""cite_ref-225""><a href=""#cite_note-225"">[225]</a></sup> former owner of what would become <a href=""/wiki/Pixar"" title=""Pixar"">Pixar</a>, <a href=""/wiki/George_Lucas"" title=""George Lucas"">George Lucas</a>,<sup class=""reference"" id=""cite_ref-http://postcards.blogs.fortune.cnn.com/2011/10/06/george-lucas-steve-jobs/_226-0""><a href=""#cite_note-http://postcards.blogs.fortune.cnn.com/2011/10/06/george-lucas-steve-jobs/-226"">[226]</a></sup> former rival, <a href=""/wiki/Microsoft"" title=""Microsoft"">Microsoft</a> co-founder <a href=""/wiki/Bill_Gates"" title=""Bill Gates"">Bill Gates</a>,<sup class=""reference"" id=""cite_ref-thegatesnotes_227-0""><a href=""#cite_note-thegatesnotes-227"">[227]</a></sup> and President <a href=""/wiki/Barack_Obama"" title=""Barack Obama"">Barack Obama</a><sup class=""reference"" id=""cite_ref-Statement_by_the_President_on_the_Passing_of_Steve_Jobs_228-0""><a href=""#cite_note-Statement_by_the_President_on_the_Passing_of_Steve_Jobs-228"">[228]</a></sup> all offered statements in response to his death.
</p>, <p>Per his request, Jobs is buried in an <a href=""/wiki/Unmarked_grave"" title=""Unmarked grave"">unmarked grave</a> at <a href=""/wiki/Alta_Mesa_Memorial_Park"" title=""Alta Mesa Memorial Park"">Alta Mesa Memorial Park</a>, the only <a href=""/wiki/Nonsectarian"" title=""Nonsectarian"">nonsectarian</a> cemetery in Palo Alto.<sup class=""reference"" id=""cite_ref-abcnews_229-0""><a href=""#cite_note-abcnews-229"">[229]</a></sup><sup class=""reference"" id=""cite_ref-yahoo5_230-0""><a href=""#cite_note-yahoo5-230"">[230]</a></sup>
</p>, <p>On October 7, 2021, Apple released a commemorative <a href=""/wiki/YouTube"" title=""YouTube"">YouTube</a> video on the tenth anniversary of Jobs's passing.<sup class=""reference"" id=""cite_ref-231""><a href=""#cite_note-231"">[231]</a></sup>
</p>, <p>Jobs's design aesthetic was influenced by philosophies of Zen and Buddhism. In India, he experienced Buddhism while on his seven-month spiritual journey,<sup class=""reference"" id=""cite_ref-autogenerated1_232-0""><a href=""#cite_note-autogenerated1-232"">[232]</a></sup> and his sense of intuition was influenced by the spiritual people with whom he studied.<sup class=""reference"" id=""cite_ref-autogenerated1_232-1""><a href=""#cite_note-autogenerated1-232"">[232]</a></sup> He also learned from many references and sources, such as <a href=""/wiki/Modern_architecture"" title=""Modern architecture"">modernist</a> architectural style of <a href=""/wiki/Joseph_Eichler"" title=""Joseph Eichler"">Joseph Eichler</a>,<sup class=""noprint Inline-Template Template-Fact"" style=""white-space:nowrap;"">[<i><a href=""/wiki/Wikipedia:Citation_needed"" title=""Wikipedia:Citation needed""><span title=""This claim needs references to reliable sources. (April 2018)"">citation needed</span></a></i>]</sup> and the <a href=""/wiki/Industrial_design"" title=""Industrial design"">industrial designs</a> of <a href=""/wiki/Richard_Sapper"" title=""Richard Sapper"">Richard Sapper</a><sup class=""reference"" id=""cite_ref-233""><a href=""#cite_note-233"">[233]</a></sup> and <a href=""/wiki/Dieter_Rams"" title=""Dieter Rams"">Dieter Rams</a>.<sup class=""noprint Inline-Template Template-Fact"" style=""white-space:nowrap;"">[<i><a href=""/wiki/Wikipedia:Citation_needed"" title=""Wikipedia:Citation needed""><span title=""This claim needs references to reliable sources. (April 2018)"">citation needed</span></a></i>]</sup>
</p>, <p>According to Apple co-founder Steve Wozniak, ""Steve didn't ever code. He wasn't an engineer and he didn't do any original design...""<sup class=""reference"" id=""cite_ref-234""><a href=""#cite_note-234"">[234]</a></sup><sup class=""reference"" id=""cite_ref-Does_Steve_Jobs_know_how_to_code_235-0""><a href=""#cite_note-Does_Steve_Jobs_know_how_to_code-235"">[235]</a></sup> <a href=""/wiki/Daniel_Kottke"" title=""Daniel Kottke"">Daniel Kottke</a>, one of Apple's earliest employees and a college friend of Jobs's, stated: ""Between Woz and Jobs, Woz was the innovator, the inventor. Steve Jobs was the marketing person.""<sup class=""reference"" id=""cite_ref-236""><a href=""#cite_note-236"">[236]</a></sup>
</p>, <p>He is listed as either primary inventor or co-inventor in 346 United States patents or patent applications related to a range of technologies from actual computer and portable devices to user interfaces (including touch-based), speakers, keyboards, power adapters, staircases, clasps, sleeves, <a href=""/wiki/Lanyard"" title=""Lanyard"">lanyards</a> and packages. Jobs's contributions to most of his patents were to ""the look and feel of the product"". His industrial design chief <a class=""mw-redirect"" href=""/wiki/Jonathan_Ive"" title=""Jonathan Ive"">Jonathan Ive</a> had his name along with Jobs's name for 200 of the patents.<sup class=""reference"" id=""cite_ref-Portfolio_237-0""><a href=""#cite_note-Portfolio-237"">[237]</a></sup> Most of these are design patents (specific product designs; for example, Jobs listed as primary inventor in patents for both original and lamp-style <a href=""/wiki/IMac"" title=""IMac"">iMacs</a>, as well as <a href=""/wiki/PowerBook_G4"" title=""PowerBook G4"">PowerBook G4 Titanium</a>) as opposed to utility patents (inventions).<sup class=""reference"" id=""cite_ref-Patents_registry_database_1_238-0""><a href=""#cite_note-Patents_registry_database_1-238"">[238]</a></sup><sup class=""reference"" id=""cite_ref-Patents_registry_database_2_239-0""><a href=""#cite_note-Patents_registry_database_2-239"">[239]</a></sup> He has 43 issued US patents on inventions.<sup class=""reference"" id=""cite_ref-Patents_registry_database_1_238-1""><a href=""#cite_note-Patents_registry_database_1-238"">[238]</a></sup> The patent on the Mac OS X <a class=""mw-redirect"" href=""/wiki/Dock_(Mac_OS_X)"" title=""Dock (Mac OS X)"">Dock</a> user interface with ""magnification"" feature was issued the day before he died.<sup class=""reference"" id=""cite_ref-patent_claim_240-0""><a href=""#cite_note-patent_claim-240"">[240]</a></sup> Although Jobs had little involvement in the engineering and technical side of the original Apple computers,<sup class=""reference"" id=""cite_ref-Does_Steve_Jobs_know_how_to_code_235-1""><a href=""#cite_note-Does_Steve_Jobs_know_how_to_code-235"">[235]</a></sup> Jobs later used his CEO position to directly involve himself with product design.<sup class=""reference"" id=""cite_ref-241""><a href=""#cite_note-241"">[241]</a></sup>
</p>, <p>Involved in many projects throughout his career was his long-time marketing executive and confidant <a href=""/wiki/Joanna_Hoffman"" title=""Joanna Hoffman"">Joanna Hoffman</a>, known as one of the few employees at Apple and NeXT who could successfully stand up to Jobs while also engaging with him.<sup class=""reference"" id=""cite_ref-hoffman_242-0""><a href=""#cite_note-hoffman-242"">[242]</a></sup>
</p>, <p>Even while terminally ill in the hospital, Jobs sketched new devices that would hold the iPad in a hospital bed.<sup class=""reference"" id=""cite_ref-eulogy_209-2""><a href=""#cite_note-eulogy-209"">[209]</a></sup> He also despised the oxygen monitor on his finger, and suggested ways to revise the design for simplicity.<sup class=""reference"" id=""cite_ref-Ever_Inventor_2_243-0""><a href=""#cite_note-Ever_Inventor_2-243"">[243]</a></sup>
</p>, <p>Since his death, the former Apple CEO has won 141 patents, more than most inventors win during their lifetimes. Currently, Jobs holds over 450 patents.<sup class=""reference"" id=""cite_ref-244""><a href=""#cite_note-244"">[244]</a></sup>
</p>, <p>Although entirely designed by Steve Wozniak, Jobs had the idea of selling the <a href=""/wiki/Desktop_computer"" title=""Desktop computer"">desktop computer</a>, which led to the formation of <a class=""mw-redirect"" href=""/wiki/Apple_Computer"" title=""Apple Computer"">Apple Computer</a> in 1976. Both Jobs and Wozniak constructed several of the first Apple I prototypes by hand, and sold some of their belongings in order to do so. Eventually, 200 units were produced.<sup class=""reference"" id=""cite_ref-AppleStoryPart1_75-1""><a href=""#cite_note-AppleStoryPart1-75"">[75]</a></sup>
</p>, <p>The <a href=""/wiki/Apple_II"" title=""Apple II"">Apple II</a> is an <a class=""mw-redirect"" href=""/wiki/8-bit"" title=""8-bit"">8-bit</a> <a href=""/wiki/Home_computer"" title=""Home computer"">home computer</a>, one of the world's first highly successful mass-produced <a href=""/wiki/Microcomputer"" title=""Microcomputer"">microcomputer</a> products,<sup class=""reference"" id=""cite_ref-Ars_Technica_2005-12-15_83-1""><a href=""#cite_note-Ars_Technica_2005-12-15-83"">[83]</a></sup> designed primarily by Wozniak (though Jobs oversaw the development of the Apple II's unusual case<sup class=""reference"" id=""cite_ref-FOOTNOTEIsaacson201573–83_245-0""><a href=""#cite_note-FOOTNOTEIsaacson201573–83-245"">[245]</a></sup> and <a href=""/wiki/Rod_Holt"" title=""Rod Holt"">Rod Holt</a> developed the unique power supply<sup class=""reference"" id=""cite_ref-wozorg_81-1""><a href=""#cite_note-wozorg-81"">[81]</a></sup>). It was introduced in 1977 at the <a href=""/wiki/West_Coast_Computer_Faire"" title=""West Coast Computer Faire"">West Coast Computer Faire</a> by Jobs and Wozniak and was the first consumer product sold by Apple.
</p>, <p>The Lisa is a personal computer designed by Apple during the early 1980s. It was the first personal computer to offer a <a href=""/wiki/Graphical_user_interface"" title=""Graphical user interface"">graphical user interface</a> in a machine aimed at individual business users. Development of the Lisa began in 1978.<sup class=""reference"" id=""cite_ref-246""><a href=""#cite_note-246"">[246]</a></sup> The Lisa sold poorly, with only 100,000 units sold.<sup class=""reference"" id=""cite_ref-247""><a href=""#cite_note-247"">[247]</a></sup>
</p>, <p>In 1982, after Jobs was forced out of the Lisa project,<sup class=""reference"" id=""cite_ref-248""><a href=""#cite_note-248"">[248]</a></sup> he joined the <a href=""/wiki/Macintosh"" title=""Macintosh"">Macintosh</a> project. The Macintosh is not a direct descendant of Lisa, although there are obvious similarities between the systems. The final revision, the Lisa 2/10, was modified and sold as the <a href=""/wiki/Macintosh_XL"" title=""Macintosh XL"">Macintosh XL</a>.<sup class=""reference"" id=""cite_ref-FOOTNOTELinzmayer200479_249-0""><a href=""#cite_note-FOOTNOTELinzmayer200479-249"">[249]</a></sup>
</p>, <p>Once he joined the <a href=""/wiki/Macintosh_128K"" title=""Macintosh 128K"">original Macintosh</a> team, Jobs took over the project after Wozniak had experienced a traumatic airplane accident and temporarily left the company.<sup class=""reference"" id=""cite_ref-TheVerge_95-2""><a href=""#cite_note-TheVerge-95"">[95]</a></sup> Jobs introduced the Macintosh computer on January 24, 1984. This was the first mass-market personal computer featuring an integral <a href=""/wiki/Graphical_user_interface"" title=""Graphical user interface"">graphical user interface</a> and <a class=""mw-redirect"" href=""/wiki/Mouse_(computing)"" title=""Mouse (computing)"">mouse</a>.<sup class=""reference"" id=""cite_ref-250""><a href=""#cite_note-250"">[250]</a></sup> This first model was later renamed to ""Macintosh 128k"" for uniqueness amongst a populous family of subsequently updated models which are also based on Apple's same proprietary architecture. Since 1998, Apple has largely phased out the Macintosh name in favor of ""Mac"", though the product family has been nicknamed ""Mac"" or ""the Mac"" since the development of the first model. The Macintosh was introduced by a US$1.5 million <a href=""/wiki/Ridley_Scott"" title=""Ridley Scott"">Ridley Scott</a> television commercial, ""<a href=""/wiki/1984_(advertisement)"" title=""1984 (advertisement)"">1984</a>"".<sup class=""reference"" id=""cite_ref-FOOTNOTELinzmayer2004113_251-0""><a href=""#cite_note-FOOTNOTELinzmayer2004113-251"">[251]</a></sup> It most notably aired during the third quarter of <a href=""/wiki/Super_Bowl_XVIII"" title=""Super Bowl XVIII"">Super Bowl XVIII</a> on January 22, 1984, and some people consider the ad a ""watershed event""<sup class=""reference"" id=""cite_ref-252""><a href=""#cite_note-252"">[252]</a></sup> and a ""masterpiece"".<sup class=""reference"" id=""cite_ref-masterpiece_253-0""><a href=""#cite_note-masterpiece-253"">[253]</a></sup> <a href=""/wiki/Regis_McKenna"" title=""Regis McKenna"">Regis McKenna</a> called the ad ""more successful than the Mac itself"".<sup class=""reference"" id=""cite_ref-254""><a href=""#cite_note-254"">[254]</a></sup> ""1984"" uses an unnamed heroine to represent the coming of the Macintosh (indicated by a <a href=""/wiki/Pablo_Picasso"" title=""Pablo Picasso"">Picasso</a>-style picture of the computer on her white <a href=""/wiki/Sleeveless_shirt"" title=""Sleeveless shirt"">tank top</a>) as a means of saving humanity from the conformity of IBM's attempts to dominate the computer industry. The ad <a href=""/wiki/Allusion"" title=""Allusion"">alludes</a> to <a href=""/wiki/George_Orwell"" title=""George Orwell"">George Orwell</a>'s novel, <i><a href=""/wiki/Nineteen_Eighty-Four"" title=""Nineteen Eighty-Four"">Nineteen Eighty-Four</a></i>, which describes a <a href=""/wiki/Dystopia"" title=""Dystopia"">dystopian future</a> ruled by a televised ""<a class=""mw-redirect"" href=""/wiki/Big_Brother_(1984)"" title=""Big Brother (1984)"">Big Brother</a>.""<sup class=""reference"" id=""cite_ref-cellini_255-0""><a href=""#cite_note-cellini-255"">[255]</a></sup><sup class=""reference"" id=""cite_ref-256""><a href=""#cite_note-256"">[256]</a></sup>
</p>, <p>The Macintosh, however, was expensive, which hindered its ability to be competitive in a market already dominated by the <a href=""/wiki/Commodore_64"" title=""Commodore 64"">Commodore 64</a> for consumers, as well as the <a href=""/wiki/IBM_Personal_Computer"" title=""IBM Personal Computer"">IBM Personal Computer</a> and its accompanying <a href=""/wiki/IBM_PC_compatible"" title=""IBM PC compatible"">clone</a> market for businesses.<sup class=""reference"" id=""cite_ref-arstech_257-0""><a href=""#cite_note-arstech-257"">[257]</a></sup> Macintosh systems still found success in education and desktop publishing and kept Apple as the second-largest PC manufacturer for the next decade.
</p>, <p>After Jobs was forced out of Apple in 1985, he started <a href=""/wiki/NeXT"" title=""NeXT"">NeXT</a>, a <a href=""/wiki/Workstation"" title=""Workstation"">workstation</a> computer company. The NeXT Computer was introduced in 1988 at a <a href=""/wiki/NeXT_Introduction"" title=""NeXT Introduction"">lavish launch event</a>. Using the NeXT Computer, <a href=""/wiki/Tim_Berners-Lee"" title=""Tim Berners-Lee"">Tim Berners-Lee</a> created the world's first <a href=""/wiki/Web_browser"" title=""Web browser"">web browser</a>, the <a href=""/wiki/WorldWideWeb"" title=""WorldWideWeb"">WorldWideWeb</a>. The NeXT Computer's operating system, named <a href=""/wiki/NeXTSTEP"" title=""NeXTSTEP"">NeXTSTEP</a>, begat <a href=""/wiki/Darwin_(operating_system)"" title=""Darwin (operating system)"">Darwin</a>, which is now the foundation of most of Apple's products such as <a href=""/wiki/Macintosh"" title=""Macintosh"">Macintosh</a>'s <a href=""/wiki/MacOS"" title=""MacOS"">macOS</a> and <a href=""/wiki/IPhone"" title=""IPhone"">iPhone</a>'s <a href=""/wiki/IOS"" title=""IOS"">iOS</a>.<sup class=""reference"" id=""cite_ref-258""><a href=""#cite_note-258"">[258]</a></sup><sup class=""reference"" id=""cite_ref-NeXT_259-0""><a href=""#cite_note-NeXT-259"">[259]</a></sup>
</p>, <p>Apple <a href=""/wiki/IMac_G3"" title=""IMac G3"">iMac G3</a> was introduced in 1998 and its innovative design was directly the result of Jobs's return to Apple. Apple boasted ""the back of our computer looks better than the front of anyone else's.""<sup class=""reference"" id=""cite_ref-260""><a href=""#cite_note-260"">[260]</a></sup> Described as ""cartoonlike"", the first iMac, clad in Bondi Blue plastic, was unlike any personal computer that came before. In 1999, Apple introduced the Graphite gray Apple iMac and since has varied the shape, color and size considerably while maintaining the all-in-one design. Design ideas were intended to create a connection with the user such as the handle and a ""breathing"" light effect when the computer went to sleep.<sup class=""reference"" id=""cite_ref-261""><a href=""#cite_note-261"">[261]</a></sup> The Apple iMac sold for $1,299 at that time. The iMac also featured forward-thinking changes, such as eschewing the <a href=""/wiki/Floppy_disk"" title=""Floppy disk"">floppy disk drive</a> and moving exclusively to <a href=""/wiki/USB"" title=""USB"">USB</a> for connecting peripherals. This latter change resulted, through the iMac's success, in the interface being popularized among third-party peripheral makers—as evidenced by the fact that many early USB peripherals were made of translucent plastic (to match the iMac design).<sup class=""reference"" id=""cite_ref-262""><a href=""#cite_note-262"">[262]</a></sup>
</p>, <p>iTunes is a <a class=""mw-redirect"" href=""/wiki/Media_player_(application_software)"" title=""Media player (application software)"">media player</a>, media library, online radio broadcaster, and mobile device management application developed by Apple. It is used to play, download, and organize digital <a href=""/wiki/Sound_recording_and_reproduction"" title=""Sound recording and reproduction"">audio</a> and video (as well as other types of media available on the iTunes Store) on personal computers running the <a href=""/wiki/MacOS"" title=""MacOS"">macOS</a> and <a href=""/wiki/Microsoft_Windows"" title=""Microsoft Windows"">Microsoft Windows</a> operating systems. The <a href=""/wiki/ITunes_Store"" title=""ITunes Store"">iTunes Store</a> is also available on the <a href=""/wiki/IPod_Touch"" title=""IPod Touch"">iPod Touch</a>, <a href=""/wiki/IPhone"" title=""IPhone"">iPhone</a>, and <a href=""/wiki/IPad"" title=""IPad"">iPad</a>.
</p>, <p>Through the iTunes Store, users can purchase and download music, music videos, television shows, <a href=""/wiki/Audiobook"" title=""Audiobook"">audiobooks</a>, <a href=""/wiki/Podcast"" title=""Podcast"">podcasts</a>, movies, and movie rentals in some countries, and <a href=""/wiki/Ringtone"" title=""Ringtone"">ringtones</a>, available on the iPhone and iPod Touch (fourth generation onward). <a href=""/wiki/Application_software"" title=""Application software"">Application software</a> for the iPhone, iPad and iPod Touch can be downloaded from the <a class=""mw-redirect"" href=""/wiki/App_Store_(iOS)"" title=""App Store (iOS)"">App Store</a>.
</p>, <p>The <a href=""/wiki/IPod_Classic#1st_generation"" title=""IPod Classic"">first generation of iPod</a> was released October 23, 2001. The major innovation of the iPod was its small size achieved by using a 1.8"" hard drive compared to the 2.5"" drives common to players at that time. The capacity of the first generation iPod ranged from 5 GB to 10 GB.<sup class=""reference"" id=""cite_ref-263""><a href=""#cite_note-263"">[263]</a></sup> The iPod sold for US$399 and more than 100,000 iPods were sold before the end of 2001. The introduction of the iPod resulted in Apple becoming a major player in the music industry.<sup class=""reference"" id=""cite_ref-Block_264-0""><a href=""#cite_note-Block-264"">[264]</a></sup> Also, the iPod's success prepared the way for the iTunes music store and the iPhone.<sup class=""reference"" id=""cite_ref-Mia_Carter_265-0""><a href=""#cite_note-Mia_Carter-265"">[265]</a></sup> After the first few generations of iPod, Apple released the touchscreen iPod Touch, the reduced-size <a href=""/wiki/IPod_Mini"" title=""IPod Mini"">iPod Mini</a> and <a href=""/wiki/IPod_Nano"" title=""IPod Nano"">iPod Nano</a>, and the screenless <a href=""/wiki/IPod_Shuffle"" title=""IPod Shuffle"">iPod Shuffle</a> in the following years.<sup class=""reference"" id=""cite_ref-Block_264-1""><a href=""#cite_note-Block-264"">[264]</a></sup>
</p>, <p>Apple began work on the <a href=""/wiki/IPhone_(1st_generation)"" title=""IPhone (1st generation)"">first iPhone</a> in 2005 and the first iPhone was released on June 29, 2007. The iPhone created such a sensation that a survey indicated six out of ten Americans were aware of its release. <i><a href=""/wiki/Time_(magazine)"" title=""Time (magazine)"">Time</a></i> declared it ""Invention of the Year"" for 2007 and included it in the All-TIME 100 Gadgets list in 2010, in the category of Communication<sup class=""reference"" id=""cite_ref-266""><a href=""#cite_note-266"">[266]</a></sup>.<sup class=""reference"" id=""cite_ref-Read_About_The_iPhone_Story_Here_267-0""><a href=""#cite_note-Read_About_The_iPhone_Story_Here-267"">[267]</a></sup> The completed iPhone had multimedia capabilities and functioned as a quad-band touch screen smartphone.<sup class=""reference"" id=""cite_ref-268""><a href=""#cite_note-268"">[268]</a></sup> A year later, the <a href=""/wiki/IPhone_3G"" title=""IPhone 3G"">iPhone 3G</a> was released in July 2008 with three key features: support for GPS, 3G data and tri-band UMTS/HSDPA. In June 2009, the <a href=""/wiki/IPhone_3GS"" title=""IPhone 3GS"">iPhone 3GS</a>, whose improvements included voice control, a better camera, and a faster processor, was introduced by Phil Schiller.<sup class=""reference"" id=""cite_ref-269""><a href=""#cite_note-269"">[269]</a></sup> The iPhone 4 was thinner than previous models, had a five megapixel camera capable of recording video in 720p HD, and added a secondary front-facing camera for video calls.<sup class=""reference"" id=""cite_ref-270""><a href=""#cite_note-270"">[270]</a></sup> A major feature of the <a href=""/wiki/IPhone_4S"" title=""IPhone 4S"">iPhone 4S</a>, introduced in October 2011, was <a href=""/wiki/Siri"" title=""Siri"">Siri</a>, a virtual assistant capable of voice recognition.<sup class=""reference"" id=""cite_ref-Read_About_The_iPhone_Story_Here_267-1""><a href=""#cite_note-Read_About_The_iPhone_Story_Here-267"">[267]</a></sup>
</p>, <p>The iPad is an <a href=""/wiki/IOS"" title=""IOS"">iOS</a>-based line of <a href=""/wiki/Tablet_computer"" title=""Tablet computer"">tablet computers</a> designed and marketed by Apple. The <a href=""/wiki/IPad_(1st_generation)"" title=""IPad (1st generation)"">first iPad</a> was released on April 3, 2010. The <a href=""/wiki/User_interface"" title=""User interface"">user interface</a> is built around the device's <a href=""/wiki/Multi-touch"" title=""Multi-touch"">multi-touch</a> screen, including a <a href=""/wiki/Virtual_keyboard"" title=""Virtual keyboard"">virtual keyboard</a>. The iPad includes built-in <a href=""/wiki/Wi-Fi"" title=""Wi-Fi"">Wi-Fi</a> and cellular connectivity on select models. As of April 2015<sup class=""plainlinks noexcerpt noprint asof-tag update"" style=""display:none;""><a class=""external text"" href=""https://en.wikipedia.org/w/index.php?title=Steve_Jobs&amp;action=edit"">[update]</a></sup>, more than 250 million iPads have been sold.<sup class=""reference"" id=""cite_ref-number_sold_271-0""><a href=""#cite_note-number_sold-271"">[271]</a></sup>
</p>, <p>In 1989, Jobs first met his future wife, <a href=""/wiki/Laurene_Powell_Jobs"" title=""Laurene Powell Jobs"">Laurene Powell</a>, when he gave a lecture at the <a href=""/wiki/Stanford_Graduate_School_of_Business"" title=""Stanford Graduate School of Business"">Stanford Graduate School of Business</a>, where she was a student. Soon after the event, he stated that Laurene ""was right there in the front row in the lecture hall, and I couldn't take my eyes off of her ... kept losing my train of thought, and started feeling a little giddy.""<sup class=""reference"" id=""cite_ref-bsj_19-2""><a href=""#cite_note-bsj-19"">[19]</a></sup><sup class=""noprint Inline-Template"" style=""white-space:nowrap;"">[<i><a href=""/wiki/Wikipedia:Citing_sources"" title=""Wikipedia:Citing sources""><span title="" (March 2018)"">page needed</span></a></i>]</sup> After the lecture, Jobs met up with her in the parking lot and invited her out to dinner. From that point forward, they were together, with a few minor exceptions, for the rest of his life.<sup class=""reference"" id=""cite_ref-bsj_19-3""><a href=""#cite_note-bsj-19"">[19]</a></sup><sup class=""noprint Inline-Template"" style=""white-space:nowrap;"">[<i><a href=""/wiki/Wikipedia:Citing_sources"" title=""Wikipedia:Citing sources""><span title="" (March 2018)"">page needed</span></a></i>]</sup>
</p>, <p>Jobs proposed on New Year's Day 1990 with ""a fistful of freshly picked wildflowers"".<sup class=""reference"" id=""cite_ref-bsj_19-4""><a href=""#cite_note-bsj-19"">[19]</a></sup><sup class=""noprint Inline-Template"" style=""white-space:nowrap;"">[<i><a href=""/wiki/Wikipedia:Citing_sources"" title=""Wikipedia:Citing sources""><span title="" (March 2018)"">page needed</span></a></i>]</sup> They married on March 18, 1991, in a Buddhist ceremony at the <a href=""/wiki/Ahwahnee_Hotel"" title=""Ahwahnee Hotel"">Ahwahnee Hotel</a> in <a href=""/wiki/Yosemite_National_Park"" title=""Yosemite National Park"">Yosemite National Park</a>.<sup class=""reference"" id=""cite_ref-bsj_19-5""><a href=""#cite_note-bsj-19"">[19]</a></sup><sup class=""noprint Inline-Template"" style=""white-space:nowrap;"">[<i><a href=""/wiki/Wikipedia:Citing_sources"" title=""Wikipedia:Citing sources""><span title="" (March 2018)"">page needed</span></a></i>]</sup> Fifty people, including Jobs's father, Paul, and his sister Mona, attended. The ceremony was conducted by Jobs's <a href=""/wiki/Guru"" title=""Guru"">guru</a>, <a class=""mw-redirect"" href=""/wiki/Kobun_Chino_Otogawa"" title=""Kobun Chino Otogawa"">Kobun Chino Otogawa</a>. The vegan wedding cake was in the shape of Yosemite's <a href=""/wiki/Half_Dome"" title=""Half Dome"">Half Dome</a>, and the wedding ended with a hike (during which Laurene's brothers had a snowball fight). Jobs is reported to have said to Mona: ""You see, Mona [...], Laurene is descended from <a href=""/wiki/Joe_Namath"" title=""Joe Namath"">Joe Namath</a>, and we're descended from <a href=""/wiki/John_Muir"" title=""John Muir"">John Muir</a>.""<sup class=""reference"" id=""cite_ref-FOOTNOTEIsaacson2015274_272-0""><a href=""#cite_note-FOOTNOTEIsaacson2015274-272"">[272]</a></sup>
</p>, <p>Jobs's and Powell's first child, Reed, was born in September 1991.<sup class=""reference"" id=""cite_ref-FOOTNOTELinzmayer200481_273-0""><a href=""#cite_note-FOOTNOTELinzmayer200481-273"">[273]</a></sup> Jobs's father, Paul, died a year and a half later, on March 5, 1993. Jobs's childhood home remains a tourist attraction and is currently owned by his stepmother (Paul's second wife), Marilyn Jobs.<sup class=""reference"" id=""cite_ref-274""><a href=""#cite_note-274"">[274]</a></sup>
</p>, <p>Jobs and Powell had two more children, Erin, born in August 1995, and Eve, born in May 1998.<sup class=""reference"" id=""cite_ref-FOOTNOTELinzmayer200481_273-1""><a href=""#cite_note-FOOTNOTELinzmayer200481-273"">[273]</a></sup> The family lived in <a href=""/wiki/Palo_Alto,_California"" title=""Palo Alto, California"">Palo Alto, California</a>.<sup class=""reference"" id=""cite_ref-children_275-0""><a href=""#cite_note-children-275"">[275]</a></sup> A journalist who grew up locally remembered him as owning the house with ""the scariest [Halloween] decorations in Palo Alto ... I don't remember seeing him. I was busy being terrified.""<sup class=""reference"" id=""cite_ref-Halloween_at_Steve_Jobs_276-0""><a href=""#cite_note-Halloween_at_Steve_Jobs-276"">[276]</a></sup>
</p>, <p>Although a billionaire, Jobs made it known that, like Bill Gates, he had stipulated that most of his monetary fortune would not be left to his children.<sup class=""reference"" id=""cite_ref-277""><a href=""#cite_note-277"">[277]</a></sup><sup class=""reference"" id=""cite_ref-278""><a href=""#cite_note-278"">[278]</a></sup> These technology leaders also had in common another family-related area: both men limited their children's access, age appropriate, to social media, computer games and the Internet.<sup class=""reference"" id=""cite_ref-279""><a href=""#cite_note-279"">[279]</a></sup><sup class=""reference"" id=""cite_ref-280""><a href=""#cite_note-280"">[280]</a></sup>
</p>, <p><a href=""/wiki/Chrisann_Brennan"" title=""Chrisann Brennan"">Chrisann Brennan</a> notes that after Jobs was forced out of Apple, ""he apologized many times over for his behavior"" towards her and Lisa. She also states that Jobs ""said that he never took responsibility when he should have, and that he was sorry"".<sup class=""reference"" id=""cite_ref-281""><a href=""#cite_note-281"">[281]</a></sup> By this time, Jobs had developed a strong relationship with Lisa and when she was nine, Jobs had her name on her birth certificate changed from ""Lisa Brennan"" to ""Lisa Brennan-Jobs"".<sup class=""reference"" id=""cite_ref-bite_13-5""><a href=""#cite_note-bite-13"">[13]</a></sup><sup class=""noprint Inline-Template"" style=""white-space:nowrap;"">[<i><a href=""/wiki/Wikipedia:Citing_sources"" title=""Wikipedia:Citing sources""><span title="" (March 2018)"">page needed</span></a></i>]</sup> In addition, Jobs and Brennan developed a working relationship to <a class=""mw-redirect"" href=""/wiki/Co-parent"" title=""Co-parent"">co-parent</a> Lisa, a change Brennan credits to the influence of his newly found biological sister, <a href=""/wiki/Mona_Simpson"" title=""Mona Simpson"">Mona Simpson</a> (who worked to repair the relationship between Lisa and Jobs).<sup class=""reference"" id=""cite_ref-bite_13-6""><a href=""#cite_note-bite-13"">[13]</a></sup><sup class=""noprint Inline-Template"" style=""white-space:nowrap;"">[<i><a href=""/wiki/Wikipedia:Citing_sources"" title=""Wikipedia:Citing sources""><span title="" (March 2018)"">page needed</span></a></i>]</sup> Jobs found Mona after first finding his birth mother, Joanne Schieble Simpson, shortly after he left Apple.<sup class=""reference"" id=""cite_ref-FOOTNOTEIsaacson2011253–255_282-0""><a href=""#cite_note-FOOTNOTEIsaacson2011253–255-282"">[282]</a></sup>
</p>, <p>Jobs did not contact his birth family during his adoptive mother Clara's lifetime, however. He would later tell his official biographer <a href=""/wiki/Walter_Isaacson"" title=""Walter Isaacson"">Walter Isaacson</a>: ""I never wanted [Paul and Clara] to feel like I didn't consider them my parents, because they were totally my parents [...] I loved them so much that I never wanted them to know of my search, and I even had reporters keep it quiet when any of them found out.""<sup class=""reference"" id=""cite_ref-FOOTNOTEIsaacson2011253–255_282-1""><a href=""#cite_note-FOOTNOTEIsaacson2011253–255-282"">[282]</a></sup> However, in 1986, when Jobs was 31, Clara was diagnosed with lung cancer. He began to spend a great deal of time with her and learned more details about her background and his adoption, information that motivated him to find his biological mother. Jobs found on his birth certificate the name of the San Francisco doctor to whom Schieble had turned when she was pregnant. Although the doctor did not help Jobs while he was alive, he left a letter for Jobs to be opened upon his death. As he died soon afterwards, Jobs was given the letter which stated that ""his mother had been an unmarried graduate student from Wisconsin named Joanne Schieble.""<sup class=""reference"" id=""cite_ref-FOOTNOTEIsaacson2011253–255_282-2""><a href=""#cite_note-FOOTNOTEIsaacson2011253–255-282"">[282]</a></sup>
</p>, <p>Jobs only contacted Schieble after Clara died in early 1986 and after he received permission from his father, Paul. In addition, out of respect for Paul, he asked the media not to report on his search.<sup class=""reference"" id=""cite_ref-FOOTNOTEIsaacson2015253–255_283-0""><a href=""#cite_note-FOOTNOTEIsaacson2015253–255-283"">[283]</a></sup> Jobs stated that he was motivated to find his birth mother out of both curiosity and a need ""to see if she was okay and to thank her, because I'm glad I didn't end up as an abortion. She was twenty-three and she went through a lot to have me.""<sup class=""reference"" id=""cite_ref-FOOTNOTEIsaacson2015254_284-0""><a href=""#cite_note-FOOTNOTEIsaacson2015254-284"">[284]</a></sup> Schieble was emotional during their first meeting (though she wasn't familiar with the history of Apple or Jobs's role in it) and told him that she had been pressured into signing the adoption papers. She said that she regretted giving him up and repeatedly apologized to him for it. Jobs and Schieble would develop a friendly relationship throughout the rest of his life and would spend Christmas together.<sup class=""reference"" id=""cite_ref-FOOTNOTEIsaacson2015258_285-0""><a href=""#cite_note-FOOTNOTEIsaacson2015258-285"">[285]</a></sup>
</p>, <p>During this first visit, Schieble told Jobs that he had a sister, Mona, who was not aware that she had a brother.<sup class=""reference"" id=""cite_ref-FOOTNOTEIsaacson2015254_284-1""><a href=""#cite_note-FOOTNOTEIsaacson2015254-284"">[284]</a></sup> Schieble then arranged for them to meet in New York where Mona worked. Her first impression of Jobs was that ""he was totally straightforward and lovely, just a normal and sweet guy.""<sup class=""reference"" id=""cite_ref-FOOTNOTEIsaacson2015255_286-0""><a href=""#cite_note-FOOTNOTEIsaacson2015255-286"">[286]</a></sup> Simpson and Jobs then went for a long walk to get to know each other.<sup class=""reference"" id=""cite_ref-FOOTNOTEIsaacson2015255_286-1""><a href=""#cite_note-FOOTNOTEIsaacson2015255-286"">[286]</a></sup> Jobs later told his biographer that ""Mona was not completely thrilled at first to have me in her life and have her mother so emotionally affectionate toward me<span class=""nowrap""> </span>... As we got to know each other, we became really good friends, and she is my family. I don't know what I'd do without her. I can't imagine a better sister. My adopted sister, Patty, and I were never close.""<sup class=""reference"" id=""cite_ref-FOOTNOTEIsaacson2015255_286-2""><a href=""#cite_note-FOOTNOTEIsaacson2015255-286"">[286]</a></sup>
</p>, <p>""I grew up as an only child, with a single mother. Because we were poor and because I knew my father had emigrated from Syria, I imagined he looked like <a href=""/wiki/Omar_Sharif"" title=""Omar Sharif"">Omar Sharif</a>. I hoped he would be rich and kind and would come into our lives (and our not-yet-furnished apartment) and help us. Later, after I'd met my father, I tried to believe he'd changed his number and left no forwarding address because he was an idealistic revolutionary, plotting a new world for the Arab people. Even as a feminist, my whole life I'd been waiting for a man to love, who could love me. For decades, I'd thought that man would be my father. When I was 25, I met that man, and he was my brother.""
</p>, <p><cite class=""left-aligned"" style="""">—<a href=""/wiki/Mona_Simpson"" title=""Mona Simpson"">Mona Simpson</a><sup class=""reference"" id=""cite_ref-eulogy_209-3""><a href=""#cite_note-eulogy-209"">[209]</a></sup></cite>
</p>, <p>
Jobs then learned his family history. Six months after he was given up for adoption, Schieble's father died, she wed Jandali, and they had a daughter, Mona.<sup class=""reference"" id=""cite_ref-sg_3-3""><a href=""#cite_note-sg-3"">[3]</a></sup><sup class=""reference"" id=""cite_ref-FOOTNOTEIsaacson2015253_287-0""><a href=""#cite_note-FOOTNOTEIsaacson2015253-287"">[287]</a></sup> Jandali states that after finishing his PhD he returned to Syria to work and that it was during this period that Schieble left him<sup class=""reference"" id=""cite_ref-sg_3-4""><a href=""#cite_note-sg-3"">[3]</a></sup> (they divorced in 1962).<sup class=""reference"" id=""cite_ref-FOOTNOTEIsaacson201116_20-2""><a href=""#cite_note-FOOTNOTEIsaacson201116-20"">[20]</a></sup> He also states that after the divorce he lost contact with Mona for a period of time: </p>, <p>I also bear the responsibility for being away from my daughter when she was four years old, as her mother divorced me when I went to Syria, but we got back in touch after 10 years. We lost touch again when her mother moved and I didn't know where she was, but since 10 years ago we've been in constant contact, and I see her three times a year. I organized a trip for her last year to visit Syria and Lebanon and she went with a relative from Florida.<sup class=""reference"" id=""cite_ref-sg_3-5""><a href=""#cite_note-sg-3"">[3]</a></sup></p>, <p> A few years later, Schieble married an ice skating teacher, George Simpson.<sup class=""reference"" id=""cite_ref-FOOTNOTEIsaacson2015253_287-1""><a href=""#cite_note-FOOTNOTEIsaacson2015253-287"">[287]</a></sup> Mona Jandali took her stepfather's last name and thus became Mona Simpson. In 1970, after divorcing her second husband, Schieble took Mona to Los Angeles and raised her on her own.<sup class=""reference"" id=""cite_ref-FOOTNOTEIsaacson2015253_287-2""><a href=""#cite_note-FOOTNOTEIsaacson2015253-287"">[287]</a></sup>
</p>, <p>When Simpson found that their father, Abdulfattah Jandali, was living in <a href=""/wiki/Sacramento,_California"" title=""Sacramento, California"">Sacramento, California</a>, Jobs had no interest in meeting him as he believed Jandali didn't treat his children well.<sup class=""reference"" id=""cite_ref-FOOTNOTEIsaacson2015256_288-0""><a href=""#cite_note-FOOTNOTEIsaacson2015256-288"">[288]</a></sup> Simpson went to Sacramento alone and met Jandali, who worked in a small restaurant.<sup class=""reference"" id=""cite_ref-FOOTNOTEIsaacson2015257_289-0""><a href=""#cite_note-FOOTNOTEIsaacson2015257-289"">[289]</a></sup> Jandali and Simpson spoke for several hours, during which time he told her that he had left teaching for the restaurant business.<sup class=""reference"" id=""cite_ref-FOOTNOTEIsaacson2015257_289-1""><a href=""#cite_note-FOOTNOTEIsaacson2015257-289"">[289]</a></sup> He also said that he and Schieble had given another child away for adoption but that ""we'll never see that baby again. That baby's gone.""<sup class=""reference"" id=""cite_ref-FOOTNOTEIsaacson2015257_289-2""><a href=""#cite_note-FOOTNOTEIsaacson2015257-289"">[289]</a></sup> At the request of Jobs, Simpson did not tell Jandali that she had met his son.<sup class=""reference"" id=""cite_ref-FOOTNOTEIsaacson2015257_289-3""><a href=""#cite_note-FOOTNOTEIsaacson2015257-289"">[289]</a></sup> Jandali further told Simpson that he once managed a Mediterranean restaurant near <a href=""/wiki/San_Jose,_California"" title=""San Jose, California"">San Jose</a> and that ""all of the successful technology people used to come there. Even Steve Jobs ... oh yeah, he used to come in, and he was a sweet guy and a big tipper.""<sup class=""reference"" id=""cite_ref-FOOTNOTEIsaacson2015257_289-4""><a href=""#cite_note-FOOTNOTEIsaacson2015257-289"">[289]</a></sup>
</p>, <p>After hearing about the visit, Jobs recalled that ""it was amazing ... I had been to that restaurant a few times, and I remember meeting the owner. He was Syrian. Balding. We shook hands.""<sup class=""reference"" id=""cite_ref-FOOTNOTEIsaacson2015257_289-5""><a href=""#cite_note-FOOTNOTEIsaacson2015257-289"">[289]</a></sup> However, Jobs still did not want to meet Jandali because ""I was a wealthy man by then, and I didn't trust him not to try to blackmail me or go to the press about it ... I asked Mona not to tell him about me.""<sup class=""reference"" id=""cite_ref-FOOTNOTEIsaacson2015257_289-6""><a href=""#cite_note-FOOTNOTEIsaacson2015257-289"">[289]</a></sup> Jandali later discovered his relationship to Jobs through an online blog. He then contacted Simpson and asked ""what is this thing about Steve Jobs?"" Simpson told him that it was true and later commented, ""My father is thoughtful and a beautiful storyteller, but he is very, very passive ... He never contacted Steve.""<sup class=""reference"" id=""cite_ref-FOOTNOTEIsaacson2015258_285-1""><a href=""#cite_note-FOOTNOTEIsaacson2015258-285"">[285]</a></sup> Because Simpson herself researched her Syrian roots and began to meet members of the family, she assumed that Jobs would eventually want to meet their father, but he never did.<sup class=""reference"" id=""cite_ref-FOOTNOTEIsaacson2015258_285-2""><a href=""#cite_note-FOOTNOTEIsaacson2015258-285"">[285]</a></sup> Jobs also never showed an interest in his Syrian heritage or the Middle East.<sup class=""reference"" id=""cite_ref-FOOTNOTEIsaacson2015258_285-3""><a href=""#cite_note-FOOTNOTEIsaacson2015258-285"">[285]</a></sup> Simpson fictionalized the search for their father in her 1992 novel <i><a href=""/wiki/The_Lost_Father"" title=""The Lost Father"">The Lost Father</a></i>.<sup class=""reference"" id=""cite_ref-FOOTNOTEIsaacson2015258_285-4""><a href=""#cite_note-FOOTNOTEIsaacson2015258-285"">[285]</a></sup> <a href=""/wiki/Malek_Jandali"" title=""Malek Jandali"">Malek Jandali</a> is their cousin.<sup class=""reference"" id=""cite_ref-290""><a href=""#cite_note-290"">[290]</a></sup>
</p>, <p>Jobs kept his philanthropic and charitable efforts private; he donated $50 million to Stanford hospital and also contributed to efforts to cure AIDS.<sup class=""reference"" id=""cite_ref-291""><a href=""#cite_note-291"">[291]</a></sup> He also formed his own charitable foundation called the Steven P. Jobs foundation in 1985.<sup class=""reference"" id=""cite_ref-292""><a href=""#cite_note-292"">[292]</a></sup>
</p>, <p><b>Sources:</b>
</p>]"
